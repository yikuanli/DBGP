{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.insert(0, '/home/yikuan/project/Code/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.pytorch import save_model\n",
    "from common.common import create_folder, load_obj\n",
    "import os\n",
    "import torch\n",
    "from ACM.model.utils.utils import age_vocab\n",
    "import pandas as pd\n",
    "import pytorch_pretrained_bert as Bert\n",
    "from ACM.dataLoader.HF import HF_data\n",
    "from ACM.model.behrt import BertHF\n",
    "from ACM.model.optimiser import adam\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertConfig(Bert.modeling.BertConfig):\n",
    "    def __init__(self, config):\n",
    "        super(BertConfig, self).__init__(\n",
    "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_hidden_layers=config.get('num_hidden_layers'),\n",
    "            num_attention_heads=config.get('num_attention_heads'),\n",
    "            intermediate_size=config.get('intermediate_size'),\n",
    "            hidden_act=config.get('hidden_act'),\n",
    "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
    "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
    "            max_position_embeddings=config.get('max_position_embeddings'),\n",
    "            initializer_range=config.get('initializer_range'),\n",
    "        )\n",
    "        self.seg_vocab_size = config.get('seg_vocab_size')\n",
    "        self.age_vocab_size = config.get('age_vocab_size')\n",
    "        self.num_labels = config.get('num_labels')\n",
    "\n",
    "\n",
    "class TrainConfig(object):\n",
    "    def __init__(self, config):\n",
    "        self.batch_size = config.get('batch_size')\n",
    "        self.use_cuda = config.get('use_cuda')\n",
    "        self.max_len_seq = config.get('max_len_seq')\n",
    "        self.train_loader_workers = config.get('train_loader_workers')\n",
    "        self.test_loader_workers = config.get('test_loader_workers')\n",
    "        self.device = config.get('device')\n",
    "        self.output_dir = config.get('output_dir')\n",
    "        self.output_name = config.get('output_name')\n",
    "        self.best_name = config.get('best_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_config = {\n",
    "    'vocab':'/home/yikuan/project/Code/ACM/data/Full_vocab',\n",
    "    'train': '/home/shared/yikuan/ACM/data/Depression/Depression_clean_train.parquet',\n",
    "    'test': '/home/shared/yikuan/ACM/data/Depression/Depression_clean_test.parquet'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_params = {\n",
    "    'max_seq_len': 256,\n",
    "    'max_age': 110,\n",
    "    'month': 1,\n",
    "    'age_symbol': None,\n",
    "}\n",
    "\n",
    "optim_param = {\n",
    "    'lr': 3e-5,\n",
    "    'warmup_proportion': 0.1,\n",
    "    'weight_decay': 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertVocab = load_obj(file_config['vocab'])\n",
    "ageVocab, _ = age_vocab(max_age=global_params['max_age'], mon=global_params['month'], symbol=global_params['age_symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = pd.read_parquet(file_config['train']).reset_index(drop=True)\n",
    "testData = pd.read_parquet(file_config['test']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    'batch_size': 64,\n",
    "    'use_cuda': True,\n",
    "    'max_len_seq': global_params['max_seq_len'],\n",
    "    'train_loader_workers': 3,\n",
    "    'test_loader_workers': 3,\n",
    "    'device': 'cuda:1',\n",
    "    'output_dir': '/home/shared/yikuan/ACM/model/Depression',\n",
    "    'output_name': 'behrt.bin',\n",
    "    'best_name': 'behrt_best.bin',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainConfig = TrainConfig(train_params)\n",
    "\n",
    "data_set = HF_data(trainConfig, trainData, testData, BertVocab['token2idx'], ageVocab, code='code', age='age')\n",
    "\n",
    "trainData = data_set.get_weighted_sample_train(4)\n",
    "testData = data_set.get_test_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_param = {\n",
    "    'vocab_size': len(BertVocab['token2idx'].keys()),\n",
    "    'hidden_size': 150,\n",
    "    'num_hidden_layers': 4,\n",
    "    'num_attention_heads': 6,\n",
    "    'hidden_act': 'gelu',\n",
    "    'intermediate_size': 108,\n",
    "    'max_position_embeddings': global_params['max_seq_len'],\n",
    "    'seg_vocab_size': 2,\n",
    "    'age_vocab_size': len(ageVocab.keys()),\n",
    "    'prior_prec': 1e0,\n",
    "    'prec_init': 1e0,\n",
    "    'initializer_range': 0.02,\n",
    "    'num_labels': 1,\n",
    "    'hidden_dropout_prob': 0.29,\n",
    "    'attention_probs_dropout_prob': 0.38\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelConfig = BertConfig(model_param)\n",
    "model = BertHF(modelConfig, num_labels=modelConfig.num_labels, n_dim=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n"
     ]
    }
   ],
   "source": [
    "def load_model(path, model):\n",
    "    # load pretrained model and update weights\n",
    "    pretrained_dict = torch.load(path)\n",
    "    model_dict = model.state_dict()\n",
    "    # 1. filter out unnecessary keys\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and k not in ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']}\n",
    "    # 2. overwrite entries in the existing state dict\n",
    "    model_dict.update(pretrained_dict)\n",
    "    # 3. load the new state dict\n",
    "    model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "model = load_model('/home/shared/yikuan/HF/MLM/PureICD_diag_med.bin', model)\n",
    "model = model.to(trainConfig.device)\n",
    "optim = adam(list(model.named_parameters()),config=optim_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(logits, label):\n",
    "    sig = nn.Sigmoid()\n",
    "    output=sig(logits)\n",
    "    label, output=label.cpu(), output.detach().cpu()\n",
    "    prc= average_precision_score(label.numpy(),output.numpy())\n",
    "    auc = roc_auc_score(label.numpy(),output.numpy())\n",
    "    return prc, auc\n",
    "\n",
    "def precision_test(logits, label):\n",
    "    sig = nn.Sigmoid()\n",
    "    output=sig(logits)\n",
    "    prc= average_precision_score(label.numpy(),output.numpy())\n",
    "    auc = roc_auc_score(label.numpy(),output.numpy())\n",
    "    return prc, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(e, trainload, model, optim):\n",
    "    model.train()\n",
    "\n",
    "    tr_loss = 0\n",
    "    temp_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    cnt = 0\n",
    "    for step, batch in enumerate(trainload):\n",
    "        cnt += 1\n",
    "\n",
    "        age_ids, input_ids, posi_ids, segment_ids, attMask, targets, _,_ = batch\n",
    "\n",
    "        age_ids = age_ids.to(train_params['device'])\n",
    "        input_ids = input_ids.to(train_params['device'])\n",
    "        posi_ids = posi_ids.to(train_params['device'])\n",
    "        segment_ids = segment_ids.to(train_params['device'])\n",
    "        attMask = attMask.to(train_params['device'])\n",
    "        targets = targets.view(-1).to(train_params['device'])\n",
    "\n",
    "        loss, _ = model(input_ids, age_ids, segment_ids, posi_ids, attention_mask=attMask, labels=targets)\n",
    "        loss.backward()\n",
    "\n",
    "        temp_loss += loss.item()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            #             prec, a, b = precision(logits, targets)\n",
    "            #             print(\"epoch: {}\\t| Cnt: {}\\t| Loss: {}\\t| precision: {}\".format(e, cnt, temp_loss / 500, prec))\n",
    "            print(\"epoch: {}\\t|step: {}\\t|Loss: {}\".format(e, step, temp_loss / 500))\n",
    "            temp_loss = 0\n",
    "\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "    # Save a trained model\n",
    "    output_model_file = os.path.join(trainConfig.output_dir, trainConfig.output_name)\n",
    "    create_folder(trainConfig.output_dir)\n",
    "    save_model(output_model_file, model)\n",
    "\n",
    "\n",
    "def evaluation(testload, model):\n",
    "    model.eval()\n",
    "    y = []\n",
    "    y_label = []\n",
    "    loss_temp = 0\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(testload):\n",
    "            age_ids, input_ids, posi_ids, segment_ids, attMask, targets, _,_ = batch\n",
    "\n",
    "            age_ids = age_ids.to(train_params['device'])\n",
    "            input_ids = input_ids.to(train_params['device'])\n",
    "            posi_ids = posi_ids.to(train_params['device'])\n",
    "            segment_ids = segment_ids.to(train_params['device'])\n",
    "            attMask = attMask.to(train_params['device'])\n",
    "            targets = targets.view(-1).to(train_params['device'])\n",
    "\n",
    "            logits =model(input_ids, age_ids, segment_ids, posi_ids, attention_mask=attMask, labels=None)\n",
    "            targets = targets.cpu()\n",
    "\n",
    "            y_label.append(targets)\n",
    "            y.append(logits.cpu())\n",
    "\n",
    "        y_label = torch.cat(y_label, dim=0)\n",
    "        y = torch.cat(y, dim=0)\n",
    "\n",
    "        tempprc, prc = precision_test(y.view(-1), y_label.view(-1))\n",
    "        return tempprc, prc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t|step: 0\t|Loss: 0.0013849666118621825\n",
      "epoch: 0\t|step: 500\t|Loss: 0.4600920511484146\n",
      "epoch: 0\t|step: 1000\t|Loss: 0.43003752520680427\n",
      "epoch: 0\t|step: 1500\t|Loss: 0.4301555517315865\n",
      "epoch: 0\t|step: 2000\t|Loss: 0.4221146545410156\n",
      "epoch: 0\t|step: 2500\t|Loss: 0.426911899805069\n",
      "epoch: 0\t|step: 3000\t|Loss: 0.422379921913147\n",
      "epoch: 0\t|step: 3500\t|Loss: 0.4210122010707855\n",
      "epoch: 0\t|step: 4000\t|Loss: 0.4199312850832939\n",
      "epoch: 0\t|step: 4500\t|Loss: 0.424438825905323\n",
      "epoch: 0\t|step: 5000\t|Loss: 0.4217379032969475\n",
      "epoch: 0\t|step: 5500\t|Loss: 0.4201628498733044\n",
      "epoch: 0\t|step: 6000\t|Loss: 0.4155524201989174\n",
      "epoch: 0\t|step: 6500\t|Loss: 0.41815310961008073\n",
      "epoch: 0\t|step: 7000\t|Loss: 0.4222968499660492\n",
      "epoch: 0\t|step: 7500\t|Loss: 0.41978851452469823\n",
      "epoch: 0\t|step: 8000\t|Loss: 0.4156451911628246\n",
      "epoch: 0\t|step: 8500\t|Loss: 0.4214037240743637\n",
      "epoch: 0\t|step: 9000\t|Loss: 0.4193955805301666\n",
      "epoch: 0\t|step: 9500\t|Loss: 0.4215896881520748\n",
      "epoch: 0\t|step: 10000\t|Loss: 0.4155043041408062\n",
      "epoch: 0\t|step: 10500\t|Loss: 0.4225359671711922\n",
      "epoch: 0\t|step: 11000\t|Loss: 0.4203523560166359\n",
      "epoch: 0\t|step: 11500\t|Loss: 0.4166896490156651\n",
      "epoch: 0\t|step: 12000\t|Loss: 0.4129855362176895\n",
      "epoch: 0\t|step: 12500\t|Loss: 0.4149273901581764\n",
      "epoch: 0\t|step: 13000\t|Loss: 0.4174021102786064\n",
      "epoch: 0\t|step: 13500\t|Loss: 0.41454746904969214\n",
      "epoch: 0\t|step: 14000\t|Loss: 0.4150637721717358\n",
      "epoch: 0\t|step: 14500\t|Loss: 0.41974673959612846\n",
      "epoch: 0\t|step: 15000\t|Loss: 0.4176964304745197\n",
      "epoch: 0\t|step: 15500\t|Loss: 0.410126649081707\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test prc: 0.43477640909439824, auc0.7817769952968706\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t|step: 0\t|Loss: 0.0008227934837341308\n",
      "epoch: 1\t|step: 500\t|Loss: 0.4148051988482475\n",
      "epoch: 1\t|step: 1000\t|Loss: 0.4124573830068111\n",
      "epoch: 1\t|step: 1500\t|Loss: 0.4117074400186539\n",
      "epoch: 1\t|step: 2000\t|Loss: 0.4137388020157814\n",
      "epoch: 1\t|step: 2500\t|Loss: 0.4116367244422436\n",
      "epoch: 1\t|step: 3000\t|Loss: 0.41298723804950715\n",
      "epoch: 1\t|step: 3500\t|Loss: 0.41071211236715316\n",
      "epoch: 1\t|step: 4000\t|Loss: 0.40715817591547965\n",
      "epoch: 1\t|step: 4500\t|Loss: 0.41846193301677703\n",
      "epoch: 1\t|step: 5000\t|Loss: 0.4129525806605816\n",
      "epoch: 1\t|step: 5500\t|Loss: 0.41335370048880576\n",
      "epoch: 1\t|step: 6500\t|Loss: 0.4126672578752041\n",
      "epoch: 1\t|step: 7000\t|Loss: 0.4112657586336136\n",
      "epoch: 1\t|step: 7500\t|Loss: 0.41212818202376367\n",
      "epoch: 1\t|step: 8000\t|Loss: 0.4116474665999413\n",
      "epoch: 1\t|step: 8500\t|Loss: 0.419022166788578\n",
      "epoch: 1\t|step: 9000\t|Loss: 0.4127207798063755\n",
      "epoch: 1\t|step: 9500\t|Loss: 0.4178226061463356\n",
      "epoch: 1\t|step: 10000\t|Loss: 0.4135720997750759\n",
      "epoch: 1\t|step: 10500\t|Loss: 0.4152988600134849\n",
      "epoch: 1\t|step: 11000\t|Loss: 0.4111266621351242\n",
      "epoch: 1\t|step: 11500\t|Loss: 0.4041328621506691\n",
      "epoch: 1\t|step: 12000\t|Loss: 0.41278192412853243\n",
      "epoch: 1\t|step: 12500\t|Loss: 0.41362256643176076\n",
      "epoch: 1\t|step: 13000\t|Loss: 0.4122621383666992\n",
      "epoch: 1\t|step: 13500\t|Loss: 0.4128369737863541\n",
      "epoch: 1\t|step: 14000\t|Loss: 0.408621773570776\n",
      "epoch: 1\t|step: 14500\t|Loss: 0.41153576847910883\n",
      "epoch: 1\t|step: 15000\t|Loss: 0.41464397379755974\n",
      "epoch: 1\t|step: 15500\t|Loss: 0.41359678837656977\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test prc: 0.4425352470620078, auc0.7852435536445266\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t|step: 0\t|Loss: 0.0009754829406738282\n",
      "epoch: 2\t|step: 500\t|Loss: 0.41045190158486367\n",
      "epoch: 2\t|step: 1000\t|Loss: 0.41055545100569724\n",
      "epoch: 2\t|step: 1500\t|Loss: 0.41195042634010315\n",
      "epoch: 2\t|step: 2000\t|Loss: 0.41433848306536675\n",
      "epoch: 2\t|step: 2500\t|Loss: 0.41344643238186835\n",
      "epoch: 2\t|step: 3000\t|Loss: 0.41676555716991426\n",
      "epoch: 2\t|step: 3500\t|Loss: 0.4153448798656464\n",
      "epoch: 2\t|step: 4000\t|Loss: 0.4134080564081669\n",
      "epoch: 2\t|step: 4500\t|Loss: 0.4087799954414368\n",
      "epoch: 2\t|step: 5000\t|Loss: 0.40478287091851234\n",
      "epoch: 2\t|step: 5500\t|Loss: 0.4103893699347973\n",
      "epoch: 2\t|step: 6000\t|Loss: 0.40740079247951505\n",
      "epoch: 2\t|step: 6500\t|Loss: 0.41067841142416\n",
      "epoch: 2\t|step: 7000\t|Loss: 0.40937672698497773\n",
      "epoch: 2\t|step: 7500\t|Loss: 0.4094982563853264\n",
      "epoch: 2\t|step: 8000\t|Loss: 0.40917703065276145\n",
      "epoch: 2\t|step: 8500\t|Loss: 0.41908813309669496\n",
      "epoch: 2\t|step: 9000\t|Loss: 0.4141696395277977\n",
      "epoch: 2\t|step: 9500\t|Loss: 0.40913425576686857\n",
      "epoch: 2\t|step: 10000\t|Loss: 0.40648363569378854\n",
      "epoch: 2\t|step: 10500\t|Loss: 0.4024693852365017\n",
      "epoch: 2\t|step: 11000\t|Loss: 0.4048862947821617\n",
      "epoch: 2\t|step: 11500\t|Loss: 0.40953577134013175\n",
      "epoch: 2\t|step: 12000\t|Loss: 0.4093145617246628\n",
      "epoch: 2\t|step: 12500\t|Loss: 0.4069192344844341\n",
      "epoch: 2\t|step: 13000\t|Loss: 0.40762615367770194\n",
      "epoch: 2\t|step: 13500\t|Loss: 0.4004665571451187\n",
      "epoch: 2\t|step: 14000\t|Loss: 0.40527531325817107\n",
      "epoch: 2\t|step: 14500\t|Loss: 0.4055661446750164\n",
      "epoch: 2\t|step: 15000\t|Loss: 0.4156261720061302\n",
      "epoch: 2\t|step: 15500\t|Loss: 0.41116704973578455\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test prc: 0.44237401556158207, auc0.78498446624308\n",
      "epoch: 3\t|step: 0\t|Loss: 0.000776368498802185\n",
      "epoch: 3\t|step: 500\t|Loss: 0.4119457678496838\n",
      "epoch: 3\t|step: 1000\t|Loss: 0.4091699900329113\n",
      "epoch: 3\t|step: 1500\t|Loss: 0.41308115059137346\n",
      "epoch: 3\t|step: 2000\t|Loss: 0.4023112610578537\n",
      "epoch: 3\t|step: 2500\t|Loss: 0.3997615396976471\n",
      "epoch: 3\t|step: 3000\t|Loss: 0.4031705037355423\n",
      "epoch: 3\t|step: 3500\t|Loss: 0.4081346816122532\n",
      "epoch: 3\t|step: 4000\t|Loss: 0.4068806882202625\n",
      "epoch: 3\t|step: 4500\t|Loss: 0.40735761830210687\n",
      "epoch: 3\t|step: 5000\t|Loss: 0.40765592938661577\n",
      "epoch: 3\t|step: 5500\t|Loss: 0.40884457957744597\n",
      "epoch: 3\t|step: 6000\t|Loss: 0.40179503893852236\n",
      "epoch: 3\t|step: 6500\t|Loss: 0.40537402960658075\n",
      "epoch: 3\t|step: 7000\t|Loss: 0.4092785102725029\n",
      "epoch: 3\t|step: 7500\t|Loss: 0.40207497248053553\n",
      "epoch: 3\t|step: 8000\t|Loss: 0.40629642572999003\n",
      "epoch: 3\t|step: 8500\t|Loss: 0.40934419226646424\n",
      "epoch: 3\t|step: 9000\t|Loss: 0.4057192339897156\n",
      "epoch: 3\t|step: 9500\t|Loss: 0.40677816382050513\n",
      "epoch: 3\t|step: 10000\t|Loss: 0.40996639579534533\n",
      "epoch: 3\t|step: 10500\t|Loss: 0.4048558548092842\n",
      "epoch: 3\t|step: 11000\t|Loss: 0.40072346705198286\n",
      "epoch: 3\t|step: 11500\t|Loss: 0.4073726886808872\n",
      "epoch: 3\t|step: 12000\t|Loss: 0.4095272087752819\n",
      "epoch: 3\t|step: 12500\t|Loss: 0.4046936227381229\n",
      "epoch: 3\t|step: 13000\t|Loss: 0.4065614065229893\n",
      "epoch: 3\t|step: 13500\t|Loss: 0.410229678183794\n",
      "epoch: 3\t|step: 14000\t|Loss: 0.40742514371871946\n",
      "epoch: 3\t|step: 14500\t|Loss: 0.40629309821128845\n",
      "epoch: 3\t|step: 15000\t|Loss: 0.40238697853684424\n",
      "epoch: 3\t|step: 15500\t|Loss: 0.4134832581877708\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test prc: 0.44378150422953455, auc0.7858471192042583\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 4\t|step: 0\t|Loss: 0.0005914911627769471\n",
      "epoch: 4\t|step: 500\t|Loss: 0.40449871438741686\n",
      "epoch: 4\t|step: 1000\t|Loss: 0.40303928539156914\n",
      "epoch: 4\t|step: 1500\t|Loss: 0.4041541968882084\n",
      "epoch: 4\t|step: 2000\t|Loss: 0.4051728937923908\n",
      "epoch: 4\t|step: 2500\t|Loss: 0.41086031433939935\n",
      "epoch: 4\t|step: 3000\t|Loss: 0.402692596167326\n",
      "epoch: 4\t|step: 3500\t|Loss: 0.40302496987581254\n",
      "epoch: 4\t|step: 4000\t|Loss: 0.40038004767894747\n",
      "epoch: 4\t|step: 4500\t|Loss: 0.4032637013196945\n",
      "epoch: 4\t|step: 5000\t|Loss: 0.40863598173856736\n",
      "epoch: 4\t|step: 5500\t|Loss: 0.40127420184016227\n",
      "epoch: 4\t|step: 6000\t|Loss: 0.40557352870702745\n",
      "epoch: 4\t|step: 6500\t|Loss: 0.40011075326800344\n",
      "epoch: 4\t|step: 7000\t|Loss: 0.406176574409008\n",
      "epoch: 4\t|step: 7500\t|Loss: 0.4051509750187397\n",
      "epoch: 4\t|step: 8000\t|Loss: 0.40340447238087657\n",
      "epoch: 4\t|step: 8500\t|Loss: 0.4031464386880398\n",
      "epoch: 4\t|step: 9000\t|Loss: 0.4093677188158035\n",
      "epoch: 4\t|step: 9500\t|Loss: 0.4002987895309925\n",
      "epoch: 4\t|step: 10000\t|Loss: 0.4019908310770988\n",
      "epoch: 4\t|step: 10500\t|Loss: 0.40112329307198524\n",
      "epoch: 4\t|step: 11000\t|Loss: 0.4118686127066612\n",
      "epoch: 4\t|step: 11500\t|Loss: 0.3939557189643383\n",
      "epoch: 4\t|step: 12000\t|Loss: 0.4027358944416046\n",
      "epoch: 4\t|step: 12500\t|Loss: 0.3985314245820045\n",
      "epoch: 4\t|step: 13000\t|Loss: 0.4042148501574993\n",
      "epoch: 4\t|step: 13500\t|Loss: 0.40232991963624953\n",
      "epoch: 4\t|step: 14000\t|Loss: 0.3999708767533302\n",
      "epoch: 4\t|step: 14500\t|Loss: 0.40219082617759705\n",
      "epoch: 4\t|step: 15000\t|Loss: 0.39815556624531745\n",
      "epoch: 4\t|step: 15500\t|Loss: 0.4025499642789364\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test prc: 0.4488880127780346, auc0.786770623067419\n",
      "** ** * Saving fine - tuned model ** ** * \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5\t|step: 0\t|Loss: 0.0006321948766708374\n",
      "epoch: 5\t|step: 500\t|Loss: 0.40258555123209955\n",
      "epoch: 5\t|step: 1000\t|Loss: 0.4039654767513275\n",
      "epoch: 5\t|step: 1500\t|Loss: 0.3990810188651085\n",
      "epoch: 5\t|step: 2000\t|Loss: 0.4041485235691071\n",
      "epoch: 5\t|step: 2500\t|Loss: 0.3985883340239525\n",
      "epoch: 5\t|step: 3000\t|Loss: 0.40369786059856416\n",
      "epoch: 5\t|step: 3500\t|Loss: 0.40100994789600375\n",
      "epoch: 5\t|step: 4000\t|Loss: 0.40069636118412016\n",
      "epoch: 5\t|step: 4500\t|Loss: 0.4013688159286976\n",
      "epoch: 5\t|step: 5000\t|Loss: 0.40321208247542384\n",
      "epoch: 5\t|step: 5500\t|Loss: 0.4081941133439541\n",
      "epoch: 5\t|step: 6000\t|Loss: 0.3997779286503792\n",
      "epoch: 5\t|step: 6500\t|Loss: 0.4030659210085869\n",
      "epoch: 5\t|step: 7000\t|Loss: 0.4006944591403008\n",
      "epoch: 5\t|step: 7500\t|Loss: 0.4001564683318138\n",
      "epoch: 5\t|step: 8000\t|Loss: 0.3993958587050438\n",
      "epoch: 5\t|step: 8500\t|Loss: 0.4020997706055641\n",
      "epoch: 5\t|step: 9000\t|Loss: 0.4058608768582344\n",
      "epoch: 5\t|step: 9500\t|Loss: 0.40648863703012467\n",
      "epoch: 5\t|step: 10000\t|Loss: 0.3958017321527004\n",
      "epoch: 5\t|step: 10500\t|Loss: 0.4065148436427116\n",
      "epoch: 5\t|step: 11000\t|Loss: 0.39910576659440994\n",
      "epoch: 5\t|step: 11500\t|Loss: 0.4018503671884537\n",
      "epoch: 5\t|step: 12000\t|Loss: 0.39886672237515447\n",
      "epoch: 5\t|step: 12500\t|Loss: 0.4000700420439243\n",
      "epoch: 5\t|step: 13000\t|Loss: 0.39572405302524566\n",
      "epoch: 5\t|step: 13500\t|Loss: 0.4024902083277702\n",
      "epoch: 5\t|step: 14000\t|Loss: 0.3983835067152977\n",
      "epoch: 5\t|step: 14500\t|Loss: 0.3995885782837868\n",
      "epoch: 5\t|step: 15000\t|Loss: 0.4021599091887474\n",
      "epoch: 5\t|step: 15500\t|Loss: 0.40066164344549177\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test prc: 0.4480431406616745, auc0.7863806685643283\n",
      "epoch: 6\t|step: 0\t|Loss: 0.0009865198135375977\n",
      "epoch: 6\t|step: 500\t|Loss: 0.3992483631968498\n",
      "epoch: 6\t|step: 1000\t|Loss: 0.39888118124008176\n",
      "epoch: 6\t|step: 1500\t|Loss: 0.39965184047818186\n",
      "epoch: 6\t|step: 2000\t|Loss: 0.3965765334367752\n",
      "epoch: 6\t|step: 2500\t|Loss: 0.40498060673475267\n",
      "epoch: 6\t|step: 3000\t|Loss: 0.4071063560247421\n",
      "epoch: 6\t|step: 3500\t|Loss: 0.40465875709056853\n",
      "epoch: 6\t|step: 4000\t|Loss: 0.40048525139689445\n",
      "epoch: 6\t|step: 4500\t|Loss: 0.395323477268219\n",
      "epoch: 6\t|step: 5000\t|Loss: 0.40046525532007216\n",
      "epoch: 6\t|step: 5500\t|Loss: 0.40280733463168145\n",
      "epoch: 6\t|step: 6000\t|Loss: 0.3974215880930424\n",
      "epoch: 6\t|step: 6500\t|Loss: 0.39869495522975923\n",
      "epoch: 6\t|step: 7000\t|Loss: 0.4015707770586014\n",
      "epoch: 6\t|step: 7500\t|Loss: 0.4031114329397678\n",
      "epoch: 6\t|step: 8000\t|Loss: 0.39530992808938026\n",
      "epoch: 6\t|step: 8500\t|Loss: 0.40068357306718827\n",
      "epoch: 6\t|step: 9000\t|Loss: 0.3992542735636234\n",
      "epoch: 6\t|step: 9500\t|Loss: 0.3998819570243359\n",
      "epoch: 6\t|step: 10000\t|Loss: 0.3993361813426018\n",
      "epoch: 6\t|step: 10500\t|Loss: 0.40008430206775664\n",
      "epoch: 6\t|step: 11000\t|Loss: 0.4020302365720272\n",
      "epoch: 6\t|step: 11500\t|Loss: 0.3967804736196995\n",
      "epoch: 6\t|step: 12000\t|Loss: 0.4041657285094261\n",
      "epoch: 6\t|step: 12500\t|Loss: 0.39491089484095576\n",
      "epoch: 6\t|step: 13000\t|Loss: 0.3979732776284218\n",
      "epoch: 6\t|step: 13500\t|Loss: 0.397675870269537\n",
      "epoch: 6\t|step: 14000\t|Loss: 0.40118282264471056\n",
      "epoch: 6\t|step: 14500\t|Loss: 0.39995053273439407\n",
      "epoch: 6\t|step: 15000\t|Loss: 0.3938331491649151\n",
      "epoch: 6\t|step: 15500\t|Loss: 0.4012106110155582\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test prc: 0.44443768557167784, auc0.7842286717416085\n",
      "epoch: 7\t|step: 0\t|Loss: 0.0007107691764831542\n",
      "epoch: 7\t|step: 500\t|Loss: 0.40124730810523035\n",
      "epoch: 7\t|step: 1000\t|Loss: 0.39475353723764417\n",
      "epoch: 7\t|step: 1500\t|Loss: 0.39601983776688576\n",
      "epoch: 7\t|step: 2000\t|Loss: 0.3981639645397663\n",
      "epoch: 7\t|step: 2500\t|Loss: 0.3980400691330433\n",
      "epoch: 7\t|step: 3000\t|Loss: 0.3974131872355938\n",
      "epoch: 7\t|step: 3500\t|Loss: 0.39637145653367045\n",
      "epoch: 7\t|step: 4000\t|Loss: 0.3976507315635681\n",
      "epoch: 7\t|step: 4500\t|Loss: 0.398423088490963\n",
      "epoch: 7\t|step: 5000\t|Loss: 0.39530939716100694\n",
      "epoch: 7\t|step: 5500\t|Loss: 0.3986153562664986\n",
      "epoch: 7\t|step: 6000\t|Loss: 0.39784986171126363\n",
      "epoch: 7\t|step: 6500\t|Loss: 0.3948560699224472\n",
      "epoch: 7\t|step: 7000\t|Loss: 0.4026941627264023\n",
      "epoch: 7\t|step: 7500\t|Loss: 0.39285854452848434\n",
      "epoch: 7\t|step: 8000\t|Loss: 0.38938146468997004\n",
      "epoch: 7\t|step: 8500\t|Loss: 0.39538991433382037\n",
      "epoch: 7\t|step: 9000\t|Loss: 0.3966566558778286\n",
      "epoch: 7\t|step: 9500\t|Loss: 0.39323361629247666\n",
      "epoch: 7\t|step: 10000\t|Loss: 0.3964097594022751\n",
      "epoch: 7\t|step: 10500\t|Loss: 0.40112446960806847\n",
      "epoch: 7\t|step: 11000\t|Loss: 0.39497519662976266\n",
      "epoch: 7\t|step: 11500\t|Loss: 0.40136408814787866\n",
      "epoch: 7\t|step: 12000\t|Loss: 0.3912497681975365\n",
      "epoch: 7\t|step: 12500\t|Loss: 0.3940766126215458\n",
      "epoch: 7\t|step: 13000\t|Loss: 0.3962231339216232\n",
      "epoch: 7\t|step: 13500\t|Loss: 0.3997068945467472\n",
      "epoch: 7\t|step: 14000\t|Loss: 0.394644385188818\n",
      "epoch: 7\t|step: 14500\t|Loss: 0.3992121039927006\n",
      "epoch: 7\t|step: 15000\t|Loss: 0.3954405565559864\n",
      "epoch: 7\t|step: 15500\t|Loss: 0.39706028228998186\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test prc: 0.4436397474220993, auc0.7835486130718642\n",
      "epoch: 8\t|step: 0\t|Loss: 0.0007467182874679565\n",
      "epoch: 8\t|step: 500\t|Loss: 0.39548687463998794\n",
      "epoch: 8\t|step: 1000\t|Loss: 0.39842774376273155\n",
      "epoch: 8\t|step: 1500\t|Loss: 0.39473450085520745\n",
      "epoch: 8\t|step: 2000\t|Loss: 0.3971542472243309\n",
      "epoch: 8\t|step: 2500\t|Loss: 0.3991136848628521\n",
      "epoch: 8\t|step: 3000\t|Loss: 0.3961704962551594\n",
      "epoch: 8\t|step: 3500\t|Loss: 0.39979411363601686\n",
      "epoch: 8\t|step: 4000\t|Loss: 0.3953745034635067\n",
      "epoch: 8\t|step: 4500\t|Loss: 0.4019600292146206\n",
      "epoch: 8\t|step: 5000\t|Loss: 0.39695248517394066\n",
      "epoch: 8\t|step: 5500\t|Loss: 0.39667145201563836\n",
      "epoch: 8\t|step: 6000\t|Loss: 0.3972434697151184\n",
      "epoch: 8\t|step: 6500\t|Loss: 0.39582742872834203\n",
      "epoch: 8\t|step: 7000\t|Loss: 0.39862409883737565\n",
      "epoch: 8\t|step: 7500\t|Loss: 0.39378374627232554\n",
      "epoch: 8\t|step: 8000\t|Loss: 0.39571144101023675\n",
      "epoch: 8\t|step: 8500\t|Loss: 0.39661596205830574\n",
      "epoch: 8\t|step: 9000\t|Loss: 0.39383391988277433\n",
      "epoch: 8\t|step: 9500\t|Loss: 0.40133421713113787\n",
      "epoch: 8\t|step: 10000\t|Loss: 0.39055258095264433\n",
      "epoch: 8\t|step: 10500\t|Loss: 0.3978743031322956\n",
      "epoch: 8\t|step: 11000\t|Loss: 0.3961993508636951\n",
      "epoch: 8\t|step: 11500\t|Loss: 0.39276139116287234\n",
      "epoch: 8\t|step: 12000\t|Loss: 0.3940216153562069\n",
      "epoch: 8\t|step: 12500\t|Loss: 0.3931902677118778\n",
      "epoch: 8\t|step: 13000\t|Loss: 0.395757433116436\n",
      "epoch: 8\t|step: 13500\t|Loss: 0.39518232467770575\n",
      "epoch: 8\t|step: 14000\t|Loss: 0.40128126356005667\n",
      "epoch: 8\t|step: 14500\t|Loss: 0.3872746797800064\n",
      "epoch: 8\t|step: 15000\t|Loss: 0.39736423298716544\n",
      "epoch: 8\t|step: 15500\t|Loss: 0.3918086900115013\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test prc: 0.4405509817885201, auc0.7829838808759373\n",
      "epoch: 9\t|step: 0\t|Loss: 0.0007221896052360535\n",
      "epoch: 9\t|step: 500\t|Loss: 0.3940963231921196\n",
      "epoch: 9\t|step: 1000\t|Loss: 0.39425247383117673\n",
      "epoch: 9\t|step: 1500\t|Loss: 0.3921267023682594\n",
      "epoch: 9\t|step: 2000\t|Loss: 0.39264889404177666\n",
      "epoch: 9\t|step: 2500\t|Loss: 0.39435035425424575\n",
      "epoch: 9\t|step: 3000\t|Loss: 0.3893404570221901\n",
      "epoch: 9\t|step: 3500\t|Loss: 0.39114624792337416\n",
      "epoch: 9\t|step: 4000\t|Loss: 0.39223224550485614\n",
      "epoch: 9\t|step: 4500\t|Loss: 0.39167357492446897\n",
      "epoch: 9\t|step: 5000\t|Loss: 0.3959761064350605\n",
      "epoch: 9\t|step: 5500\t|Loss: 0.38837006455659867\n",
      "epoch: 9\t|step: 6000\t|Loss: 0.3904459525942802\n",
      "epoch: 9\t|step: 6500\t|Loss: 0.39863470834493636\n",
      "epoch: 9\t|step: 7000\t|Loss: 0.3936664982438087\n",
      "epoch: 9\t|step: 7500\t|Loss: 0.3970665004849434\n",
      "epoch: 9\t|step: 8000\t|Loss: 0.39589410066604613\n",
      "epoch: 9\t|step: 8500\t|Loss: 0.3907419926524162\n",
      "epoch: 9\t|step: 9000\t|Loss: 0.3959114249050617\n",
      "epoch: 9\t|step: 9500\t|Loss: 0.3922901036739349\n",
      "epoch: 9\t|step: 10000\t|Loss: 0.3938642733693123\n",
      "epoch: 9\t|step: 10500\t|Loss: 0.3900742605626583\n",
      "epoch: 9\t|step: 11000\t|Loss: 0.39212269297242164\n",
      "epoch: 9\t|step: 11500\t|Loss: 0.3907442120909691\n",
      "epoch: 9\t|step: 12000\t|Loss: 0.3933318493962288\n",
      "epoch: 9\t|step: 12500\t|Loss: 0.39412955433130265\n",
      "epoch: 9\t|step: 13000\t|Loss: 0.3892083165049553\n",
      "epoch: 9\t|step: 13500\t|Loss: 0.39233657482266426\n",
      "epoch: 9\t|step: 14000\t|Loss: 0.39351798897981644\n",
      "epoch: 9\t|step: 14500\t|Loss: 0.38999014419317246\n",
      "epoch: 9\t|step: 15000\t|Loss: 0.38849365627765653\n",
      "epoch: 9\t|step: 15500\t|Loss: 0.3906943975389004\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test prc: 0.441694428124136, auc0.7817424657983115\n",
      "epoch: 10\t|step: 0\t|Loss: 0.0009217742085456848\n",
      "epoch: 10\t|step: 500\t|Loss: 0.38850395420193673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _DataLoaderIter.__del__ at 0x7fea1bfb8598>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 677, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 659, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/multiprocessing/process.py\", line 140, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/multiprocessing/popen_fork.py\", line 48, in wait\n",
      "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
      "  File \"/home/yikuan/anaconda/envs/py3/lib/python3.7/multiprocessing/popen_fork.py\", line 28, in poll\n",
      "    pid, sts = os.waitpid(self.pid, flag)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-d910bfbaa763>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbest_pre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#     auc_train, time_cost_train = evaluation(trainData, model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mauc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-ee645f669055>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(e, trainload, model, optim)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposi_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattMask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtemp_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_pre = 0\n",
    "for e in range(20):\n",
    "    train(e, trainData, model,optim)\n",
    "#     auc_train, time_cost_train = evaluation(trainData, model)\n",
    "    auc_test, auc = evaluation(testData, model)\n",
    "    print('test prc: {}, auc{}'.format(auc_test, auc))\n",
    "    if auc_test >best_pre:\n",
    "        # Save a trained model\n",
    "        output_model_file = os.path.join(trainConfig.output_dir, trainConfig.best_name)\n",
    "        create_folder(trainConfig.output_dir)\n",
    "        save_model(output_model_file, model)\n",
    "        best_pre = auc_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3)",
   "language": "python",
   "name": "py3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
