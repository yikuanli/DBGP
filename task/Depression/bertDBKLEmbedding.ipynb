{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.insert(0, '/home/yikuan/project/Code/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.pytorch import save_model, load_model\n",
    "from common.common import create_folder, load_obj\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from ACM.model.utils.utils import age_vocab\n",
    "import pandas as pd\n",
    "import pytorch_pretrained_bert as Bert\n",
    "from ACM.dataLoader.HF import HF_data\n",
    "from ACM.model.bertDBKLEmbedding import BertHF\n",
    "import gpytorch\n",
    "from ACM.model.optimiser import adam\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertConfig(Bert.modeling.BertConfig):\n",
    "    def __init__(self, config):\n",
    "        super(BertConfig, self).__init__(\n",
    "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_hidden_layers=config.get('num_hidden_layers'),\n",
    "            num_attention_heads=config.get('num_attention_heads'),\n",
    "            intermediate_size=config.get('intermediate_size'),\n",
    "            hidden_act=config.get('hidden_act'),\n",
    "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
    "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
    "            max_position_embeddings=config.get('max_position_embeddings'),\n",
    "            initializer_range=config.get('initializer_range'),\n",
    "        )\n",
    "        self.seg_vocab_size = config.get('seg_vocab_size')\n",
    "        self.age_vocab_size = config.get('age_vocab_size')\n",
    "        self.num_labels = config.get('num_labels')\n",
    "        self.prior_rate = config.get('prior_rate')\n",
    "\n",
    "\n",
    "class TrainConfig(object):\n",
    "    def __init__(self, config):\n",
    "        self.batch_size = config.get('batch_size')\n",
    "        self.use_cuda = config.get('use_cuda')\n",
    "        self.max_len_seq = config.get('max_len_seq')\n",
    "        self.train_loader_workers = config.get('train_loader_workers')\n",
    "        self.test_loader_workers = config.get('test_loader_workers')\n",
    "        self.device = config.get('device')\n",
    "        self.output_dir = config.get('output_dir')\n",
    "        self.output_name = config.get('output_name')\n",
    "        self.best_name = config.get('best_name')\n",
    "        self.device1 = config.get('device1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_config = {\n",
    "    'vocab':'/home/yikuan/project/Code/ACM/data/Full_vocab',\n",
    "    'train': '/home/shared/yikuan/ACM/data/Depression/Depression_clean_train.parquet',\n",
    "    'test': '/home/shared/yikuan/ACM/data/Depression/Depression_clean_test.parquet'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_params = {\n",
    "    'max_seq_len': 256,\n",
    "    'max_age': 110,\n",
    "    'month': 1,\n",
    "    'age_symbol': None,\n",
    "}\n",
    "\n",
    "optim_param = {\n",
    "    'lr': 3e-5,\n",
    "    'warmup_proportion': 0.1,\n",
    "    'weight_decay': 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertVocab = load_obj(file_config['vocab'])\n",
    "ageVocab, _ = age_vocab(max_age=global_params['max_age'], mon=global_params['month'], symbol=global_params['age_symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData_raw = pd.read_parquet(file_config['train']).reset_index(drop=True)\n",
    "testData_raw = pd.read_parquet(file_config['test']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    'batch_size': 128,\n",
    "    'use_cuda': True,\n",
    "    'max_len_seq': global_params['max_seq_len'],\n",
    "    'train_loader_workers': 3,\n",
    "    'test_loader_workers': 3,\n",
    "    'device': 'cuda:1',\n",
    "    'device1': 'cuda:0',\n",
    "    'output_dir': '/home/shared/yikuan/ACM/model/Depression',\n",
    "    'output_name': 'behrtDBKLEmbeddingsPretrain.bin',\n",
    "    'best_name': 'behrtDBKLEmbeddingsPretrain_best.bin',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainConfig = TrainConfig(train_params)\n",
    "\n",
    "data_set = HF_data(trainConfig, trainData_raw, testData_raw, BertVocab['token2idx'], ageVocab, code='code', age='age')\n",
    "\n",
    "trainData = data_set.get_weighted_sample_train(4)\n",
    "testData = data_set.get_test_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_param = {\n",
    "    'vocab_size': len(BertVocab['token2idx'].keys()),\n",
    "    'hidden_size': 150,\n",
    "    'num_hidden_layers': 4,\n",
    "    'num_attention_heads': 6,\n",
    "    'hidden_act': 'gelu',\n",
    "    'intermediate_size': 108,\n",
    "    'max_position_embeddings': global_params['max_seq_len'],\n",
    "    'seg_vocab_size': 2,\n",
    "    'age_vocab_size': len(ageVocab.keys()),\n",
    "    'prior_prec': 1e0,\n",
    "    'prec_init': 1e0,\n",
    "    'initializer_range': 0.02,\n",
    "    'num_labels': 1,\n",
    "    'hidden_dropout_prob': 0.29,\n",
    "    'attention_probs_dropout_prob': 0.38,\n",
    "    'prior_rate': 0.347\n",
    "}\n",
    "\n",
    "feature_dict = {\n",
    "    'word': True,\n",
    "    'age': False,\n",
    "    'seg': False,\n",
    "    'norm': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelConfig = BertConfig(model_param)\n",
    "\n",
    "model = BertHF(modelConfig, n_dim=24,grid_size=40, ard_num_dims=24, feature_dict=feature_dict, cuda1=trainConfig.device, cuda2=trainConfig.device1, split=True)\n",
    "\n",
    "likelihood = gpytorch.likelihoods.BernoulliLikelihood().to(trainConfig.device1)\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model.gp_layer, num_data=len(trainData.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_dict = torch.load('/home/shared/yikuan/HF/MLM/PureICD_diag_med.bin')\n",
    "# pretrained_dict = torch.load('/home/shared/yikuan/ACM/model/Depression/behrtKISSGP_best.bin')\n",
    "model_dict = model.state_dict()\n",
    "name_dict = {\n",
    "    'bert.embeddings.word_embeddings.weight_posterior.loc': 'bert.embeddings.word_embeddings.weight',\n",
    "#     'bert.embeddings.segment_embeddings.weight_posterior.loc': 'bert.embeddings.segment_embeddings.weight',\n",
    "#     'bert.embeddings.age_embeddings.weight_posterior.loc': 'bert.embeddings.age_embeddings.weight'\n",
    "}\n",
    "for k,v in pretrained_dict.items():\n",
    "    if (k in model_dict) and (k not in ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']):\n",
    "#     if k in model_dict:\n",
    "        model_dict[k] = v\n",
    "for k,v in name_dict.items():\n",
    "    model_dict[k] = pretrained_dict[v]\n",
    "    \n",
    "# model_dict.update(pretrained_dict)\n",
    "# 3. load the new state dict\n",
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n"
     ]
    }
   ],
   "source": [
    "# def load_model(path, model):\n",
    "#     # load pretrained model and update weights\n",
    "#     pretrained_dict = torch.load(path)\n",
    "#     model_dict = model.state_dict()\n",
    "#     # 1. filter out unnecessary keys\n",
    "#     pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and k not in ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']}\n",
    "#     # 2. overwrite entries in the existing state dict\n",
    "#     model_dict.update(pretrained_dict)\n",
    "#     # 3. load the new state dict\n",
    "#     model.load_state_dict(model_dict)\n",
    "#     return model\n",
    "\n",
    "# model = load_model('/home/shared/yikuan/HF/MLM/PureICD_diag_med.bin', model)\n",
    "model.allocateGPU()\n",
    "optim = adam(list(model.named_parameters()),config=optim_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(logits, label):\n",
    "    sig = nn.Sigmoid()\n",
    "    output=sig(logits)\n",
    "    label, output=label.cpu(), output.detach().cpu()\n",
    "    tempprc= average_precision_score(label.numpy(),output.numpy())\n",
    "    auc = roc_auc_score(label.numpy(),output.numpy())\n",
    "    return tempprc, output, label\n",
    "\n",
    "def precision_test(logits, label):\n",
    "#     sig = nn.Sigmoid()\n",
    "#     output=sig(logits)\n",
    "    tempprc= average_precision_score(label.numpy(),logits.numpy())\n",
    "    auc = roc_auc_score(label.numpy(),logits.numpy())\n",
    "    return tempprc, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beta(batch_idx, m, beta_type):\n",
    "    if beta_type == \"Blundell\":\n",
    "        beta = 2 ** (m - (batch_idx + 1)) / (2 ** m - 1) \n",
    "    elif beta_type == \"Soenderby\":\n",
    "        beta = min(epoch / (num_epochs // 4), 1)\n",
    "    elif beta_type == \"Standard\":\n",
    "        beta = 1 / m \n",
    "    else:\n",
    "        beta = 0\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(e, trainload, model, likelihood, optim, m, beta_type):\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    tr_loss = 0\n",
    "    temp_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    cnt = 0\n",
    "    for step, batch in enumerate(trainload):\n",
    "        cnt += 1\n",
    "        \n",
    "        beta = get_beta(step, m, beta_type)\n",
    "        \n",
    "        age_ids, input_ids, posi_ids, segment_ids, attMask, targets, _, _ = batch\n",
    "\n",
    "        age_ids = age_ids.to(train_params['device'])\n",
    "        input_ids = input_ids.to(train_params['device'])\n",
    "        posi_ids = posi_ids.to(train_params['device'])\n",
    "        segment_ids = segment_ids.to(train_params['device'])\n",
    "        attMask = attMask.to(train_params['device'])\n",
    "        targets = targets.view(-1).to(train_params['device1'])\n",
    "\n",
    "        output, kl = model(input_ids, age_ids, segment_ids, posi_ids, attention_mask=attMask, labels=targets)\n",
    "        loss = -mll(output, targets)\n",
    "        loss = loss + beta*kl\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        temp_loss += loss.item()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            #             prec, a, b = precision(logits, targets)\n",
    "            #             print(\"epoch: {}\\t| Cnt: {}\\t| Loss: {}\\t| precision: {}\".format(e, cnt, temp_loss / 500, prec))\n",
    "            print(\"epoch: {}\\t|step: {}\\t|Loss: {}\".format(e, step, temp_loss / 500))\n",
    "            temp_loss = 0\n",
    "\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "    # Save a trained model\n",
    "    output_model_file = os.path.join(trainConfig.output_dir, trainConfig.output_name)\n",
    "    create_folder(trainConfig.output_dir)\n",
    "    save_model(output_model_file, model)\n",
    "\n",
    "\n",
    "def evaluation(testload, model, likelihood, n_sample=10):\n",
    "    model.eval()\n",
    "    y = []\n",
    "    y_label = []\n",
    "    loss_temp = 0\n",
    "\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(testload):\n",
    "            age_ids, input_ids, posi_ids, segment_ids, attMask, targets, _, _ = batch\n",
    "\n",
    "            age_ids = age_ids.to(train_params['device'])\n",
    "            input_ids = input_ids.to(train_params['device'])\n",
    "            posi_ids = posi_ids.to(train_params['device'])\n",
    "            segment_ids = segment_ids.to(train_params['device'])\n",
    "            attMask = attMask.to(train_params['device'])\n",
    "            targets = targets.view(-1).to(train_params['device'])\n",
    "\n",
    "#             output = likelihood(\n",
    "#                 model(input_ids, age_ids, segment_ids, posi_ids, attention_mask=attMask, labels=targets)[0])\n",
    "            output_list = []\n",
    "            for _ in range(n_sample):\n",
    "                output, kl = model(input_ids, age_ids, segment_ids, posi_ids, attention_mask=attMask, labels=targets)\n",
    "                output_list.append(output.sample())\n",
    "            output_list = torch.sigmoid(torch.stack(output_list, dim=0))\n",
    "            output_list = torch.mean(output_list, dim=0)\n",
    "            \n",
    "\n",
    "            logits = output_list.cpu()\n",
    "            targets = targets.cpu()\n",
    "\n",
    "            y_label.append(targets)\n",
    "            y.append(logits)\n",
    "\n",
    "        y_label = torch.cat(y_label, dim=0)\n",
    "        y = torch.cat(y, dim=0)\n",
    "\n",
    "        tempprc, auc = precision_test(y.view(-1), y_label.view(-1))\n",
    "        return tempprc, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t|step: 0\t|Loss: 9900.191\n",
      "epoch: 0\t|step: 1000\t|Loss: 3.136951189517975\n",
      "epoch: 0\t|step: 1500\t|Loss: 2.854350857257843\n",
      "epoch: 0\t|step: 2000\t|Loss: 2.6872472224235535\n",
      "epoch: 0\t|step: 2500\t|Loss: 2.573996618747711\n",
      "epoch: 0\t|step: 3000\t|Loss: 2.48343283033371\n",
      "epoch: 0\t|step: 3500\t|Loss: 2.387514896392822\n",
      "epoch: 0\t|step: 4000\t|Loss: 2.2992854778766634\n",
      "epoch: 0\t|step: 4500\t|Loss: 2.2257377982139586\n",
      "epoch: 0\t|step: 5000\t|Loss: 2.1009510695934295\n",
      "epoch: 0\t|step: 5500\t|Loss: 1.9919399330615997\n",
      "epoch: 0\t|step: 6000\t|Loss: 1.865952675819397\n",
      "epoch: 0\t|step: 6500\t|Loss: 1.7510594432353974\n",
      "epoch: 0\t|step: 7000\t|Loss: 1.6328491456508636\n",
      "epoch: 0\t|step: 7500\t|Loss: 1.517032470703125\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.2604050862577605, auc 0.6570423919247885\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t|step: 0\t|Loss: 9819.754\n",
      "epoch: 1\t|step: 500\t|Loss: 9819.46222919166\n",
      "epoch: 1\t|step: 1000\t|Loss: 1.2547689453363418\n",
      "epoch: 1\t|step: 1500\t|Loss: 1.1749115127325058\n",
      "epoch: 1\t|step: 2000\t|Loss: 1.1143974411487578\n",
      "epoch: 1\t|step: 2500\t|Loss: 1.0502974232435227\n",
      "epoch: 1\t|step: 3000\t|Loss: 0.9991341025829316\n",
      "epoch: 1\t|step: 3500\t|Loss: 0.9426658467054367\n",
      "epoch: 1\t|step: 4000\t|Loss: 0.8948725200891495\n",
      "epoch: 1\t|step: 4500\t|Loss: 0.8549500164985657\n",
      "epoch: 1\t|step: 5000\t|Loss: 0.8323820155858993\n",
      "epoch: 1\t|step: 5500\t|Loss: 0.7863845447301865\n",
      "epoch: 1\t|step: 6000\t|Loss: 0.7572477382421493\n",
      "epoch: 1\t|step: 6500\t|Loss: 0.725709881067276\n",
      "epoch: 1\t|step: 7000\t|Loss: 0.7084040115475655\n",
      "epoch: 1\t|step: 7500\t|Loss: 0.6801173577904701\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.3168108137162621, auc 0.7122725114525293\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t|step: 0\t|Loss: 9739.818\n",
      "epoch: 2\t|step: 500\t|Loss: 9738.796447775125\n",
      "epoch: 2\t|step: 1000\t|Loss: 0.6187845491170884\n",
      "epoch: 2\t|step: 1500\t|Loss: 0.596048192024231\n",
      "epoch: 2\t|step: 2000\t|Loss: 0.580190491259098\n",
      "epoch: 2\t|step: 2500\t|Loss: 0.5687557125091552\n",
      "epoch: 2\t|step: 3000\t|Loss: 0.55520373827219\n",
      "epoch: 2\t|step: 3500\t|Loss: 0.539960709631443\n",
      "epoch: 2\t|step: 4000\t|Loss: 0.5324043473601341\n",
      "epoch: 2\t|step: 4500\t|Loss: 0.5176843278408051\n",
      "epoch: 2\t|step: 5000\t|Loss: 0.5118695385456086\n",
      "epoch: 2\t|step: 5500\t|Loss: 0.5005279087424278\n",
      "epoch: 2\t|step: 6000\t|Loss: 0.4938418830037117\n",
      "epoch: 2\t|step: 6500\t|Loss: 0.4882780541777611\n",
      "epoch: 2\t|step: 7000\t|Loss: 0.48403419637680056\n",
      "epoch: 2\t|step: 7500\t|Loss: 0.47448855942487717\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.34560571204334944, auc 0.7541819834868407\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 3\t|step: 0\t|Loss: 9660.104\n",
      "epoch: 3\t|step: 500\t|Loss: 9658.931047134698\n",
      "epoch: 3\t|step: 1000\t|Loss: 0.4628797702789307\n",
      "epoch: 3\t|step: 1500\t|Loss: 0.452868354678154\n",
      "epoch: 3\t|step: 2000\t|Loss: 0.45131911170482636\n",
      "epoch: 3\t|step: 2500\t|Loss: 0.45229988092184065\n",
      "epoch: 3\t|step: 3000\t|Loss: 0.4425578773021698\n",
      "epoch: 3\t|step: 3500\t|Loss: 0.4452386857867241\n",
      "epoch: 3\t|step: 4000\t|Loss: 0.4412075102329254\n",
      "epoch: 3\t|step: 4500\t|Loss: 0.4400073894262314\n",
      "epoch: 3\t|step: 5000\t|Loss: 0.4390523782968521\n",
      "epoch: 3\t|step: 5500\t|Loss: 0.4346892825961113\n",
      "epoch: 3\t|step: 6000\t|Loss: 0.43399220931529997\n",
      "epoch: 3\t|step: 6500\t|Loss: 0.4330301014482975\n",
      "epoch: 3\t|step: 7000\t|Loss: 0.4327316782474518\n",
      "epoch: 3\t|step: 7500\t|Loss: 0.42929585033655165\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.3793523182329604, auc 0.7653846838727714\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 4\t|step: 0\t|Loss: 9581.266\n",
      "epoch: 4\t|step: 500\t|Loss: 9580.073010249078\n",
      "epoch: 4\t|step: 1000\t|Loss: 0.4268707273006439\n",
      "epoch: 4\t|step: 1500\t|Loss: 0.42835332942008975\n",
      "epoch: 4\t|step: 2000\t|Loss: 0.42917091292142867\n",
      "epoch: 4\t|step: 2500\t|Loss: 0.4295743301510811\n",
      "epoch: 4\t|step: 3000\t|Loss: 0.4339672501683235\n",
      "epoch: 4\t|step: 3500\t|Loss: 0.430281701028347\n",
      "epoch: 4\t|step: 4000\t|Loss: 0.42742888647317884\n",
      "epoch: 4\t|step: 4500\t|Loss: 0.4246242044568062\n",
      "epoch: 4\t|step: 5000\t|Loss: 0.4243681880235672\n",
      "epoch: 4\t|step: 5500\t|Loss: 0.4247417901158333\n",
      "epoch: 4\t|step: 6000\t|Loss: 0.42725049287080763\n",
      "epoch: 4\t|step: 6500\t|Loss: 0.42573060727119444\n",
      "epoch: 4\t|step: 7000\t|Loss: 0.4264003096818924\n",
      "epoch: 4\t|step: 7500\t|Loss: 0.42647933787107467\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.4025142572024446, auc 0.7702002488908843\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 5\t|step: 0\t|Loss: 9503.175\n",
      "epoch: 5\t|step: 500\t|Loss: 9501.990492168903\n",
      "epoch: 5\t|step: 1000\t|Loss: 0.4240826542973518\n",
      "epoch: 5\t|step: 1500\t|Loss: 0.4282268510460854\n",
      "epoch: 5\t|step: 2000\t|Loss: 0.42386961668729783\n",
      "epoch: 5\t|step: 2500\t|Loss: 0.42111477518081664\n",
      "epoch: 5\t|step: 3000\t|Loss: 0.4234087063074112\n",
      "epoch: 5\t|step: 3500\t|Loss: 0.4226401839256287\n",
      "epoch: 5\t|step: 4000\t|Loss: 0.42698963451385497\n",
      "epoch: 5\t|step: 4500\t|Loss: 0.4241705987453461\n",
      "epoch: 5\t|step: 5000\t|Loss: 0.4243473743200302\n",
      "epoch: 5\t|step: 5500\t|Loss: 0.42498378032445905\n",
      "epoch: 5\t|step: 6000\t|Loss: 0.42132295805215836\n",
      "epoch: 5\t|step: 6500\t|Loss: 0.42164892578125\n",
      "epoch: 5\t|step: 7000\t|Loss: 0.42141562926769255\n",
      "epoch: 5\t|step: 7500\t|Loss: 0.4257036280632019\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.40630912923553986, auc 0.7720338552390106\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 6\t|step: 0\t|Loss: 9425.713\n",
      "epoch: 6\t|step: 500\t|Loss: 9424.53348562634\n",
      "epoch: 6\t|step: 1000\t|Loss: 0.42160443598032\n",
      "epoch: 6\t|step: 1500\t|Loss: 0.4250543850064278\n",
      "epoch: 6\t|step: 2000\t|Loss: 0.42581761902570725\n",
      "epoch: 6\t|step: 2500\t|Loss: 0.4215185632109642\n",
      "epoch: 6\t|step: 3000\t|Loss: 0.42228737515211107\n",
      "epoch: 6\t|step: 3500\t|Loss: 0.41859264302253724\n",
      "epoch: 6\t|step: 4000\t|Loss: 0.42310749214887616\n",
      "epoch: 6\t|step: 4500\t|Loss: 0.4228329092860222\n",
      "epoch: 6\t|step: 5000\t|Loss: 0.4252429360151291\n",
      "epoch: 6\t|step: 5500\t|Loss: 0.42337803560495374\n",
      "epoch: 6\t|step: 6000\t|Loss: 0.4238393865823746\n",
      "epoch: 6\t|step: 6500\t|Loss: 0.418960972070694\n",
      "epoch: 6\t|step: 7000\t|Loss: 0.421690102994442\n",
      "epoch: 6\t|step: 7500\t|Loss: 0.42272374433279036\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.40672352178146787, auc 0.7724831520172233\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 7\t|step: 0\t|Loss: 9348.842\n",
      "epoch: 7\t|step: 500\t|Loss: 9347.661519216836\n",
      "epoch: 7\t|step: 1000\t|Loss: 0.4239498415589333\n",
      "epoch: 7\t|step: 1500\t|Loss: 0.4237436854839325\n",
      "epoch: 7\t|step: 2000\t|Loss: 0.4218967344760895\n",
      "epoch: 7\t|step: 2500\t|Loss: 0.4191213933229446\n",
      "epoch: 7\t|step: 3000\t|Loss: 0.4226763922572136\n",
      "epoch: 7\t|step: 3500\t|Loss: 0.42215400058031083\n",
      "epoch: 7\t|step: 4000\t|Loss: 0.4220836392045021\n",
      "epoch: 7\t|step: 4500\t|Loss: 0.42238299107551575\n",
      "epoch: 7\t|step: 5000\t|Loss: 0.42363987743854525\n",
      "epoch: 7\t|step: 5500\t|Loss: 0.41742467415332796\n",
      "epoch: 7\t|step: 6000\t|Loss: 0.4216830781698227\n",
      "epoch: 7\t|step: 6500\t|Loss: 0.421953298330307\n",
      "epoch: 7\t|step: 7000\t|Loss: 0.41999225270748136\n",
      "epoch: 7\t|step: 7500\t|Loss: 0.4218474681377411\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.4109802110372908, auc 0.7740371488948298\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 8\t|step: 0\t|Loss: 9272.529\n",
      "epoch: 8\t|step: 500\t|Loss: 9271.356664399862\n",
      "epoch: 8\t|step: 1000\t|Loss: 0.4174937264919281\n",
      "epoch: 8\t|step: 1500\t|Loss: 0.41868936794996264\n",
      "epoch: 8\t|step: 2000\t|Loss: 0.41759932470321653\n",
      "epoch: 8\t|step: 2500\t|Loss: 0.42579883271455765\n",
      "epoch: 8\t|step: 3000\t|Loss: 0.4214155161976814\n",
      "epoch: 8\t|step: 3500\t|Loss: 0.42041283375024796\n",
      "epoch: 8\t|step: 4000\t|Loss: 0.4250563247203827\n",
      "epoch: 8\t|step: 4500\t|Loss: 0.4204565578699112\n",
      "epoch: 8\t|step: 5000\t|Loss: 0.42063370352983476\n",
      "epoch: 8\t|step: 5500\t|Loss: 0.4187443096637726\n",
      "epoch: 8\t|step: 6000\t|Loss: 0.4217821734547615\n",
      "epoch: 8\t|step: 6500\t|Loss: 0.4171736075282097\n",
      "epoch: 8\t|step: 7000\t|Loss: 0.4178783389925957\n",
      "epoch: 8\t|step: 7500\t|Loss: 0.4178488919734955\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.41155520810629476, auc 0.7738549375347965\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 9\t|step: 0\t|Loss: 9196.836\n",
      "epoch: 9\t|step: 500\t|Loss: 9195.678449448465\n",
      "epoch: 9\t|step: 1000\t|Loss: 0.4237418282032013\n",
      "epoch: 9\t|step: 1500\t|Loss: 0.4172749316394329\n",
      "epoch: 9\t|step: 2000\t|Loss: 0.4249884521365166\n",
      "epoch: 9\t|step: 2500\t|Loss: 0.4172399411797523\n",
      "epoch: 9\t|step: 3000\t|Loss: 0.4196151407957077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9\t|step: 3500\t|Loss: 0.41970478308200837\n",
      "epoch: 9\t|step: 4000\t|Loss: 0.41765498691797254\n",
      "epoch: 9\t|step: 4500\t|Loss: 0.4172998984456062\n",
      "epoch: 9\t|step: 5000\t|Loss: 0.4237609326839447\n",
      "epoch: 9\t|step: 5500\t|Loss: 0.42025239539146425\n",
      "epoch: 9\t|step: 6000\t|Loss: 0.4214719729423523\n",
      "epoch: 9\t|step: 6500\t|Loss: 0.4178698251247406\n",
      "epoch: 9\t|step: 7000\t|Loss: 0.42299673533439636\n",
      "epoch: 9\t|step: 7500\t|Loss: 0.4176134424805641\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.41244999946756905, auc 0.7743396939816081\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 10\t|step: 0\t|Loss: 9121.717\n",
      "epoch: 10\t|step: 500\t|Loss: 9120.562701673567\n",
      "epoch: 10\t|step: 1000\t|Loss: 0.4182465274333954\n",
      "epoch: 10\t|step: 1500\t|Loss: 0.4211911106705666\n",
      "epoch: 10\t|step: 2000\t|Loss: 0.42071356779336927\n",
      "epoch: 10\t|step: 2500\t|Loss: 0.41597839975357054\n",
      "epoch: 10\t|step: 3000\t|Loss: 0.41881372189521787\n",
      "epoch: 10\t|step: 3500\t|Loss: 0.4159713637828827\n",
      "epoch: 10\t|step: 4000\t|Loss: 0.42093318212032316\n",
      "epoch: 10\t|step: 4500\t|Loss: 0.4166827784180641\n",
      "epoch: 10\t|step: 5000\t|Loss: 0.41591501343250276\n",
      "epoch: 10\t|step: 5500\t|Loss: 0.41470158904790877\n",
      "epoch: 10\t|step: 6000\t|Loss: 0.4196343629360199\n",
      "epoch: 10\t|step: 6500\t|Loss: 0.42075738322734835\n",
      "epoch: 10\t|step: 7000\t|Loss: 0.4169539997577667\n",
      "epoch: 10\t|step: 7500\t|Loss: 0.41665229386091235\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.4133069629846079, auc 0.7752561890626427\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 11\t|step: 0\t|Loss: 9047.142\n",
      "epoch: 11\t|step: 500\t|Loss: 9045.995379765749\n",
      "epoch: 11\t|step: 1000\t|Loss: 0.4176228619813919\n",
      "epoch: 11\t|step: 1500\t|Loss: 0.41778061151504514\n",
      "epoch: 11\t|step: 2000\t|Loss: 0.4170239409208298\n",
      "epoch: 11\t|step: 2500\t|Loss: 0.4180762843489647\n",
      "epoch: 11\t|step: 3000\t|Loss: 0.4198821417093277\n",
      "epoch: 11\t|step: 3500\t|Loss: 0.418503957927227\n",
      "epoch: 11\t|step: 4000\t|Loss: 0.4156691751480103\n",
      "epoch: 11\t|step: 4500\t|Loss: 0.41372409415245054\n",
      "epoch: 11\t|step: 5000\t|Loss: 0.4186832482814789\n",
      "epoch: 11\t|step: 5500\t|Loss: 0.4210606731772423\n",
      "epoch: 11\t|step: 6000\t|Loss: 0.41635374957323074\n",
      "epoch: 11\t|step: 6500\t|Loss: 0.4176391891539097\n",
      "epoch: 11\t|step: 7000\t|Loss: 0.4161238186955452\n",
      "epoch: 11\t|step: 7500\t|Loss: 0.4153840460777283\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.4145382913925264, auc 0.7753284001465719\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 12\t|step: 0\t|Loss: 8973.115\n",
      "epoch: 12\t|step: 500\t|Loss: 8971.975388312996\n",
      "epoch: 12\t|step: 1000\t|Loss: 0.4166568377017975\n",
      "epoch: 12\t|step: 1500\t|Loss: 0.41703493624925614\n",
      "epoch: 12\t|step: 2000\t|Loss: 0.41498350918293\n",
      "epoch: 12\t|step: 2500\t|Loss: 0.41811426883935926\n",
      "epoch: 12\t|step: 3000\t|Loss: 0.4190134072899818\n",
      "epoch: 12\t|step: 3500\t|Loss: 0.41409332245588304\n",
      "epoch: 12\t|step: 4000\t|Loss: 0.41379328924417497\n",
      "epoch: 12\t|step: 4500\t|Loss: 0.41831375944614413\n",
      "epoch: 12\t|step: 5000\t|Loss: 0.4150455281436443\n",
      "epoch: 12\t|step: 5500\t|Loss: 0.41338477504253385\n",
      "epoch: 12\t|step: 6000\t|Loss: 0.41546411204338074\n",
      "epoch: 12\t|step: 6500\t|Loss: 0.41963543075323106\n",
      "epoch: 12\t|step: 7000\t|Loss: 0.4157591417431831\n",
      "epoch: 12\t|step: 7500\t|Loss: 0.4183958940505981\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.41544811205013205, auc 0.7754313576512942\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 13\t|step: 0\t|Loss: 8899.67\n",
      "epoch: 13\t|step: 500\t|Loss: 8898.5351774078\n",
      "epoch: 13\t|step: 1000\t|Loss: 0.41520762515068055\n",
      "epoch: 13\t|step: 1500\t|Loss: 0.41795942491292953\n",
      "epoch: 13\t|step: 2000\t|Loss: 0.4162329650521278\n",
      "epoch: 13\t|step: 2500\t|Loss: 0.41307446801662445\n",
      "epoch: 13\t|step: 3000\t|Loss: 0.4130590322315693\n",
      "epoch: 13\t|step: 3500\t|Loss: 0.4153835317492485\n",
      "epoch: 13\t|step: 4000\t|Loss: 0.4166638458967209\n",
      "epoch: 13\t|step: 4500\t|Loss: 0.41491402113437653\n",
      "epoch: 13\t|step: 5000\t|Loss: 0.4129774066209793\n",
      "epoch: 13\t|step: 5500\t|Loss: 0.4176574116945267\n",
      "epoch: 13\t|step: 6000\t|Loss: 0.41196324211359026\n",
      "epoch: 13\t|step: 6500\t|Loss: 0.41523691287636755\n",
      "epoch: 13\t|step: 7000\t|Loss: 0.41785886627435687\n",
      "epoch: 13\t|step: 7500\t|Loss: 0.41539695447683334\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.41500167455337106, auc 0.7754321110345739\n",
      "epoch: 14\t|step: 0\t|Loss: 8826.79\n",
      "epoch: 14\t|step: 500\t|Loss: 8825.660971253752\n",
      "epoch: 14\t|step: 1000\t|Loss: 0.41814097136259076\n",
      "epoch: 14\t|step: 1500\t|Loss: 0.41569442486763003\n",
      "epoch: 14\t|step: 2000\t|Loss: 0.4174586832523346\n",
      "epoch: 14\t|step: 2500\t|Loss: 0.4164998353719711\n",
      "epoch: 14\t|step: 3000\t|Loss: 0.41490513062477113\n",
      "epoch: 14\t|step: 3500\t|Loss: 0.4130438460111618\n",
      "epoch: 14\t|step: 4000\t|Loss: 0.41475581204891204\n",
      "epoch: 14\t|step: 4500\t|Loss: 0.4172628433704376\n",
      "epoch: 14\t|step: 5000\t|Loss: 0.41731468534469607\n",
      "epoch: 14\t|step: 5500\t|Loss: 0.4145547807216644\n",
      "epoch: 14\t|step: 6000\t|Loss: 0.4217149201631546\n",
      "epoch: 14\t|step: 6500\t|Loss: 0.41257196420431136\n",
      "epoch: 14\t|step: 7000\t|Loss: 0.4209514493346214\n",
      "epoch: 14\t|step: 7500\t|Loss: 0.41168841248750687\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.41437842300959105, auc 0.7748871903113711\n",
      "epoch: 15\t|step: 0\t|Loss: 8754.433\n",
      "epoch: 15\t|step: 500\t|Loss: 8753.311225459694\n",
      "epoch: 15\t|step: 1000\t|Loss: 0.4120759705901146\n",
      "epoch: 15\t|step: 1500\t|Loss: 0.4122031245827675\n",
      "epoch: 15\t|step: 2000\t|Loss: 0.41275737875699997\n",
      "epoch: 15\t|step: 2500\t|Loss: 0.41414120864868165\n",
      "epoch: 15\t|step: 3000\t|Loss: 0.41415946650505064\n",
      "epoch: 15\t|step: 3500\t|Loss: 0.41636391800642014\n",
      "epoch: 15\t|step: 4000\t|Loss: 0.4160170040726662\n",
      "epoch: 15\t|step: 4500\t|Loss: 0.4116489476561546\n",
      "epoch: 15\t|step: 5000\t|Loss: 0.4145267189741135\n",
      "epoch: 15\t|step: 5500\t|Loss: 0.41350229316949844\n",
      "epoch: 15\t|step: 6000\t|Loss: 0.4188918974995613\n",
      "epoch: 15\t|step: 6500\t|Loss: 0.41081817841529844\n",
      "epoch: 15\t|step: 7000\t|Loss: 0.4121148512363434\n",
      "epoch: 15\t|step: 7500\t|Loss: 0.4129647113084793\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.41616109659469436, auc 0.7761986306382496\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 16\t|step: 0\t|Loss: 8682.64\n",
      "epoch: 16\t|step: 500\t|Loss: 8681.528814872592\n",
      "epoch: 16\t|step: 1000\t|Loss: 0.41596068352460863\n",
      "epoch: 16\t|step: 1500\t|Loss: 0.41690924459695816\n",
      "epoch: 16\t|step: 2000\t|Loss: 0.41489832079410555\n",
      "epoch: 16\t|step: 2500\t|Loss: 0.41084730088710786\n",
      "epoch: 16\t|step: 3000\t|Loss: 0.4114086073637009\n",
      "epoch: 16\t|step: 3500\t|Loss: 0.4112770465016365\n",
      "epoch: 16\t|step: 4000\t|Loss: 0.4165775310397148\n",
      "epoch: 16\t|step: 4500\t|Loss: 0.40977140802145007\n",
      "epoch: 16\t|step: 5000\t|Loss: 0.41153278544545174\n",
      "epoch: 16\t|step: 5500\t|Loss: 0.4178282124400139\n",
      "epoch: 16\t|step: 6000\t|Loss: 0.4156472290158272\n",
      "epoch: 16\t|step: 6500\t|Loss: 0.416394101023674\n",
      "epoch: 16\t|step: 7000\t|Loss: 0.41586895102262494\n",
      "epoch: 16\t|step: 7500\t|Loss: 0.4183873916864395\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.4145369079942333, auc 0.7754887998030178\n",
      "epoch: 17\t|step: 0\t|Loss: 8611.432\n",
      "epoch: 17\t|step: 500\t|Loss: 8610.327922359169\n",
      "epoch: 17\t|step: 1000\t|Loss: 0.4175943608283997\n",
      "epoch: 17\t|step: 1500\t|Loss: 0.4126259719133377\n",
      "epoch: 17\t|step: 2000\t|Loss: 0.4142756186723709\n",
      "epoch: 17\t|step: 2500\t|Loss: 0.4158395664691925\n",
      "epoch: 17\t|step: 3000\t|Loss: 0.41868207830190657\n",
      "epoch: 17\t|step: 3500\t|Loss: 0.41318760871887206\n",
      "epoch: 17\t|step: 4000\t|Loss: 0.4178403292894363\n",
      "epoch: 17\t|step: 4500\t|Loss: 0.41319833785295484\n",
      "epoch: 17\t|step: 5000\t|Loss: 0.4169158970117569\n",
      "epoch: 17\t|step: 5500\t|Loss: 0.413482762157917\n",
      "epoch: 17\t|step: 6000\t|Loss: 0.41567890578508376\n",
      "epoch: 17\t|step: 6500\t|Loss: 0.41287863194942476\n",
      "epoch: 17\t|step: 7000\t|Loss: 0.40997728604078293\n",
      "epoch: 17\t|step: 7500\t|Loss: 0.41337804913520815\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.4153994434085009, auc 0.7757232979353541\n",
      "epoch: 18\t|step: 0\t|Loss: 8540.753\n",
      "epoch: 18\t|step: 500\t|Loss: 8539.649449842871\n",
      "epoch: 18\t|step: 1000\t|Loss: 0.4139952452778816\n",
      "epoch: 18\t|step: 1500\t|Loss: 0.41294640535116195\n",
      "epoch: 18\t|step: 2000\t|Loss: 0.4146380761861801\n",
      "epoch: 18\t|step: 2500\t|Loss: 0.4138609212636948\n",
      "epoch: 18\t|step: 3000\t|Loss: 0.4169869295358658\n",
      "epoch: 18\t|step: 3500\t|Loss: 0.41230417692661286\n",
      "epoch: 18\t|step: 4000\t|Loss: 0.4141654129624367\n",
      "epoch: 18\t|step: 4500\t|Loss: 0.4141313109993935\n",
      "epoch: 18\t|step: 5000\t|Loss: 0.41530169212818147\n",
      "epoch: 18\t|step: 5500\t|Loss: 0.40903694194555285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18\t|step: 6000\t|Loss: 0.4133999134898186\n",
      "epoch: 18\t|step: 6500\t|Loss: 0.4117538890838623\n",
      "epoch: 18\t|step: 7000\t|Loss: 0.41335689508914947\n",
      "epoch: 18\t|step: 7500\t|Loss: 0.4111737092137337\n",
      "** ** * Saving fine - tuned model ** ** * \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e4d20d44925b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Blundell\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     auc_train, time_cost_train = evaluation(trainData, model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mauc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test precision: {}, auc {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mauc_test\u001b[0m \u001b[0;34m>\u001b[0m\u001b[0mbest_pre\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-1121b80d69a7>\u001b[0m in \u001b[0;36mevaluation\u001b[0;34m(testload, model, likelihood, n_sample)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0moutput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposi_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattMask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                 \u001b[0moutput_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0moutput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/Code/ACM/model/bertDBKLEmbedding.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, age_ids, seg_ids, posi_ids, attention_mask, labels, beta)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposi_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;31m# pooled_output = self.dropout(pooled_output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_to_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_bounds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_bounds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;31m# if self.split:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py3/lib/python3.7/site-packages/gpytorch/utils/grid.py\u001b[0m in \u001b[0;36mscale_to_bounds\u001b[0;34m(x, lower_bound, upper_bound)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \"\"\"\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Scale features so they fit inside grid bounds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mmin_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mmax_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_val\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmin_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_pre = 0\n",
    "for e in range(20):\n",
    "    train(e, trainData, model, likelihood,optim, len(trainData), \"Blundell\")\n",
    "#     auc_train, time_cost_train = evaluation(trainData, model)\n",
    "    auc_test, auc = evaluation(testData, model, likelihood, n_sample=20)\n",
    "    print('test precision: {}, auc {}'.format(auc_test, auc))\n",
    "    if auc_test >best_pre:\n",
    "        # Save a trained model\n",
    "        output_model_file = os.path.join(trainConfig.output_dir, trainConfig.best_name)\n",
    "        create_folder(trainConfig.output_dir)\n",
    "        save_model(output_model_file, model)\n",
    "        best_pre = auc_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3)",
   "language": "python",
   "name": "py3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
