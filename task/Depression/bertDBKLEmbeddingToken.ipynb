{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.insert(0, '/home/yikuan/project/Code/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.pytorch import save_model\n",
    "from common.common import create_folder, load_obj\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from ACM.model.utils.utils import age_vocab\n",
    "import pandas as pd\n",
    "import pytorch_pretrained_bert as Bert\n",
    "from ACM.dataLoader.HF import HF_data\n",
    "from ACM.model.bertBayesianToken import BertHF\n",
    "import gpytorch\n",
    "from ACM.model.optimiser import adam\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertConfig(Bert.modeling.BertConfig):\n",
    "    def __init__(self, config):\n",
    "        super(BertConfig, self).__init__(\n",
    "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_hidden_layers=config.get('num_hidden_layers'),\n",
    "            num_attention_heads=config.get('num_attention_heads'),\n",
    "            intermediate_size=config.get('intermediate_size'),\n",
    "            hidden_act=config.get('hidden_act'),\n",
    "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
    "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
    "            max_position_embeddings=config.get('max_position_embeddings'),\n",
    "            initializer_range=config.get('initializer_range'),\n",
    "        )\n",
    "        self.seg_vocab_size = config.get('seg_vocab_size')\n",
    "        self.age_vocab_size = config.get('age_vocab_size')\n",
    "        self.num_labels = config.get('num_labels')\n",
    "        self.prior_rate = config.get('prior_rate')\n",
    "\n",
    "\n",
    "class TrainConfig(object):\n",
    "    def __init__(self, config):\n",
    "        self.batch_size = config.get('batch_size')\n",
    "        self.use_cuda = config.get('use_cuda')\n",
    "        self.max_len_seq = config.get('max_len_seq')\n",
    "        self.train_loader_workers = config.get('train_loader_workers')\n",
    "        self.test_loader_workers = config.get('test_loader_workers')\n",
    "        self.device = config.get('device')\n",
    "        self.output_dir = config.get('output_dir')\n",
    "        self.output_name = config.get('output_name')\n",
    "        self.best_name = config.get('best_name')\n",
    "        self.device1 = config.get('device1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_config = {\n",
    "    'vocab':'/home/yikuan/project/Code/ACM/data/Full_vocab',\n",
    "    'train': '/home/shared/yikuan/ACM/data/Diabetes/diabetes_clean_train.parquet',\n",
    "    'test': '/home/shared/yikuan/ACM/data/Diabetes/diabetes_clean_test.parquet'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_params = {\n",
    "    'max_seq_len': 256,\n",
    "    'max_age': 110,\n",
    "    'month': 1,\n",
    "    'age_symbol': None,\n",
    "}\n",
    "\n",
    "optim_param = {\n",
    "    'lr': 3e-5,\n",
    "    'warmup_proportion': 0.1,\n",
    "    'weight_decay': 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertVocab = load_obj(file_config['vocab'])\n",
    "ageVocab, _ = age_vocab(max_age=global_params['max_age'], mon=global_params['month'], symbol=global_params['age_symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData_raw = pd.read_parquet(file_config['train']).reset_index(drop=True)\n",
    "testData_raw = pd.read_parquet(file_config['test']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    'batch_size': 64,\n",
    "    'use_cuda': True,\n",
    "    'max_len_seq': global_params['max_seq_len'],\n",
    "    'train_loader_workers': 3,\n",
    "    'test_loader_workers': 3,\n",
    "    'device': 'cuda:1',\n",
    "    'device1': 'cuda:1',\n",
    "    'output_dir': '/home/shared/yikuan/ACM/model/Diabetes',\n",
    "    'output_name': 'behrtDBKLEmbeddingToken.bin',\n",
    "    'best_name': 'behrtDBKLEmbeddingsToken_best.bin',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainConfig = TrainConfig(train_params)\n",
    "\n",
    "data_set = HF_data(trainConfig, trainData_raw, testData_raw, BertVocab['token2idx'], ageVocab, code='code', age='age')\n",
    "\n",
    "trainData = data_set.get_weighted_sample_train(4)\n",
    "testData = data_set.get_test_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_param = {\n",
    "    'vocab_size': len(BertVocab['token2idx'].keys()),\n",
    "    'hidden_size': 150,\n",
    "    'num_hidden_layers': 4,\n",
    "    'num_attention_heads': 6,\n",
    "    'hidden_act': 'gelu',\n",
    "    'intermediate_size': 108,\n",
    "    'max_position_embeddings': global_params['max_seq_len'],\n",
    "    'seg_vocab_size': 2,\n",
    "    'age_vocab_size': len(ageVocab.keys()),\n",
    "    'prior_prec': 1e0,\n",
    "    'prec_init': 1e0,\n",
    "    'initializer_range': 0.02,\n",
    "    'num_labels': 1,\n",
    "    'hidden_dropout_prob': 0.29,\n",
    "    'attention_probs_dropout_prob': 0.38,\n",
    "    'prior_rate': 0.347\n",
    "}\n",
    "\n",
    "feature_dict = {\n",
    "    'word': True,\n",
    "    'age': False,\n",
    "    'seg': False,\n",
    "    'norm': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelConfig = BertConfig(model_param)\n",
    "\n",
    "model = BertHF(modelConfig, n_dim=24,grid_size=40, ard_num_dims=24, feature_dict=feature_dict, cuda1=trainConfig.device, cuda2=trainConfig.device1, split=True)\n",
    "\n",
    "likelihood = gpytorch.likelihoods.BernoulliLikelihood().to(trainConfig.device1)\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model.gp_layer, num_data=len(trainData.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_dict = torch.load('/home/shared/yikuan/HF/MLM/PureICD_diag_med.bin')\n",
    "model_dict = model.state_dict()\n",
    "name_dict = {\n",
    "    'bert.embeddings.word_embeddings_mu.weight': 'bert.embeddings.word_embeddings.weight',\n",
    "    'bert.embeddings.segment_embeddings_mu.weight': 'bert.embeddings.segment_embeddings.weight',\n",
    "    'bert.embeddings.age_embeddings_mu.weight': 'bert.embeddings.age_embeddings.weight'\n",
    "}\n",
    "for k,v in pretrained_dict.items():\n",
    "    if (k in model_dict) and (k not in ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']):\n",
    "        model_dict[k] = v\n",
    "for k,v in name_dict.items():\n",
    "    model_dict[k] = pretrained_dict[v]\n",
    "    \n",
    "# model_dict.update(pretrained_dict)\n",
    "# 3. load the new state dict\n",
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n"
     ]
    }
   ],
   "source": [
    "def load_model(path, model):\n",
    "    # load pretrained model and update weights\n",
    "    pretrained_dict = torch.load(path)\n",
    "    model_dict = model.state_dict()\n",
    "    # 1. filter out unnecessary keys\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and k not in ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']}\n",
    "    # 2. overwrite entries in the existing state dict\n",
    "    model_dict.update(pretrained_dict)\n",
    "    # 3. load the new state dict\n",
    "    model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "# model = load_model('/home/shared/yikuan/HF/MLM/PureICD_diag_med.bin', model)\n",
    "model.allocateGPU()\n",
    "optim = adam(list(model.named_parameters()),config=optim_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(logits, label):\n",
    "    sig = nn.Sigmoid()\n",
    "    output=sig(logits)\n",
    "    label, output=label.cpu(), output.detach().cpu()\n",
    "    tempprc= average_precision_score(label.numpy(),output.numpy())\n",
    "    auc = roc_auc_score(label.numpy(),output.numpy())\n",
    "    return tempprc, output, label\n",
    "\n",
    "def precision_test(logits, label):\n",
    "#     sig = nn.Sigmoid()\n",
    "#     output=sig(logits)\n",
    "    tempprc= average_precision_score(label.numpy(),logits.numpy())\n",
    "    auc = roc_auc_score(label.numpy(),logits.numpy())\n",
    "    return tempprc, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beta(batch_idx, m, beta_type):\n",
    "    if beta_type == \"Blundell\":\n",
    "        beta = 2 ** (m - (batch_idx + 1)) / (2 ** m - 1) \n",
    "    elif beta_type == \"Soenderby\":\n",
    "        beta = min(epoch / (num_epochs // 4), 1)\n",
    "    elif beta_type == \"Standard\":\n",
    "        beta = 1 / m \n",
    "    else:\n",
    "        beta = 0\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(e, trainload, model, likelihood, optim, m, beta_type):\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    tr_loss = 0\n",
    "    temp_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    cnt = 0\n",
    "    for step, batch in enumerate(trainload):\n",
    "        cnt += 1\n",
    "        \n",
    "        beta = get_beta(step, m, beta_type)\n",
    "        \n",
    "        age_ids, input_ids, posi_ids, segment_ids, attMask, targets, _, _ = batch\n",
    "\n",
    "        age_ids = age_ids.to(train_params['device'])\n",
    "        input_ids = input_ids.to(train_params['device'])\n",
    "        posi_ids = posi_ids.to(train_params['device'])\n",
    "        segment_ids = segment_ids.to(train_params['device'])\n",
    "        attMask = attMask.to(train_params['device'])\n",
    "        targets = targets.view(-1).to(train_params['device1'])\n",
    "\n",
    "        output, kl = model(input_ids, age_ids, segment_ids, posi_ids, attention_mask=attMask, labels=targets)\n",
    "        loss = -mll(output, targets)\n",
    "        loss = loss + beta*kl\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        temp_loss += loss.item()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            #             prec, a, b = precision(logits, targets)\n",
    "            #             print(\"epoch: {}\\t| Cnt: {}\\t| Loss: {}\\t| precision: {}\".format(e, cnt, temp_loss / 500, prec))\n",
    "            print(\"epoch: {}\\t|step: {}\\t|Loss: {}\".format(e, step, temp_loss / 500))\n",
    "            temp_loss = 0\n",
    "\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "    # Save a trained model\n",
    "    output_model_file = os.path.join(trainConfig.output_dir, trainConfig.output_name)\n",
    "    create_folder(trainConfig.output_dir)\n",
    "    save_model(output_model_file, model)\n",
    "\n",
    "\n",
    "def evaluation(testload, model, likelihood, n_sample=10):\n",
    "    model.eval()\n",
    "    y = []\n",
    "    y_label = []\n",
    "    loss_temp = 0\n",
    "\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(testload):\n",
    "            age_ids, input_ids, posi_ids, segment_ids, attMask, targets, _, _ = batch\n",
    "\n",
    "            age_ids = age_ids.to(train_params['device'])\n",
    "            input_ids = input_ids.to(train_params['device'])\n",
    "            posi_ids = posi_ids.to(train_params['device'])\n",
    "            segment_ids = segment_ids.to(train_params['device'])\n",
    "            attMask = attMask.to(train_params['device'])\n",
    "            targets = targets.view(-1).to(train_params['device'])\n",
    "\n",
    "#             output = likelihood(\n",
    "#                 model(input_ids, age_ids, segment_ids, posi_ids, attention_mask=attMask, labels=targets)[0])\n",
    "            output_list = []\n",
    "            for _ in range(n_sample):\n",
    "                output, kl = model(input_ids, age_ids, segment_ids, posi_ids, attention_mask=attMask, labels=targets)\n",
    "                output_list.append(output.sample())\n",
    "            output_list = torch.sigmoid(torch.stack(output_list, dim=0))\n",
    "            output_list = torch.mean(output_list, dim=0)\n",
    "            \n",
    "\n",
    "            logits = output_list.cpu()\n",
    "            targets = targets.cpu()\n",
    "\n",
    "            y_label.append(targets)\n",
    "            y.append(logits)\n",
    "\n",
    "        y_label = torch.cat(y_label, dim=0)\n",
    "        y = torch.cat(y, dim=0)\n",
    "\n",
    "        tempprc, auc = precision_test(y.view(-1), y_label.view(-1))\n",
    "        return tempprc, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t|step: 0\t|Loss: 272.72628125\n",
      "epoch: 0\t|step: 500\t|Loss: 275.8788877673149\n",
      "epoch: 0\t|step: 1000\t|Loss: 3.2125873761177064\n",
      "epoch: 0\t|step: 1500\t|Loss: 2.92262318944931\n",
      "epoch: 0\t|step: 2000\t|Loss: 2.77904425573349\n",
      "epoch: 0\t|step: 2500\t|Loss: 2.661195772886276\n",
      "epoch: 0\t|step: 3000\t|Loss: 2.5114359769821166\n",
      "epoch: 0\t|step: 3500\t|Loss: 2.4358776679039003\n",
      "epoch: 0\t|step: 4000\t|Loss: 2.389394411087036\n",
      "epoch: 0\t|step: 4500\t|Loss: 2.274125104188919\n",
      "epoch: 0\t|step: 5000\t|Loss: 2.196348573446274\n",
      "epoch: 0\t|step: 5500\t|Loss: 2.110581315279007\n",
      "epoch: 0\t|step: 6000\t|Loss: 2.0474341073036193\n",
      "epoch: 0\t|step: 6500\t|Loss: 1.9849241609573365\n",
      "epoch: 0\t|step: 7000\t|Loss: 1.8827027394771576\n",
      "epoch: 0\t|step: 7500\t|Loss: 1.8232688717842103\n",
      "epoch: 0\t|step: 8000\t|Loss: 1.7440813524723053\n",
      "epoch: 0\t|step: 8500\t|Loss: 1.663948002576828\n",
      "epoch: 0\t|step: 9000\t|Loss: 1.5636728065013885\n",
      "epoch: 0\t|step: 9500\t|Loss: 1.453490492105484\n",
      "epoch: 0\t|step: 10000\t|Loss: 1.34150514960289\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.11391706855262748, auc 0.49910808350800184\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t|step: 0\t|Loss: 262.974125\n",
      "epoch: 1\t|step: 500\t|Loss: 263.78710243773463\n",
      "epoch: 1\t|step: 1000\t|Loss: 1.1308924112319947\n",
      "epoch: 1\t|step: 1500\t|Loss: 1.0346596376895905\n",
      "epoch: 1\t|step: 2000\t|Loss: 0.9694269263744354\n",
      "epoch: 1\t|step: 2500\t|Loss: 0.9086201196908951\n",
      "epoch: 1\t|step: 3000\t|Loss: 0.8598622806072235\n",
      "epoch: 1\t|step: 3500\t|Loss: 0.8196964362859726\n",
      "epoch: 1\t|step: 4000\t|Loss: 0.772802645623684\n",
      "epoch: 1\t|step: 4500\t|Loss: 0.7547032395601273\n",
      "epoch: 1\t|step: 5000\t|Loss: 0.7263716861605645\n",
      "epoch: 1\t|step: 5500\t|Loss: 0.697739095389843\n",
      "epoch: 1\t|step: 6000\t|Loss: 0.6794009778499603\n",
      "epoch: 1\t|step: 6500\t|Loss: 0.6530702992677688\n",
      "epoch: 1\t|step: 7000\t|Loss: 0.6384117841124535\n",
      "epoch: 1\t|step: 7500\t|Loss: 0.6121404565572739\n",
      "epoch: 1\t|step: 8000\t|Loss: 0.6074002035260201\n",
      "epoch: 1\t|step: 8500\t|Loss: 0.5949568788409233\n",
      "epoch: 1\t|step: 9000\t|Loss: 0.5758595158457755\n",
      "epoch: 1\t|step: 9500\t|Loss: 0.5666580753922462\n",
      "epoch: 1\t|step: 10000\t|Loss: 0.5603759002685547\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.11393098280567433, auc 0.4977509219291882\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t|step: 0\t|Loss: 253.442734375\n",
      "epoch: 2\t|step: 500\t|Loss: 253.0548564377427\n",
      "epoch: 2\t|step: 1000\t|Loss: 0.5391724845767021\n",
      "epoch: 2\t|step: 1500\t|Loss: 0.5322431366443634\n",
      "epoch: 2\t|step: 2000\t|Loss: 0.5325505960583686\n",
      "epoch: 2\t|step: 2500\t|Loss: 0.5287239843010902\n",
      "epoch: 2\t|step: 3000\t|Loss: 0.5207391868233681\n",
      "epoch: 2\t|step: 3500\t|Loss: 0.5219530236721038\n",
      "epoch: 2\t|step: 4000\t|Loss: 0.5136297234296798\n",
      "epoch: 2\t|step: 4500\t|Loss: 0.5237939133644104\n",
      "epoch: 2\t|step: 5000\t|Loss: 0.5149335306286812\n",
      "epoch: 2\t|step: 5500\t|Loss: 0.5117583910822868\n",
      "epoch: 2\t|step: 6000\t|Loss: 0.5124944954514503\n",
      "epoch: 2\t|step: 6500\t|Loss: 0.5076859110593795\n",
      "epoch: 2\t|step: 7000\t|Loss: 0.508415765762329\n",
      "epoch: 2\t|step: 7500\t|Loss: 0.5048613274097442\n",
      "epoch: 2\t|step: 8000\t|Loss: 0.5056211071014405\n",
      "epoch: 2\t|step: 8500\t|Loss: 0.5031824799776077\n",
      "epoch: 2\t|step: 9000\t|Loss: 0.5047774815559387\n",
      "epoch: 2\t|step: 9500\t|Loss: 0.4990263242125511\n",
      "epoch: 2\t|step: 10000\t|Loss: 0.5028719906806945\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.1278442888825066, auc 0.5229837002729312\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 3\t|step: 0\t|Loss: 243.292625\n",
      "epoch: 3\t|step: 500\t|Loss: 243.65214891815185\n",
      "epoch: 3\t|step: 1000\t|Loss: 0.5029152331352233\n",
      "epoch: 3\t|step: 1500\t|Loss: 0.49309352004528045\n",
      "epoch: 3\t|step: 2000\t|Loss: 0.4924054372906685\n",
      "epoch: 3\t|step: 2500\t|Loss: 0.4848059737086296\n",
      "epoch: 3\t|step: 3000\t|Loss: 0.48434913688898085\n",
      "epoch: 3\t|step: 3500\t|Loss: 0.4695749419331551\n",
      "epoch: 3\t|step: 4000\t|Loss: 0.4643984531164169\n",
      "epoch: 3\t|step: 4500\t|Loss: 0.45175521266460417\n",
      "epoch: 3\t|step: 5000\t|Loss: 0.45188156628608706\n",
      "epoch: 3\t|step: 5500\t|Loss: 0.4435955756306648\n",
      "epoch: 3\t|step: 6000\t|Loss: 0.4348707223534584\n",
      "epoch: 3\t|step: 6500\t|Loss: 0.4379746337234974\n",
      "epoch: 3\t|step: 7000\t|Loss: 0.43162555539608\n",
      "epoch: 3\t|step: 7500\t|Loss: 0.4279633809626102\n",
      "epoch: 3\t|step: 8000\t|Loss: 0.42776655504107475\n",
      "epoch: 3\t|step: 8500\t|Loss: 0.41897099670767785\n",
      "epoch: 3\t|step: 9000\t|Loss: 0.41636475414037705\n",
      "epoch: 3\t|step: 9500\t|Loss: 0.41968946090340614\n",
      "epoch: 3\t|step: 10000\t|Loss: 0.41535857030749324\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.43738744385943734, auc 0.7938213059785897\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 4\t|step: 0\t|Loss: 235.406015625\n",
      "epoch: 4\t|step: 500\t|Loss: 236.20596469461918\n",
      "epoch: 4\t|step: 1000\t|Loss: 0.42168176084756853\n",
      "epoch: 4\t|step: 1500\t|Loss: 0.4109768806397915\n",
      "epoch: 4\t|step: 2000\t|Loss: 0.4090682602524757\n",
      "epoch: 4\t|step: 2500\t|Loss: 0.41225975382328034\n",
      "epoch: 4\t|step: 3000\t|Loss: 0.4086876408755779\n",
      "epoch: 4\t|step: 3500\t|Loss: 0.40792578318715095\n",
      "epoch: 4\t|step: 4000\t|Loss: 0.40331425485014916\n",
      "epoch: 4\t|step: 4500\t|Loss: 0.4085676416158676\n",
      "epoch: 4\t|step: 5000\t|Loss: 0.4001035315990448\n",
      "epoch: 4\t|step: 5500\t|Loss: 0.3996843865811825\n",
      "epoch: 4\t|step: 6000\t|Loss: 0.40069387677311896\n",
      "epoch: 4\t|step: 6500\t|Loss: 0.4053044219315052\n",
      "epoch: 4\t|step: 7000\t|Loss: 0.39996291279792784\n",
      "epoch: 4\t|step: 7500\t|Loss: 0.3938192603290081\n",
      "epoch: 4\t|step: 8000\t|Loss: 0.4005629546940327\n",
      "epoch: 4\t|step: 8500\t|Loss: 0.3958570834994316\n",
      "epoch: 4\t|step: 9000\t|Loss: 0.39352473989129066\n",
      "epoch: 4\t|step: 9500\t|Loss: 0.3968906814455986\n",
      "epoch: 4\t|step: 10000\t|Loss: 0.40006368482112886\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.47469839960037485, auc 0.8051801135208285\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 5\t|step: 0\t|Loss: 233.231015625\n",
      "epoch: 5\t|step: 500\t|Loss: 232.59215222150087\n",
      "epoch: 5\t|step: 1000\t|Loss: 0.39063805639743804\n",
      "epoch: 5\t|step: 1500\t|Loss: 0.3930099436342716\n",
      "epoch: 5\t|step: 2000\t|Loss: 0.3940376757085323\n",
      "epoch: 5\t|step: 2500\t|Loss: 0.39406470358371737\n",
      "epoch: 5\t|step: 3000\t|Loss: 0.39659191957116124\n",
      "epoch: 5\t|step: 3500\t|Loss: 0.3904624898135662\n",
      "epoch: 5\t|step: 4000\t|Loss: 0.3949914426505566\n",
      "epoch: 5\t|step: 4500\t|Loss: 0.38888056948781013\n",
      "epoch: 5\t|step: 5000\t|Loss: 0.3874321456253529\n",
      "epoch: 5\t|step: 5500\t|Loss: 0.388522696018219\n",
      "epoch: 5\t|step: 6000\t|Loss: 0.3877911484241486\n",
      "epoch: 5\t|step: 6500\t|Loss: 0.38155064594745636\n",
      "epoch: 5\t|step: 7000\t|Loss: 0.38886989068984984\n",
      "epoch: 5\t|step: 7500\t|Loss: 0.38759513542056084\n",
      "epoch: 5\t|step: 8000\t|Loss: 0.38527824681997297\n",
      "epoch: 5\t|step: 8500\t|Loss: 0.3907537736296654\n",
      "epoch: 5\t|step: 9000\t|Loss: 0.38363989666104314\n",
      "epoch: 5\t|step: 9500\t|Loss: 0.3803080309331417\n",
      "epoch: 5\t|step: 10000\t|Loss: 0.38547572699189186\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.4800512755565151, auc 0.806554785145146\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 6\t|step: 0\t|Loss: 231.425453125\n",
      "epoch: 6\t|step: 500\t|Loss: 230.02872089767456\n",
      "epoch: 6\t|step: 1000\t|Loss: 0.3910154820680618\n",
      "epoch: 6\t|step: 1500\t|Loss: 0.3818400334119797\n",
      "epoch: 6\t|step: 2000\t|Loss: 0.38675616565346715\n",
      "epoch: 6\t|step: 2500\t|Loss: 0.3876108581870794\n",
      "epoch: 6\t|step: 3000\t|Loss: 0.38222838392853736\n",
      "epoch: 6\t|step: 3500\t|Loss: 0.3883307058811188\n",
      "epoch: 6\t|step: 4000\t|Loss: 0.38613472148776057\n",
      "epoch: 6\t|step: 4500\t|Loss: 0.38553823783993724\n",
      "epoch: 6\t|step: 5000\t|Loss: 0.38878434696793557\n",
      "epoch: 6\t|step: 5500\t|Loss: 0.3844945864379406\n",
      "epoch: 6\t|step: 6000\t|Loss: 0.38753399685025214\n",
      "epoch: 6\t|step: 6500\t|Loss: 0.38194488593935966\n",
      "epoch: 6\t|step: 7000\t|Loss: 0.3837256228327751\n",
      "epoch: 6\t|step: 7500\t|Loss: 0.38510975435376166\n",
      "epoch: 6\t|step: 8000\t|Loss: 0.3838305706977844\n",
      "epoch: 6\t|step: 8500\t|Loss: 0.3854572285413742\n",
      "epoch: 6\t|step: 9000\t|Loss: 0.3808026672005653\n",
      "epoch: 6\t|step: 9500\t|Loss: 0.37993233945965765\n",
      "epoch: 6\t|step: 10000\t|Loss: 0.38366940394043925\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.48470292752241745, auc 0.808541723430497\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 7\t|step: 0\t|Loss: 227.335828125\n",
      "epoch: 7\t|step: 500\t|Loss: 228.49115453234316\n",
      "epoch: 7\t|step: 1000\t|Loss: 0.38131246969103816\n",
      "epoch: 7\t|step: 1500\t|Loss: 0.38373411867022517\n",
      "epoch: 7\t|step: 2000\t|Loss: 0.38117193749547007\n",
      "epoch: 7\t|step: 2500\t|Loss: 0.3803546722233295\n",
      "epoch: 7\t|step: 3000\t|Loss: 0.38148219329118727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7\t|step: 3500\t|Loss: 0.37837787160277364\n",
      "epoch: 7\t|step: 4000\t|Loss: 0.38151109585165977\n",
      "epoch: 7\t|step: 4500\t|Loss: 0.3838764998316765\n",
      "epoch: 7\t|step: 5000\t|Loss: 0.3844814270734787\n",
      "epoch: 7\t|step: 5500\t|Loss: 0.3781376941204071\n",
      "epoch: 7\t|step: 6000\t|Loss: 0.37471042904257773\n",
      "epoch: 7\t|step: 6500\t|Loss: 0.3864234608113766\n",
      "epoch: 7\t|step: 7500\t|Loss: 0.3801213616430759\n",
      "epoch: 7\t|step: 8000\t|Loss: 0.3829804929494858\n",
      "epoch: 7\t|step: 8500\t|Loss: 0.37861586594581603\n",
      "epoch: 7\t|step: 9000\t|Loss: 0.3775443839430809\n",
      "epoch: 7\t|step: 9500\t|Loss: 0.3810398139357567\n",
      "epoch: 7\t|step: 10000\t|Loss: 0.37937067133188246\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.4884350937296664, auc 0.8105978006310559\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 8\t|step: 0\t|Loss: 224.613203125\n",
      "epoch: 8\t|step: 500\t|Loss: 222.74926220688224\n",
      "epoch: 8\t|step: 1000\t|Loss: 0.3815859800875187\n",
      "epoch: 8\t|step: 1500\t|Loss: 0.37912702491879463\n",
      "epoch: 8\t|step: 2000\t|Loss: 0.3792658258676529\n",
      "epoch: 8\t|step: 2500\t|Loss: 0.3785090954899788\n",
      "epoch: 8\t|step: 3000\t|Loss: 0.3774855902493\n",
      "epoch: 8\t|step: 3500\t|Loss: 0.3788210234642029\n",
      "epoch: 8\t|step: 4000\t|Loss: 0.3843511504232883\n",
      "epoch: 8\t|step: 4500\t|Loss: 0.38332599821686747\n",
      "epoch: 8\t|step: 5000\t|Loss: 0.3779799690544605\n",
      "epoch: 8\t|step: 5500\t|Loss: 0.37950019708275795\n",
      "epoch: 8\t|step: 6000\t|Loss: 0.37511214715242386\n",
      "epoch: 8\t|step: 6500\t|Loss: 0.37845762461423876\n",
      "epoch: 8\t|step: 7000\t|Loss: 0.3754104272425175\n",
      "epoch: 8\t|step: 7500\t|Loss: 0.3744861163198948\n",
      "epoch: 8\t|step: 8000\t|Loss: 0.3857405840456486\n",
      "epoch: 8\t|step: 8500\t|Loss: 0.3842889482080936\n",
      "epoch: 8\t|step: 9000\t|Loss: 0.3806419771313667\n",
      "epoch: 8\t|step: 9500\t|Loss: 0.3872622270286083\n",
      "epoch: 8\t|step: 10000\t|Loss: 0.3746616738140583\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.4834195114446641, auc 0.8119108931876533\n",
      "epoch: 9\t|step: 0\t|Loss: 219.791875\n",
      "epoch: 9\t|step: 500\t|Loss: 221.40281381872296\n",
      "epoch: 9\t|step: 1000\t|Loss: 0.37906452763080595\n",
      "epoch: 9\t|step: 1500\t|Loss: 0.3780546073615551\n",
      "epoch: 9\t|step: 2000\t|Loss: 0.3749253793954849\n",
      "epoch: 9\t|step: 2500\t|Loss: 0.3738497518599033\n",
      "epoch: 9\t|step: 3000\t|Loss: 0.3796249076426029\n",
      "epoch: 9\t|step: 3500\t|Loss: 0.38212286806106566\n",
      "epoch: 9\t|step: 4000\t|Loss: 0.37833815467357634\n",
      "epoch: 9\t|step: 4500\t|Loss: 0.3742449977695942\n",
      "epoch: 9\t|step: 5000\t|Loss: 0.37592102733254434\n",
      "epoch: 9\t|step: 5500\t|Loss: 0.37774286064505574\n",
      "epoch: 9\t|step: 6000\t|Loss: 0.3739187132418156\n",
      "epoch: 9\t|step: 6500\t|Loss: 0.3778886388540268\n",
      "epoch: 9\t|step: 7000\t|Loss: 0.37844413104653357\n",
      "epoch: 9\t|step: 7500\t|Loss: 0.3777818490564823\n",
      "epoch: 9\t|step: 8000\t|Loss: 0.37618330559134483\n",
      "epoch: 9\t|step: 8500\t|Loss: 0.3750649507045746\n",
      "epoch: 9\t|step: 9000\t|Loss: 0.3845367153584957\n",
      "epoch: 9\t|step: 9500\t|Loss: 0.3795314948260784\n",
      "epoch: 9\t|step: 10000\t|Loss: 0.3740089602768421\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.49106490437666417, auc 0.8135927516095465\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 10\t|step: 0\t|Loss: 214.200578125\n",
      "epoch: 10\t|step: 500\t|Loss: 218.0449952867627\n",
      "epoch: 10\t|step: 1000\t|Loss: 0.38154168412089345\n",
      "epoch: 10\t|step: 1500\t|Loss: 0.3792193059027195\n",
      "epoch: 10\t|step: 2000\t|Loss: 0.379861118376255\n",
      "epoch: 10\t|step: 2500\t|Loss: 0.3810019728541374\n",
      "epoch: 10\t|step: 3000\t|Loss: 0.3782433200478554\n",
      "epoch: 10\t|step: 3500\t|Loss: 0.3790220992565155\n",
      "epoch: 10\t|step: 4000\t|Loss: 0.3711406847536564\n",
      "epoch: 10\t|step: 4500\t|Loss: 0.37286453023552896\n",
      "epoch: 10\t|step: 5000\t|Loss: 0.3793421061635017\n",
      "epoch: 10\t|step: 5500\t|Loss: 0.37212841454148293\n",
      "epoch: 10\t|step: 6000\t|Loss: 0.37723103231191635\n",
      "epoch: 10\t|step: 6500\t|Loss: 0.3749105181992054\n",
      "epoch: 10\t|step: 7000\t|Loss: 0.37702460154891015\n",
      "epoch: 10\t|step: 7500\t|Loss: 0.3801511926352978\n",
      "epoch: 10\t|step: 8000\t|Loss: 0.36856142249703405\n",
      "epoch: 10\t|step: 8500\t|Loss: 0.36983779537677763\n",
      "epoch: 10\t|step: 9000\t|Loss: 0.37367565536499026\n",
      "epoch: 10\t|step: 9500\t|Loss: 0.3825533570945263\n",
      "epoch: 10\t|step: 10000\t|Loss: 0.3738591336607933\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.496638435376074, auc 0.8155741746850005\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 11\t|step: 0\t|Loss: 217.51284375\n",
      "epoch: 11\t|step: 500\t|Loss: 214.06889621543885\n",
      "epoch: 11\t|step: 1000\t|Loss: 0.374421758800745\n",
      "epoch: 11\t|step: 1500\t|Loss: 0.3787836236655712\n",
      "epoch: 11\t|step: 2000\t|Loss: 0.37639050844311717\n",
      "epoch: 11\t|step: 2500\t|Loss: 0.3772812383174896\n",
      "epoch: 11\t|step: 3000\t|Loss: 0.3743519656062126\n",
      "epoch: 11\t|step: 3500\t|Loss: 0.3771553583741188\n",
      "epoch: 11\t|step: 4000\t|Loss: 0.3747146656513214\n",
      "epoch: 11\t|step: 4500\t|Loss: 0.3705565414428711\n",
      "epoch: 11\t|step: 5000\t|Loss: 0.37108232161402704\n",
      "epoch: 11\t|step: 5500\t|Loss: 0.37138692384958266\n",
      "epoch: 11\t|step: 6000\t|Loss: 0.37657618778944013\n",
      "epoch: 11\t|step: 6500\t|Loss: 0.37399119037389755\n",
      "epoch: 11\t|step: 7000\t|Loss: 0.3786509509086609\n",
      "epoch: 11\t|step: 7500\t|Loss: 0.37641526213288307\n",
      "epoch: 11\t|step: 8000\t|Loss: 0.372664300262928\n",
      "epoch: 11\t|step: 8500\t|Loss: 0.37436772567033766\n",
      "epoch: 11\t|step: 9000\t|Loss: 0.37359500432014464\n",
      "epoch: 11\t|step: 9500\t|Loss: 0.3717535364329815\n",
      "epoch: 11\t|step: 10000\t|Loss: 0.3748534948825836\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.4960816016964063, auc 0.8164136360586207\n",
      "epoch: 12\t|step: 0\t|Loss: 213.11946875\n",
      "epoch: 12\t|step: 500\t|Loss: 213.26230123665928\n",
      "epoch: 12\t|step: 1000\t|Loss: 0.3763948944807053\n",
      "epoch: 12\t|step: 1500\t|Loss: 0.36611190259456633\n",
      "epoch: 12\t|step: 2000\t|Loss: 0.3759910942018032\n",
      "epoch: 12\t|step: 2500\t|Loss: 0.37500247645378115\n",
      "epoch: 12\t|step: 3000\t|Loss: 0.3803699564039707\n",
      "epoch: 12\t|step: 3500\t|Loss: 0.37018693748116493\n",
      "epoch: 12\t|step: 4000\t|Loss: 0.3735394712984562\n",
      "epoch: 12\t|step: 4500\t|Loss: 0.3817543762028217\n",
      "epoch: 12\t|step: 5000\t|Loss: 0.37346116894483566\n",
      "epoch: 12\t|step: 5500\t|Loss: 0.37386869341135026\n",
      "epoch: 12\t|step: 6000\t|Loss: 0.37109613502025607\n",
      "epoch: 12\t|step: 6500\t|Loss: 0.37238564679026603\n",
      "epoch: 12\t|step: 7000\t|Loss: 0.3729571624100208\n",
      "epoch: 12\t|step: 7500\t|Loss: 0.3735528910756111\n",
      "epoch: 12\t|step: 8000\t|Loss: 0.3775893260538578\n",
      "epoch: 12\t|step: 8500\t|Loss: 0.36905783700942996\n",
      "epoch: 12\t|step: 9000\t|Loss: 0.3754404501914978\n",
      "epoch: 12\t|step: 9500\t|Loss: 0.3776621814370155\n",
      "epoch: 12\t|step: 10000\t|Loss: 0.37112308070063593\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.5019471471970224, auc 0.8187187299861423\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 13\t|step: 0\t|Loss: 215.37184375\n",
      "epoch: 13\t|step: 500\t|Loss: 211.09127137058974\n",
      "epoch: 13\t|step: 1000\t|Loss: 0.3711613002717495\n",
      "epoch: 13\t|step: 1500\t|Loss: 0.374548692047596\n",
      "epoch: 13\t|step: 2000\t|Loss: 0.37395801922678945\n",
      "epoch: 13\t|step: 2500\t|Loss: 0.3683261444270611\n",
      "epoch: 13\t|step: 3000\t|Loss: 0.3735700170099735\n",
      "epoch: 13\t|step: 3500\t|Loss: 0.3766464518904686\n",
      "epoch: 13\t|step: 4000\t|Loss: 0.3783089809119701\n",
      "epoch: 13\t|step: 4500\t|Loss: 0.36969286352396014\n",
      "epoch: 13\t|step: 5000\t|Loss: 0.3795103219747543\n",
      "epoch: 13\t|step: 5500\t|Loss: 0.3668604492545128\n",
      "epoch: 13\t|step: 6000\t|Loss: 0.370134083211422\n",
      "epoch: 13\t|step: 6500\t|Loss: 0.37755219694972036\n",
      "epoch: 13\t|step: 7000\t|Loss: 0.3749451178312302\n",
      "epoch: 13\t|step: 7500\t|Loss: 0.3681431823372841\n",
      "epoch: 13\t|step: 8000\t|Loss: 0.37161851769685744\n",
      "epoch: 13\t|step: 8500\t|Loss: 0.37992069801688194\n",
      "epoch: 13\t|step: 9000\t|Loss: 0.3740648020505905\n",
      "epoch: 13\t|step: 9500\t|Loss: 0.37507132908701896\n",
      "epoch: 13\t|step: 10000\t|Loss: 0.37328795552253724\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.5009378988619776, auc 0.8186182207695044\n",
      "epoch: 14\t|step: 0\t|Loss: 211.3845\n",
      "epoch: 14\t|step: 500\t|Loss: 208.7846605722606\n",
      "epoch: 14\t|step: 1000\t|Loss: 0.3665151470005512\n",
      "epoch: 14\t|step: 1500\t|Loss: 0.3702891773581505\n",
      "epoch: 14\t|step: 2000\t|Loss: 0.3715300213396549\n",
      "epoch: 14\t|step: 2500\t|Loss: 0.37110031887888906\n",
      "epoch: 14\t|step: 3000\t|Loss: 0.3726196338534355\n",
      "epoch: 14\t|step: 3500\t|Loss: 0.37128220197558404\n",
      "epoch: 14\t|step: 4000\t|Loss: 0.36860081338882444\n",
      "epoch: 14\t|step: 4500\t|Loss: 0.37351529717445375\n",
      "epoch: 14\t|step: 5000\t|Loss: 0.3772844477295876\n",
      "epoch: 14\t|step: 5500\t|Loss: 0.36997812408208847\n",
      "epoch: 14\t|step: 6000\t|Loss: 0.3746074237525463\n",
      "epoch: 14\t|step: 6500\t|Loss: 0.3716274389624596\n",
      "epoch: 14\t|step: 7000\t|Loss: 0.3749353821873665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14\t|step: 7500\t|Loss: 0.36755220010876655\n",
      "epoch: 14\t|step: 8000\t|Loss: 0.3739155548512936\n",
      "epoch: 14\t|step: 8500\t|Loss: 0.3791551950871944\n",
      "epoch: 14\t|step: 9000\t|Loss: 0.37198037579655646\n",
      "epoch: 14\t|step: 9500\t|Loss: 0.3781193730533123\n",
      "epoch: 14\t|step: 10000\t|Loss: 0.3683205246925354\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.5037300420563781, auc 0.8193325043542793\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 15\t|step: 0\t|Loss: 205.147515625\n",
      "epoch: 15\t|step: 500\t|Loss: 208.1269298503101\n",
      "epoch: 15\t|step: 1000\t|Loss: 0.3723144649863243\n",
      "epoch: 15\t|step: 1500\t|Loss: 0.3743628377318382\n",
      "epoch: 15\t|step: 2000\t|Loss: 0.37190609860420226\n",
      "epoch: 15\t|step: 2500\t|Loss: 0.3741000574827194\n",
      "epoch: 15\t|step: 3000\t|Loss: 0.3742572244107723\n",
      "epoch: 15\t|step: 3500\t|Loss: 0.36793233680725096\n",
      "epoch: 15\t|step: 4000\t|Loss: 0.36567194229364397\n",
      "epoch: 15\t|step: 4500\t|Loss: 0.37303970316052437\n",
      "epoch: 15\t|step: 5000\t|Loss: 0.3695319775044918\n",
      "epoch: 15\t|step: 5500\t|Loss: 0.37039826619625094\n",
      "epoch: 15\t|step: 6500\t|Loss: 0.37670880717039107\n",
      "epoch: 15\t|step: 7000\t|Loss: 0.3708115493953228\n",
      "epoch: 15\t|step: 7500\t|Loss: 0.375717268794775\n",
      "epoch: 15\t|step: 8000\t|Loss: 0.36984501558542254\n",
      "epoch: 15\t|step: 8500\t|Loss: 0.37301526668667795\n",
      "epoch: 15\t|step: 9000\t|Loss: 0.3751355320513248\n",
      "epoch: 15\t|step: 9500\t|Loss: 0.36535840573906897\n",
      "epoch: 15\t|step: 10000\t|Loss: 0.36764073970913885\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.503256103511375, auc 0.8197907281634766\n",
      "epoch: 16\t|step: 0\t|Loss: 203.308203125\n",
      "epoch: 16\t|step: 500\t|Loss: 205.00178642475606\n",
      "epoch: 16\t|step: 1000\t|Loss: 0.36618928146362306\n",
      "epoch: 16\t|step: 1500\t|Loss: 0.37729025012254713\n",
      "epoch: 16\t|step: 2000\t|Loss: 0.3741488365530968\n",
      "epoch: 16\t|step: 2500\t|Loss: 0.3705315546691418\n",
      "epoch: 16\t|step: 3000\t|Loss: 0.3704484446942806\n",
      "epoch: 16\t|step: 3500\t|Loss: 0.36670270982384684\n",
      "epoch: 16\t|step: 4000\t|Loss: 0.37032333508133886\n",
      "epoch: 16\t|step: 4500\t|Loss: 0.3694935350716114\n",
      "epoch: 16\t|step: 5000\t|Loss: 0.3698493074476719\n",
      "epoch: 16\t|step: 5500\t|Loss: 0.37254727658629416\n",
      "epoch: 16\t|step: 6000\t|Loss: 0.36906315433979037\n",
      "epoch: 16\t|step: 6500\t|Loss: 0.3759562739133835\n",
      "epoch: 16\t|step: 7000\t|Loss: 0.3730529725551605\n",
      "epoch: 16\t|step: 7500\t|Loss: 0.3766839935779572\n",
      "epoch: 16\t|step: 8000\t|Loss: 0.3695154578089714\n",
      "epoch: 16\t|step: 8500\t|Loss: 0.36966539627313616\n",
      "epoch: 16\t|step: 9000\t|Loss: 0.36634794586896896\n",
      "epoch: 16\t|step: 9500\t|Loss: 0.37466620302200315\n",
      "epoch: 16\t|step: 10000\t|Loss: 0.3737538864016533\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.5031907659275306, auc 0.8198407352250698\n",
      "epoch: 17\t|step: 0\t|Loss: 200.109078125\n",
      "epoch: 17\t|step: 500\t|Loss: 205.43066845700145\n",
      "epoch: 17\t|step: 1000\t|Loss: 0.3744770076274872\n",
      "epoch: 17\t|step: 1500\t|Loss: 0.3711176500022411\n",
      "epoch: 17\t|step: 2000\t|Loss: 0.37103219982981683\n",
      "epoch: 17\t|step: 2500\t|Loss: 0.3758496562838554\n",
      "epoch: 17\t|step: 3000\t|Loss: 0.36719138666987416\n",
      "epoch: 17\t|step: 3500\t|Loss: 0.37628870141506193\n",
      "epoch: 17\t|step: 4000\t|Loss: 0.369213537633419\n",
      "epoch: 17\t|step: 4500\t|Loss: 0.3709746224880219\n",
      "epoch: 17\t|step: 5000\t|Loss: 0.3736638768017292\n",
      "epoch: 17\t|step: 5500\t|Loss: 0.3686898391842842\n",
      "epoch: 17\t|step: 6000\t|Loss: 0.3694709797501564\n",
      "epoch: 17\t|step: 6500\t|Loss: 0.37061689886450766\n",
      "epoch: 17\t|step: 7000\t|Loss: 0.3642443028688431\n",
      "epoch: 17\t|step: 7500\t|Loss: 0.37343549036979673\n",
      "epoch: 17\t|step: 8000\t|Loss: 0.3716185637116432\n",
      "epoch: 17\t|step: 8500\t|Loss: 0.3705548865199089\n",
      "epoch: 17\t|step: 9000\t|Loss: 0.3742529633641243\n",
      "epoch: 17\t|step: 9500\t|Loss: 0.3703527347445488\n",
      "epoch: 17\t|step: 10000\t|Loss: 0.3745628456175327\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.5061075252154291, auc 0.8209378916253136\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 18\t|step: 0\t|Loss: 201.682078125\n",
      "epoch: 18\t|step: 500\t|Loss: 202.48660450446604\n",
      "epoch: 18\t|step: 1000\t|Loss: 0.3732176565825939\n",
      "epoch: 18\t|step: 1500\t|Loss: 0.37021704199910166\n",
      "epoch: 18\t|step: 2000\t|Loss: 0.37428385215997695\n",
      "epoch: 18\t|step: 2500\t|Loss: 0.37260242664814\n",
      "epoch: 18\t|step: 3000\t|Loss: 0.36872653687000273\n",
      "epoch: 18\t|step: 3500\t|Loss: 0.3675373492240906\n",
      "epoch: 18\t|step: 4000\t|Loss: 0.3695471707582474\n",
      "epoch: 18\t|step: 4500\t|Loss: 0.3659640766084194\n",
      "epoch: 18\t|step: 5000\t|Loss: 0.3688915882706642\n",
      "epoch: 18\t|step: 5500\t|Loss: 0.36901137989759447\n",
      "epoch: 18\t|step: 6000\t|Loss: 0.3699689211249351\n",
      "epoch: 18\t|step: 6500\t|Loss: 0.37338046231865885\n",
      "epoch: 18\t|step: 7000\t|Loss: 0.36589291405677793\n",
      "epoch: 18\t|step: 7500\t|Loss: 0.37234656408429145\n",
      "epoch: 18\t|step: 8000\t|Loss: 0.37048517289757726\n",
      "epoch: 18\t|step: 8500\t|Loss: 0.3708761125802994\n",
      "epoch: 18\t|step: 9000\t|Loss: 0.36660305655002595\n",
      "epoch: 18\t|step: 9500\t|Loss: 0.3767964576482773\n",
      "epoch: 18\t|step: 10000\t|Loss: 0.37584138149023055\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.5050200078000711, auc 0.8213733995633357\n",
      "epoch: 19\t|step: 0\t|Loss: 202.221203125\n",
      "epoch: 19\t|step: 500\t|Loss: 198.93549161577224\n"
     ]
    }
   ],
   "source": [
    "best_pre = 0\n",
    "for e in range(20):\n",
    "    train(e, trainData, model, likelihood,optim, len(trainData), \"Blundell\")\n",
    "#     auc_train, time_cost_train = evaluation(trainData, model)\n",
    "    auc_test, auc = evaluation(testData, model, likelihood, n_sample=20)\n",
    "    print('test precision: {}, auc {}'.format(auc_test, auc))\n",
    "    if auc_test >best_pre:\n",
    "        # Save a trained model\n",
    "        output_model_file = os.path.join(trainConfig.output_dir, trainConfig.best_name)\n",
    "        create_folder(trainConfig.output_dir)\n",
    "        save_model(output_model_file, model)\n",
    "        best_pre = auc_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3)",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
