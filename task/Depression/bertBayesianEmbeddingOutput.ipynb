{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.insert(0, '/home/yikuan/project/Code/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.pytorch import save_model\n",
    "from common.common import create_folder,load_obj\n",
    "from common.pytorch import load_model as torch_load_model\n",
    "import os\n",
    "import torch\n",
    "from ACM.model.utils.utils import age_vocab\n",
    "import pandas as pd\n",
    "import pytorch_pretrained_bert as Bert\n",
    "from ACM.dataLoader.HF import HF_data\n",
    "from ACM.model.bertBayesianEmbeddingOutput import BertHF\n",
    "from ACM.model.optimiser import adam\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertConfig(Bert.modeling.BertConfig):\n",
    "    def __init__(self, config):\n",
    "        super(BertConfig, self).__init__(\n",
    "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_hidden_layers=config.get('num_hidden_layers'),\n",
    "            num_attention_heads=config.get('num_attention_heads'),\n",
    "            intermediate_size=config.get('intermediate_size'),\n",
    "            hidden_act=config.get('hidden_act'),\n",
    "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
    "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
    "            max_position_embeddings=config.get('max_position_embeddings'),\n",
    "            initializer_range=config.get('initializer_range'),\n",
    "        )\n",
    "        self.seg_vocab_size = config.get('seg_vocab_size')\n",
    "        self.age_vocab_size = config.get('age_vocab_size')\n",
    "        self.num_labels = config.get('num_labels')\n",
    "\n",
    "\n",
    "class TrainConfig(object):\n",
    "    def __init__(self, config):\n",
    "        self.batch_size = config.get('batch_size')\n",
    "        self.use_cuda = config.get('use_cuda')\n",
    "        self.max_len_seq = config.get('max_len_seq')\n",
    "        self.train_loader_workers = config.get('train_loader_workers')\n",
    "        self.test_loader_workers = config.get('test_loader_workers')\n",
    "        self.device = config.get('device')\n",
    "        self.output_dir = config.get('output_dir')\n",
    "        self.output_name = config.get('output_name')\n",
    "        self.best_name = config.get('best_name')\n",
    "        self.num_samples = config.get('num_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_config = {\n",
    "    'vocab':'/home/yikuan/project/Code/ACM/data/Full_vocab',\n",
    "    'train': '/home/shared/yikuan/ACM/data/Diabetes/diabetes_clean_train.parquet',\n",
    "    'test': '/home/shared/yikuan/ACM/data/Diabetes/diabetes_clean_test.parquet'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_params = {\n",
    "    'max_seq_len': 256,\n",
    "    'max_age': 110,\n",
    "    'month': 1,\n",
    "    'age_symbol': None,\n",
    "}\n",
    "\n",
    "optim_param = {\n",
    "    'lr': 3e-5,\n",
    "    'warmup_proportion': 0.1,\n",
    "    'weight_decay': 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertVocab = load_obj(file_config['vocab'])\n",
    "ageVocab, _ = age_vocab(max_age=global_params['max_age'], mon=global_params['month'], symbol=global_params['age_symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = pd.read_parquet(file_config['train']).reset_index(drop=True)\n",
    "testData = pd.read_parquet(file_config['test']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    'batch_size': 64,\n",
    "    'use_cuda': True,\n",
    "    'max_len_seq': global_params['max_seq_len'],\n",
    "    'train_loader_workers': 3,\n",
    "    'test_loader_workers': 3,\n",
    "    'num_samples': 10,\n",
    "    'device': 'cuda:1',\n",
    "    'output_dir': '/home/shared/yikuan/ACM/model/Diabetes',\n",
    "    'output_name': 'behrtBayesianEmbeddingsOutput.bin',\n",
    "    'best_name': 'behrtBayesianEmbeddingsOutput_best.bin',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainConfig = TrainConfig(train_params)\n",
    "\n",
    "data_set = HF_data(trainConfig, trainData, testData, BertVocab['token2idx'], ageVocab, code='code', age='age')\n",
    "\n",
    "trainData = data_set.get_weighted_sample_train(4)\n",
    "testData = data_set.get_test_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_param = {\n",
    "    'vocab_size': len(BertVocab['token2idx'].keys()),\n",
    "    'hidden_size': 150,\n",
    "    'num_hidden_layers': 4,\n",
    "    'num_attention_heads': 6,\n",
    "    'hidden_act': 'gelu',\n",
    "    'intermediate_size': 108,\n",
    "    'max_position_embeddings': global_params['max_seq_len'],\n",
    "    'seg_vocab_size': 2,\n",
    "    'age_vocab_size': len(ageVocab.keys()),\n",
    "    'prior_prec': 1e0,\n",
    "    'prec_init': 1e0,\n",
    "    'initializer_range': 0.02,\n",
    "    'num_labels': 1,\n",
    "    'hidden_dropout_prob': 0.29,\n",
    "    'attention_probs_dropout_prob': 0.38\n",
    "}\n",
    "\n",
    "feature_dict = {\n",
    "    'word': True,\n",
    "    'age': False,\n",
    "    'seg': False,\n",
    "    'norm': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelConfig = BertConfig(model_param)\n",
    "model = BertHF(modelConfig, num_labels=modelConfig.num_labels, n_dim=150, feature_dict=feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n"
     ]
    }
   ],
   "source": [
    "def load_model(path, model):\n",
    "    mapdict = {\n",
    "        'bert.embeddings.word_embeddings.weight_posterior.loc': 'bert.embeddings.word_embeddings.weight',\n",
    "#         'bert.embeddings.age_embeddings.weight': 'bert.embeddings.age_embeddings.weight_posterior.loc'\n",
    "    }\n",
    "    \n",
    "#     # load pretrained model and update weights\n",
    "    pretrained_dict = torch.load(path)\n",
    "#     model_dict = model.state_dict()\n",
    "#     # 1. filter out unnecessary keys\n",
    "# #     pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and k not in ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']}\n",
    "#     pretrained_dict = {k: v for k, v in pretrained_dict.items()}\n",
    "#     # 2. overwrite entries in the existing state dict\n",
    "#     model_dict.update(pretrained_dict)\n",
    "    \n",
    "#     pretrained_dict = {mapdict.get(k): v for k, v in pretrained_dict.items() if k in list(mapdict.keys())}\n",
    "    \n",
    "#     model_dict.update(pretrained_dict)\n",
    "    \n",
    "#     # 3. load the new state dict\n",
    "#     model.load_state_dict(model_dict)\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    for k,v in pretrained_dict.items():\n",
    "        if (k in model_dict) and (k not in ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']):\n",
    "            model_dict[k] = v\n",
    "    for k,v in mapdict.items():\n",
    "        model_dict[k] = pretrained_dict[v]\n",
    "    model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "# model = torch_load_model('/home/shared/yikuan/ACM/model/Diabetes/behrtBayesianEmbeddings_best.bin', model)\n",
    "model = load_model('/home/shared/yikuan/HF/MLM/PureICD_diag_med.bin', model)\n",
    "model = model.to(trainConfig.device)\n",
    "optim = adam(list(model.named_parameters()),config=optim_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(logits, label):\n",
    "    sig = nn.Sigmoid()\n",
    "    output=sig(logits)\n",
    "    label, output=label.cpu(), output.detach().cpu()\n",
    "    tempprc= average_precision_score(label.numpy(),output.numpy())\n",
    "    auc = roc_auc_score(label.numpy(),output.numpy())\n",
    "    return tempprc, auc\n",
    "\n",
    "def precision_test(prob, label):\n",
    "#     sig = nn.Sigmoid()\n",
    "#     output=sig(logits)\n",
    "    tempprc= average_precision_score(label.numpy(),prob.numpy())\n",
    "    auc = roc_auc_score(label.numpy(),prob.numpy())\n",
    "    return tempprc, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beta(batch_idx, m, beta_type):\n",
    "    if beta_type == \"Blundell\":\n",
    "        beta = 2 ** (m - (batch_idx + 1)) / (2 ** m - 1) \n",
    "    elif beta_type == \"Soenderby\":\n",
    "        beta = min(epoch / (num_epochs // 4), 1)\n",
    "    elif beta_type == \"Standard\":\n",
    "        beta = 1 / m \n",
    "    else:\n",
    "        beta = 0\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(e, trainload, model, optim, m, beta_type):\n",
    "    model.train()\n",
    "\n",
    "    tr_loss = 0\n",
    "    temp_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    cnt = 0\n",
    "    for step, batch in enumerate(trainload):\n",
    "        cnt += 1\n",
    "        beta = get_beta(step, m, beta_type)\n",
    "\n",
    "        age_ids, input_ids, posi_ids, segment_ids, attMask, targets, _, _ = batch\n",
    "\n",
    "        age_ids = age_ids.to(train_params['device'])\n",
    "        input_ids = input_ids.to(train_params['device'])\n",
    "        posi_ids = posi_ids.to(train_params['device'])\n",
    "        segment_ids = segment_ids.to(train_params['device'])\n",
    "        attMask = attMask.to(train_params['device'])\n",
    "        targets = targets.view(-1).to(train_params['device'])\n",
    "\n",
    "        loss, _ = model(input_ids, age_ids, segment_ids, posi_ids, attention_mask=attMask, labels=targets, beta=beta)\n",
    "        loss.backward()\n",
    "\n",
    "        temp_loss += loss.item()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            #             prec, a, b = precision(logits, targets)\n",
    "            #             print(\"epoch: {}\\t| Cnt: {}\\t| Loss: {}\\t| precision: {}\".format(e, cnt, temp_loss / 500, prec))\n",
    "            print(\"epoch: {}\\t|step: {}\\t|Loss: {}\".format(e, step, temp_loss / 500))\n",
    "            temp_loss = 0\n",
    "\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "    # Save a trained model\n",
    "    output_model_file = os.path.join(trainConfig.output_dir, trainConfig.output_name)\n",
    "    create_folder(trainConfig.output_dir)\n",
    "    save_model(output_model_file, model)\n",
    "\n",
    "\n",
    "def evaluation(testload, model):\n",
    "    model.eval()\n",
    "    y = []\n",
    "    y_label = []\n",
    "    loss_temp = 0\n",
    "    sig = nn.Sigmoid()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(testload):\n",
    "            age_ids, input_ids, posi_ids, segment_ids, attMask, targets, _, _ = batch\n",
    "\n",
    "            age_ids = age_ids.to(train_params['device'])\n",
    "            input_ids = input_ids.to(train_params['device'])\n",
    "            posi_ids = posi_ids.to(train_params['device'])\n",
    "            segment_ids = segment_ids.to(train_params['device'])\n",
    "            attMask = attMask.to(train_params['device'])\n",
    "            targets = targets.view(-1).to(train_params['device'])\n",
    "            \n",
    "            logits_prob = []\n",
    "            for i in range(trainConfig.num_samples):\n",
    "                logits =model(input_ids, age_ids, segment_ids, posi_ids, attention_mask=attMask, labels=None)\n",
    "                logits_prob.append(logits)\n",
    "            \n",
    "            logits_prob = torch.mean(sig(torch.stack(logits_prob, dim=1)), dim=1)\n",
    "            \n",
    "            # get mean of logits\n",
    "            \n",
    "            \n",
    "            targets = targets.cpu()\n",
    "\n",
    "            y_label.append(targets)\n",
    "            y.append(logits_prob.cpu())\n",
    "\n",
    "        y_label = torch.cat(y_label, dim=0)\n",
    "        y = torch.cat(y, dim=0)\n",
    "\n",
    "        tempprc, auc = precision_test(y.view(-1), y_label.view(-1))\n",
    "        return tempprc, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t|step: 0\t|Loss: 828.4361875\n",
      "epoch: 0\t|step: 500\t|Loss: 828.6670606878101\n",
      "epoch: 0\t|step: 1000\t|Loss: 0.38452131673693657\n",
      "epoch: 0\t|step: 1500\t|Loss: 0.38171418297290804\n",
      "epoch: 0\t|step: 2000\t|Loss: 0.37727378955483437\n",
      "epoch: 0\t|step: 2500\t|Loss: 0.3751609977483749\n",
      "epoch: 0\t|step: 3000\t|Loss: 0.3775507554113865\n",
      "epoch: 0\t|step: 3500\t|Loss: 0.3693381267786026\n",
      "epoch: 0\t|step: 4000\t|Loss: 0.37046004268527033\n",
      "epoch: 0\t|step: 4500\t|Loss: 0.3728073055744171\n",
      "epoch: 0\t|step: 5000\t|Loss: 0.3712027149200439\n",
      "epoch: 0\t|step: 5500\t|Loss: 0.36720242381095886\n",
      "epoch: 0\t|step: 6000\t|Loss: 0.3670737019777298\n",
      "epoch: 0\t|step: 6500\t|Loss: 0.37017329841852187\n",
      "epoch: 0\t|step: 7000\t|Loss: 0.3637247652411461\n",
      "epoch: 0\t|step: 7500\t|Loss: 0.3679983682334423\n",
      "epoch: 0\t|step: 8000\t|Loss: 0.3591955662667751\n",
      "epoch: 0\t|step: 8500\t|Loss: 0.369235222786665\n",
      "epoch: 0\t|step: 9000\t|Loss: 0.3669965054690838\n",
      "epoch: 0\t|step: 9500\t|Loss: 0.36866771534085274\n",
      "epoch: 0\t|step: 10000\t|Loss: 0.36429422116279603\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.522787176711418, auc\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t|step: 0\t|Loss: 818.855125\n",
      "epoch: 1\t|step: 500\t|Loss: 819.0436095390916\n",
      "epoch: 1\t|step: 1000\t|Loss: 0.3648024287223816\n",
      "epoch: 1\t|step: 1500\t|Loss: 0.3635172734558582\n",
      "epoch: 1\t|step: 2000\t|Loss: 0.3648002126216888\n",
      "epoch: 1\t|step: 2500\t|Loss: 0.3583946103155613\n",
      "epoch: 1\t|step: 3000\t|Loss: 0.35898345896601674\n",
      "epoch: 1\t|step: 3500\t|Loss: 0.36152416956424716\n",
      "epoch: 1\t|step: 4000\t|Loss: 0.3614440540969372\n",
      "epoch: 1\t|step: 4500\t|Loss: 0.36301181429624557\n",
      "epoch: 1\t|step: 5000\t|Loss: 0.3580091170370579\n",
      "epoch: 1\t|step: 5500\t|Loss: 0.3639190660715103\n",
      "epoch: 1\t|step: 6000\t|Loss: 0.3626175546646118\n",
      "epoch: 1\t|step: 6500\t|Loss: 0.36010687398910524\n",
      "epoch: 1\t|step: 7000\t|Loss: 0.35676243844628336\n",
      "epoch: 1\t|step: 7500\t|Loss: 0.36171158841252327\n",
      "epoch: 1\t|step: 8000\t|Loss: 0.36088481536507605\n",
      "epoch: 1\t|step: 8500\t|Loss: 0.35790947622060776\n",
      "epoch: 1\t|step: 9000\t|Loss: 0.35609661135077475\n",
      "epoch: 1\t|step: 9500\t|Loss: 0.3553457948565483\n",
      "epoch: 1\t|step: 10000\t|Loss: 0.3619757912158966\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.5277976463747277, auc\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t|step: 0\t|Loss: 809.4284375\n",
      "epoch: 2\t|step: 500\t|Loss: 809.6143362455666\n",
      "epoch: 2\t|step: 1000\t|Loss: 0.35715548604726793\n",
      "epoch: 2\t|step: 1500\t|Loss: 0.36108127880096436\n",
      "epoch: 2\t|step: 2000\t|Loss: 0.3611257568895817\n",
      "epoch: 2\t|step: 2500\t|Loss: 0.3548097923099995\n",
      "epoch: 2\t|step: 3000\t|Loss: 0.3630112095475197\n",
      "epoch: 2\t|step: 3500\t|Loss: 0.35561102312803267\n",
      "epoch: 2\t|step: 4000\t|Loss: 0.35728120955824855\n",
      "epoch: 2\t|step: 4500\t|Loss: 0.35745710745453835\n",
      "epoch: 2\t|step: 5000\t|Loss: 0.34950828105211257\n",
      "epoch: 2\t|step: 5500\t|Loss: 0.35511527451872826\n",
      "epoch: 2\t|step: 6000\t|Loss: 0.3549820253252983\n",
      "epoch: 2\t|step: 6500\t|Loss: 0.3550004445016384\n",
      "epoch: 2\t|step: 7000\t|Loss: 0.35693231081962584\n",
      "epoch: 2\t|step: 7500\t|Loss: 0.35769509384036063\n",
      "epoch: 2\t|step: 8000\t|Loss: 0.35489506620168687\n",
      "epoch: 2\t|step: 8500\t|Loss: 0.35693446138501167\n",
      "epoch: 2\t|step: 9000\t|Loss: 0.36734971779584885\n",
      "epoch: 2\t|step: 9500\t|Loss: 0.3560038278102875\n",
      "epoch: 2\t|step: 10000\t|Loss: 0.35782794043421745\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.5286651294044724, auc\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 3\t|step: 0\t|Loss: 800.098\n",
      "epoch: 3\t|step: 500\t|Loss: 800.2848235212863\n",
      "epoch: 3\t|step: 1000\t|Loss: 0.3612377579808235\n",
      "epoch: 3\t|step: 1500\t|Loss: 0.3538994720876217\n",
      "epoch: 3\t|step: 2000\t|Loss: 0.3525003953576088\n",
      "epoch: 3\t|step: 2500\t|Loss: 0.3535246894955635\n",
      "epoch: 3\t|step: 3000\t|Loss: 0.3571269200146198\n",
      "epoch: 3\t|step: 3500\t|Loss: 0.3540978254377842\n",
      "epoch: 3\t|step: 4000\t|Loss: 0.35782493188977244\n",
      "epoch: 3\t|step: 4500\t|Loss: 0.3498304060101509\n",
      "epoch: 3\t|step: 5000\t|Loss: 0.35999170565605165\n",
      "epoch: 3\t|step: 5500\t|Loss: 0.3579229914844036\n",
      "epoch: 3\t|step: 6000\t|Loss: 0.3520546953678131\n",
      "epoch: 3\t|step: 6500\t|Loss: 0.35304356172680856\n",
      "epoch: 3\t|step: 7000\t|Loss: 0.3621225642859936\n",
      "epoch: 3\t|step: 7500\t|Loss: 0.3512831942439079\n",
      "epoch: 3\t|step: 8000\t|Loss: 0.35482901272177697\n",
      "epoch: 3\t|step: 8500\t|Loss: 0.35093548679351805\n",
      "epoch: 3\t|step: 9000\t|Loss: 0.34600255247950557\n",
      "epoch: 3\t|step: 9500\t|Loss: 0.35110804936289786\n",
      "epoch: 3\t|step: 10000\t|Loss: 0.35901754575967787\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.5303004425510588, auc\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 4\t|step: 0\t|Loss: 790.8593125\n",
      "epoch: 4\t|step: 500\t|Loss: 791.0456969847679\n",
      "epoch: 4\t|step: 1000\t|Loss: 0.3558390986919403\n",
      "epoch: 4\t|step: 1500\t|Loss: 0.35243778094649314\n",
      "epoch: 4\t|step: 2000\t|Loss: 0.3516376675963402\n",
      "epoch: 4\t|step: 2500\t|Loss: 0.3554061169922352\n",
      "epoch: 4\t|step: 3000\t|Loss: 0.35560261622071265\n",
      "epoch: 4\t|step: 3500\t|Loss: 0.3530525412261486\n",
      "epoch: 4\t|step: 4000\t|Loss: 0.35173139840364454\n",
      "epoch: 4\t|step: 4500\t|Loss: 0.35062626108527184\n",
      "epoch: 4\t|step: 5000\t|Loss: 0.3492028207480907\n",
      "epoch: 4\t|step: 5500\t|Loss: 0.35348169961571696\n",
      "epoch: 4\t|step: 6500\t|Loss: 0.3550030234754086\n",
      "epoch: 4\t|step: 7000\t|Loss: 0.35463306692242624\n",
      "epoch: 4\t|step: 7500\t|Loss: 0.35627174550294877\n",
      "epoch: 4\t|step: 8000\t|Loss: 0.3507438416779041\n",
      "epoch: 4\t|step: 8500\t|Loss: 0.357520359903574\n",
      "epoch: 4\t|step: 9000\t|Loss: 0.354281042098999\n",
      "epoch: 4\t|step: 9500\t|Loss: 0.3497699539065361\n",
      "epoch: 4\t|step: 10000\t|Loss: 0.3521455284655094\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.5319822947034596, auc\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 5\t|step: 0\t|Loss: 781.699875\n",
      "epoch: 5\t|step: 500\t|Loss: 781.8786310761869\n",
      "epoch: 5\t|step: 1000\t|Loss: 0.34997838860750197\n",
      "epoch: 5\t|step: 1500\t|Loss: 0.3531177772283554\n",
      "epoch: 5\t|step: 2000\t|Loss: 0.35506011471152304\n",
      "epoch: 5\t|step: 2500\t|Loss: 0.3536271463930607\n",
      "epoch: 5\t|step: 3000\t|Loss: 0.35240318295359613\n",
      "epoch: 5\t|step: 3500\t|Loss: 0.34716366562247275\n",
      "epoch: 5\t|step: 4000\t|Loss: 0.35060505917668344\n",
      "epoch: 5\t|step: 4500\t|Loss: 0.35296838110685347\n",
      "epoch: 5\t|step: 5000\t|Loss: 0.3523222508728504\n",
      "epoch: 5\t|step: 5500\t|Loss: 0.34776543152332307\n",
      "epoch: 5\t|step: 6000\t|Loss: 0.3513152330815792\n",
      "epoch: 5\t|step: 6500\t|Loss: 0.35577285394072533\n",
      "epoch: 5\t|step: 7000\t|Loss: 0.3501247162520885\n",
      "epoch: 5\t|step: 7500\t|Loss: 0.34996801260113714\n",
      "epoch: 5\t|step: 8000\t|Loss: 0.3445820035636425\n",
      "epoch: 5\t|step: 8500\t|Loss: 0.349137085288763\n",
      "epoch: 5\t|step: 9000\t|Loss: 0.34622947669029236\n",
      "epoch: 5\t|step: 9500\t|Loss: 0.35557482013106345\n",
      "epoch: 5\t|step: 10000\t|Loss: 0.3473203849196434\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.533382221402162, auc\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 6\t|step: 0\t|Loss: 772.630875\n",
      "epoch: 6\t|step: 500\t|Loss: 772.8141771087646\n",
      "epoch: 6\t|step: 1000\t|Loss: 0.3541866061091423\n",
      "epoch: 6\t|step: 2000\t|Loss: 0.3507292455136776\n",
      "epoch: 6\t|step: 2500\t|Loss: 0.34473322224617003\n",
      "epoch: 6\t|step: 3000\t|Loss: 0.35585381180047987\n",
      "epoch: 6\t|step: 3500\t|Loss: 0.3528189122080803\n",
      "epoch: 6\t|step: 4000\t|Loss: 0.3485422685444355\n",
      "epoch: 6\t|step: 4500\t|Loss: 0.3484752886891365\n",
      "epoch: 6\t|step: 5000\t|Loss: 0.3503900970518589\n",
      "epoch: 6\t|step: 5500\t|Loss: 0.34832472988963126\n",
      "epoch: 6\t|step: 6000\t|Loss: 0.3451533242762089\n",
      "epoch: 6\t|step: 6500\t|Loss: 0.3449511797726154\n",
      "epoch: 6\t|step: 7000\t|Loss: 0.34003251925110817\n",
      "epoch: 6\t|step: 7500\t|Loss: 0.3475172354578972\n",
      "epoch: 6\t|step: 8000\t|Loss: 0.3441856981217861\n",
      "epoch: 6\t|step: 8500\t|Loss: 0.34492477774620056\n",
      "epoch: 6\t|step: 9000\t|Loss: 0.3518038600683212\n",
      "epoch: 6\t|step: 9500\t|Loss: 0.35181368872523305\n",
      "epoch: 6\t|step: 10000\t|Loss: 0.34962472748756407\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.5344995290423168, auc\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 7\t|step: 0\t|Loss: 763.661\n",
      "epoch: 7\t|step: 500\t|Loss: 763.8415962630809\n",
      "epoch: 7\t|step: 1000\t|Loss: 0.3428839900791645\n",
      "epoch: 7\t|step: 1500\t|Loss: 0.3459092832505703\n",
      "epoch: 7\t|step: 2000\t|Loss: 0.3482709565460682\n",
      "epoch: 7\t|step: 2500\t|Loss: 0.34680383536219594\n",
      "epoch: 7\t|step: 3000\t|Loss: 0.3485645064413547\n",
      "epoch: 7\t|step: 3500\t|Loss: 0.338903489947319\n",
      "epoch: 7\t|step: 4000\t|Loss: 0.3491647589504719\n",
      "epoch: 7\t|step: 4500\t|Loss: 0.34991838297247885\n",
      "epoch: 7\t|step: 5000\t|Loss: 0.3470371183753014\n",
      "epoch: 7\t|step: 5500\t|Loss: 0.343986179292202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7\t|step: 6000\t|Loss: 0.343086493819952\n",
      "epoch: 7\t|step: 6500\t|Loss: 0.3473461285829544\n",
      "epoch: 7\t|step: 7000\t|Loss: 0.34557610949873924\n",
      "epoch: 7\t|step: 7500\t|Loss: 0.3498712308257818\n",
      "epoch: 7\t|step: 8000\t|Loss: 0.34393396374583246\n",
      "epoch: 7\t|step: 8500\t|Loss: 0.33884862533211707\n",
      "epoch: 7\t|step: 9000\t|Loss: 0.3437041630446911\n",
      "epoch: 7\t|step: 9500\t|Loss: 0.3468830488920212\n",
      "epoch: 7\t|step: 10000\t|Loss: 0.3483530362844467\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.534722075547962, auc\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 8\t|step: 0\t|Loss: 754.7934375\n",
      "epoch: 8\t|step: 500\t|Loss: 754.9709373695255\n",
      "epoch: 8\t|step: 1000\t|Loss: 0.34894681319594384\n",
      "epoch: 8\t|step: 1500\t|Loss: 0.34499024403095246\n",
      "epoch: 8\t|step: 2000\t|Loss: 0.3456382290124893\n",
      "epoch: 8\t|step: 2500\t|Loss: 0.3452199337184429\n",
      "epoch: 8\t|step: 3000\t|Loss: 0.3464116596579552\n",
      "epoch: 8\t|step: 3500\t|Loss: 0.351572234004736\n",
      "epoch: 8\t|step: 4000\t|Loss: 0.34380439907312393\n",
      "epoch: 8\t|step: 4500\t|Loss: 0.34345914143323897\n",
      "epoch: 8\t|step: 5000\t|Loss: 0.3416456178426743\n",
      "epoch: 8\t|step: 5500\t|Loss: 0.3497656079530716\n",
      "epoch: 8\t|step: 6000\t|Loss: 0.34685607570409777\n",
      "epoch: 8\t|step: 6500\t|Loss: 0.3493507660627365\n",
      "epoch: 8\t|step: 7000\t|Loss: 0.342962576508522\n",
      "epoch: 8\t|step: 7500\t|Loss: 0.34704054275155066\n",
      "epoch: 8\t|step: 8000\t|Loss: 0.34598712450265884\n",
      "epoch: 8\t|step: 8500\t|Loss: 0.3416278645098209\n",
      "epoch: 8\t|step: 9000\t|Loss: 0.3395136126279831\n",
      "epoch: 8\t|step: 9500\t|Loss: 0.3443257463872433\n",
      "epoch: 8\t|step: 10000\t|Loss: 0.3475849592089653\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.5344734875120065, auc\n",
      "epoch: 9\t|step: 0\t|Loss: 746.0048125\n",
      "epoch: 9\t|step: 500\t|Loss: 746.1844773482085\n",
      "epoch: 9\t|step: 1000\t|Loss: 0.3448842314183712\n",
      "epoch: 9\t|step: 1500\t|Loss: 0.3460272490978241\n",
      "epoch: 9\t|step: 2000\t|Loss: 0.3467414751648903\n",
      "epoch: 9\t|step: 2500\t|Loss: 0.3404971795976162\n",
      "epoch: 9\t|step: 3000\t|Loss: 0.3399037207067013\n",
      "epoch: 9\t|step: 3500\t|Loss: 0.3435056497454643\n",
      "epoch: 9\t|step: 4000\t|Loss: 0.34826907512545585\n",
      "epoch: 9\t|step: 4500\t|Loss: 0.3461967300474644\n",
      "epoch: 9\t|step: 5000\t|Loss: 0.34729446905851363\n",
      "epoch: 9\t|step: 5500\t|Loss: 0.3435715982615948\n",
      "epoch: 9\t|step: 6000\t|Loss: 0.34780723696947097\n",
      "epoch: 9\t|step: 6500\t|Loss: 0.343322190284729\n",
      "epoch: 9\t|step: 7000\t|Loss: 0.3392203035056591\n",
      "epoch: 9\t|step: 7500\t|Loss: 0.3500047243237495\n",
      "epoch: 9\t|step: 8000\t|Loss: 0.3386892112791538\n",
      "epoch: 9\t|step: 8500\t|Loss: 0.3502404046356678\n",
      "epoch: 9\t|step: 9000\t|Loss: 0.34676632907986643\n",
      "epoch: 9\t|step: 9500\t|Loss: 0.345125628054142\n",
      "epoch: 9\t|step: 10000\t|Loss: 0.348645438760519\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.5360252580231061, auc\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 10\t|step: 0\t|Loss: 737.3140625\n",
      "epoch: 10\t|step: 500\t|Loss: 737.4924346418977\n",
      "epoch: 10\t|step: 1000\t|Loss: 0.34202135881781576\n",
      "epoch: 10\t|step: 1500\t|Loss: 0.34412053498625755\n",
      "epoch: 10\t|step: 2000\t|Loss: 0.34660997116565706\n",
      "epoch: 10\t|step: 2500\t|Loss: 0.3435226604938507\n",
      "epoch: 10\t|step: 3000\t|Loss: 0.3376691437363625\n",
      "epoch: 10\t|step: 3500\t|Loss: 0.35219817447662355\n",
      "epoch: 10\t|step: 4000\t|Loss: 0.34104693101346495\n",
      "epoch: 10\t|step: 4500\t|Loss: 0.3422135604172945\n",
      "epoch: 10\t|step: 5000\t|Loss: 0.3420302262604237\n",
      "epoch: 10\t|step: 5500\t|Loss: 0.34200006181001663\n",
      "epoch: 10\t|step: 6000\t|Loss: 0.34527089607715605\n",
      "epoch: 10\t|step: 6500\t|Loss: 0.3434515288472176\n",
      "epoch: 10\t|step: 7000\t|Loss: 0.3456771618127823\n",
      "epoch: 10\t|step: 7500\t|Loss: 0.3349858720004559\n",
      "epoch: 10\t|step: 8000\t|Loss: 0.3401544065773487\n",
      "epoch: 10\t|step: 8500\t|Loss: 0.3364878324866295\n",
      "epoch: 10\t|step: 9000\t|Loss: 0.34311797967553137\n",
      "epoch: 10\t|step: 9500\t|Loss: 0.3452178381681442\n",
      "epoch: 10\t|step: 10000\t|Loss: 0.34360722751915457\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.5332195850230697, auc\n",
      "epoch: 11\t|step: 0\t|Loss: 728.7121875\n",
      "epoch: 11\t|step: 500\t|Loss: 728.8895182840228\n",
      "epoch: 11\t|step: 1000\t|Loss: 0.34251955461502076\n",
      "epoch: 11\t|step: 1500\t|Loss: 0.34129720455408097\n",
      "epoch: 11\t|step: 2000\t|Loss: 0.3426676433086395\n",
      "epoch: 11\t|step: 2500\t|Loss: 0.34759038576483725\n",
      "epoch: 11\t|step: 3000\t|Loss: 0.34229052329063414\n",
      "epoch: 11\t|step: 3500\t|Loss: 0.34473509767651556\n",
      "epoch: 11\t|step: 4000\t|Loss: 0.34469909715652464\n",
      "epoch: 11\t|step: 4500\t|Loss: 0.34884315913915637\n",
      "epoch: 11\t|step: 5000\t|Loss: 0.3434936401844025\n",
      "epoch: 11\t|step: 5500\t|Loss: 0.3429499888122082\n",
      "epoch: 11\t|step: 6000\t|Loss: 0.3416611588001251\n",
      "epoch: 11\t|step: 6500\t|Loss: 0.339210482686758\n",
      "epoch: 11\t|step: 7000\t|Loss: 0.34280351185798646\n",
      "epoch: 11\t|step: 7500\t|Loss: 0.33872526594996455\n",
      "epoch: 11\t|step: 8000\t|Loss: 0.34616287603974344\n",
      "epoch: 11\t|step: 8500\t|Loss: 0.34468059849739074\n",
      "epoch: 11\t|step: 9000\t|Loss: 0.3440406814813614\n",
      "epoch: 11\t|step: 9500\t|Loss: 0.3417061079442501\n",
      "epoch: 11\t|step: 10000\t|Loss: 0.3437669503390789\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.5343577518960554, auc\n",
      "epoch: 12\t|step: 0\t|Loss: 720.20825\n",
      "epoch: 12\t|step: 500\t|Loss: 720.3905261126756\n",
      "epoch: 12\t|step: 1000\t|Loss: 0.33785047751665115\n",
      "epoch: 12\t|step: 1500\t|Loss: 0.3445554395914078\n",
      "epoch: 12\t|step: 2000\t|Loss: 0.34048733764886857\n",
      "epoch: 12\t|step: 2500\t|Loss: 0.3387038614749908\n",
      "epoch: 12\t|step: 3000\t|Loss: 0.3465033623576164\n",
      "epoch: 12\t|step: 3500\t|Loss: 0.3368412605524063\n",
      "epoch: 12\t|step: 4000\t|Loss: 0.33558524599671363\n",
      "epoch: 12\t|step: 4500\t|Loss: 0.34185975590348244\n",
      "epoch: 12\t|step: 5000\t|Loss: 0.34235167324543\n",
      "epoch: 12\t|step: 5500\t|Loss: 0.34222004547715185\n",
      "epoch: 12\t|step: 6000\t|Loss: 0.34177550533413886\n",
      "epoch: 12\t|step: 6500\t|Loss: 0.33550763747096063\n",
      "epoch: 12\t|step: 7000\t|Loss: 0.3416735609471798\n",
      "epoch: 12\t|step: 7500\t|Loss: 0.3369238959848881\n",
      "epoch: 12\t|step: 8000\t|Loss: 0.3398745568096638\n",
      "epoch: 12\t|step: 8500\t|Loss: 0.33462904688715933\n",
      "epoch: 12\t|step: 9000\t|Loss: 0.3421769627928734\n",
      "epoch: 12\t|step: 9500\t|Loss: 0.34175115218758584\n",
      "epoch: 12\t|step: 10000\t|Loss: 0.3348397321999073\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.5359084247937294, auc\n",
      "epoch: 13\t|step: 0\t|Loss: 711.795875\n",
      "epoch: 13\t|step: 500\t|Loss: 711.9718486625552\n",
      "epoch: 13\t|step: 1000\t|Loss: 0.33600559905171395\n",
      "epoch: 13\t|step: 1500\t|Loss: 0.33261656379699706\n",
      "epoch: 13\t|step: 2000\t|Loss: 0.34207034108042716\n",
      "epoch: 13\t|step: 2500\t|Loss: 0.34304861384630203\n",
      "epoch: 13\t|step: 3000\t|Loss: 0.3422795913219452\n",
      "epoch: 13\t|step: 3500\t|Loss: 0.34018527498841283\n",
      "epoch: 13\t|step: 4000\t|Loss: 0.33889590603113173\n",
      "epoch: 13\t|step: 4500\t|Loss: 0.3376891195476055\n",
      "epoch: 13\t|step: 5000\t|Loss: 0.3403720781803131\n",
      "epoch: 13\t|step: 5500\t|Loss: 0.33928761520981787\n",
      "epoch: 13\t|step: 6000\t|Loss: 0.34211084243655204\n",
      "epoch: 13\t|step: 6500\t|Loss: 0.3434008941948414\n",
      "epoch: 13\t|step: 7000\t|Loss: 0.3361422141492367\n",
      "epoch: 13\t|step: 7500\t|Loss: 0.33466006225347517\n",
      "epoch: 13\t|step: 8000\t|Loss: 0.34013150832057\n",
      "epoch: 13\t|step: 8500\t|Loss: 0.33482339218258855\n",
      "epoch: 13\t|step: 9000\t|Loss: 0.33927738693356513\n",
      "epoch: 13\t|step: 9500\t|Loss: 0.33700482627749445\n",
      "epoch: 13\t|step: 10000\t|Loss: 0.3340830171108246\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.534096932247071, auc\n",
      "epoch: 14\t|step: 0\t|Loss: 703.481375\n",
      "epoch: 14\t|step: 500\t|Loss: 703.6619226194024\n",
      "epoch: 14\t|step: 1000\t|Loss: 0.34173248913884163\n",
      "epoch: 14\t|step: 1500\t|Loss: 0.3393824029266834\n",
      "epoch: 14\t|step: 2000\t|Loss: 0.33644503805041315\n",
      "epoch: 14\t|step: 2500\t|Loss: 0.33072392916679383\n",
      "epoch: 14\t|step: 3000\t|Loss: 0.3386194623112678\n",
      "epoch: 14\t|step: 3500\t|Loss: 0.3427905028164387\n",
      "epoch: 14\t|step: 4000\t|Loss: 0.3434672831296921\n",
      "epoch: 14\t|step: 4500\t|Loss: 0.3468713138699532\n",
      "epoch: 14\t|step: 5000\t|Loss: 0.33944695389270785\n",
      "epoch: 14\t|step: 5500\t|Loss: 0.3341951325237751\n",
      "epoch: 14\t|step: 6000\t|Loss: 0.33686299207806586\n",
      "epoch: 14\t|step: 6500\t|Loss: 0.3304346151947975\n",
      "epoch: 14\t|step: 7000\t|Loss: 0.33839465472102165\n",
      "epoch: 14\t|step: 7500\t|Loss: 0.3350384572446346\n",
      "epoch: 14\t|step: 8000\t|Loss: 0.34065744096040723\n",
      "epoch: 14\t|step: 8500\t|Loss: 0.33611490577459335\n",
      "epoch: 14\t|step: 9000\t|Loss: 0.3393331604897976\n",
      "epoch: 14\t|step: 9500\t|Loss: 0.33969923052191736\n",
      "epoch: 14\t|step: 10000\t|Loss: 0.332214462518692\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.5329377306421622, auc\n",
      "epoch: 15\t|step: 0\t|Loss: 695.25025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15\t|step: 500\t|Loss: 695.4347369433045\n",
      "epoch: 15\t|step: 1000\t|Loss: 0.3379757561981678\n",
      "epoch: 15\t|step: 1500\t|Loss: 0.33882561582326887\n",
      "epoch: 15\t|step: 2000\t|Loss: 0.33869811400771144\n",
      "epoch: 15\t|step: 2500\t|Loss: 0.3360218631327152\n",
      "epoch: 15\t|step: 3000\t|Loss: 0.33709218639135363\n",
      "epoch: 15\t|step: 3500\t|Loss: 0.3326101215481758\n",
      "epoch: 15\t|step: 4000\t|Loss: 0.33290890660881994\n",
      "epoch: 15\t|step: 4500\t|Loss: 0.337440148383379\n",
      "epoch: 15\t|step: 5000\t|Loss: 0.3372564110159874\n",
      "epoch: 15\t|step: 5500\t|Loss: 0.33626972046494485\n",
      "epoch: 15\t|step: 6000\t|Loss: 0.33566754472255705\n",
      "epoch: 15\t|step: 6500\t|Loss: 0.335316082239151\n",
      "epoch: 15\t|step: 7000\t|Loss: 0.34024719581007956\n",
      "epoch: 15\t|step: 7500\t|Loss: 0.32937882405519486\n",
      "epoch: 15\t|step: 8000\t|Loss: 0.33103352509438994\n",
      "epoch: 15\t|step: 8500\t|Loss: 0.3363091097474098\n",
      "epoch: 15\t|step: 9000\t|Loss: 0.3385315116345882\n",
      "epoch: 15\t|step: 9500\t|Loss: 0.3329037595391274\n",
      "epoch: 15\t|step: 10000\t|Loss: 0.3331996755897999\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.5324555991267764, auc\n",
      "epoch: 16\t|step: 0\t|Loss: 687.1115625\n",
      "epoch: 16\t|step: 500\t|Loss: 687.2883145068288\n",
      "epoch: 16\t|step: 1000\t|Loss: 0.3348693011403084\n",
      "epoch: 16\t|step: 1500\t|Loss: 0.33250521129369737\n",
      "epoch: 16\t|step: 2000\t|Loss: 0.3349116920232773\n",
      "epoch: 16\t|step: 2500\t|Loss: 0.33422874480485915\n",
      "epoch: 16\t|step: 3000\t|Loss: 0.32939378747344017\n",
      "epoch: 16\t|step: 3500\t|Loss: 0.3337421398162842\n",
      "epoch: 16\t|step: 4000\t|Loss: 0.33163527527451514\n",
      "epoch: 16\t|step: 4500\t|Loss: 0.3329745916724205\n",
      "epoch: 16\t|step: 5000\t|Loss: 0.33498730421066286\n",
      "epoch: 16\t|step: 5500\t|Loss: 0.3388702399432659\n",
      "epoch: 16\t|step: 6000\t|Loss: 0.32749249345064163\n",
      "epoch: 16\t|step: 6500\t|Loss: 0.3292305009663105\n",
      "epoch: 16\t|step: 7000\t|Loss: 0.3345682082772255\n",
      "epoch: 16\t|step: 7500\t|Loss: 0.33280812826752665\n",
      "epoch: 16\t|step: 8000\t|Loss: 0.3399652153253555\n",
      "epoch: 16\t|step: 8500\t|Loss: 0.3384458421170711\n",
      "epoch: 16\t|step: 9000\t|Loss: 0.33044613710045817\n",
      "epoch: 16\t|step: 9500\t|Loss: 0.33250698082149027\n",
      "epoch: 16\t|step: 10000\t|Loss: 0.33660447072982785\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.5347305329779025, auc\n",
      "epoch: 17\t|step: 0\t|Loss: 679.0711875\n",
      "epoch: 17\t|step: 500\t|Loss: 679.2463054874241\n",
      "epoch: 17\t|step: 1000\t|Loss: 0.32907674884796145\n",
      "epoch: 17\t|step: 1500\t|Loss: 0.33467399805784226\n",
      "epoch: 17\t|step: 2000\t|Loss: 0.3354197892844677\n",
      "epoch: 17\t|step: 2500\t|Loss: 0.3288141705393791\n",
      "epoch: 17\t|step: 3000\t|Loss: 0.3354095340073109\n",
      "epoch: 17\t|step: 3500\t|Loss: 0.3352442275285721\n",
      "epoch: 17\t|step: 4000\t|Loss: 0.33101674520969393\n",
      "epoch: 17\t|step: 4500\t|Loss: 0.3384491759240627\n",
      "epoch: 17\t|step: 5000\t|Loss: 0.33878486755490306\n",
      "epoch: 17\t|step: 5500\t|Loss: 0.332932282358408\n",
      "epoch: 17\t|step: 6000\t|Loss: 0.33270246285200117\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-6bc5e64d6c19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbest_pre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Blundell\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#     auc_train, time_cost_train = evaluation(trainData, model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mauc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-c1e5f1d13312>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(e, trainload, model, optim, m, beta_type)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposi_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattMask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtemp_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_pre = 0.0\n",
    "for e in range(50):\n",
    "    train(e, trainData, model,optim, len(trainData), \"Blundell\")\n",
    "#     auc_train, time_cost_train = evaluation(trainData, model)\n",
    "    auc_test, auc = evaluation(testData, model)\n",
    "    print('test precision: {}, auc'.format(auc_test, auc))\n",
    "    if auc_test >best_pre:\n",
    "        # Save a trained model\n",
    "        output_model_file = os.path.join(trainConfig.output_dir, trainConfig.best_name)\n",
    "        create_folder(trainConfig.output_dir)\n",
    "        save_model(output_model_file, model)\n",
    "        best_pre = auc_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3)",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
