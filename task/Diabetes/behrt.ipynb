{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.insert(0, '/home/yikuan/project/Code/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.pytorch import save_model\n",
    "from common.common import create_folder, load_obj\n",
    "import os\n",
    "import torch\n",
    "from ACM.model.utils.utils import age_vocab\n",
    "import pandas as pd\n",
    "import pytorch_pretrained_bert as Bert\n",
    "from ACM.dataLoader.HF import HF_data\n",
    "from ACM.model.behrt import BertHF\n",
    "from ACM.model.optimiser import adam\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import average_precision_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertConfig(Bert.modeling.BertConfig):\n",
    "    def __init__(self, config):\n",
    "        super(BertConfig, self).__init__(\n",
    "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_hidden_layers=config.get('num_hidden_layers'),\n",
    "            num_attention_heads=config.get('num_attention_heads'),\n",
    "            intermediate_size=config.get('intermediate_size'),\n",
    "            hidden_act=config.get('hidden_act'),\n",
    "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
    "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
    "            max_position_embeddings=config.get('max_position_embeddings'),\n",
    "            initializer_range=config.get('initializer_range'),\n",
    "        )\n",
    "        self.seg_vocab_size = config.get('seg_vocab_size')\n",
    "        self.age_vocab_size = config.get('age_vocab_size')\n",
    "        self.num_labels = config.get('num_labels')\n",
    "\n",
    "\n",
    "class TrainConfig(object):\n",
    "    def __init__(self, config):\n",
    "        self.batch_size = config.get('batch_size')\n",
    "        self.use_cuda = config.get('use_cuda')\n",
    "        self.max_len_seq = config.get('max_len_seq')\n",
    "        self.train_loader_workers = config.get('train_loader_workers')\n",
    "        self.test_loader_workers = config.get('test_loader_workers')\n",
    "        self.device = config.get('device')\n",
    "        self.output_dir = config.get('output_dir')\n",
    "        self.output_name = config.get('output_name')\n",
    "        self.best_name = config.get('best_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_config = {\n",
    "    'vocab':'/home/shared/01_data/01_dictionary/pureicd_vocab_diag',\n",
    "    'train': '/home/shared/shishir/deltaAge36_numSep10_unique_leng10_tfidf-threshold1.5_months12__D_maximalScopewY_pure_icd_train.parquet',\n",
    "    'test': '/home/shared/shishir/deltaAge36_numSep10_unique_leng10_tfidf-threshold1.5_months12__D_maximalScopewY_pure_icd_test.parquet'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_params = {\n",
    "    'max_seq_len': 100,\n",
    "    'max_age': 110,\n",
    "    'month': 1,\n",
    "    'age_symbol': None,\n",
    "}\n",
    "\n",
    "optim_param = {\n",
    "    'lr': 3e-5,\n",
    "    'warmup_proportion': 0.1,\n",
    "    'weight_decay': 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertVocab = load_obj(file_config['vocab'])\n",
    "ageVocab, _ = age_vocab(max_age=global_params['max_age'], mon=global_params['month'], symbol=global_params['age_symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = pd.read_parquet(file_config['train'])\n",
    "testData = pd.read_parquet(file_config['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    'batch_size': 64,\n",
    "    'use_cuda': True,\n",
    "    'max_len_seq': global_params['max_seq_len'],\n",
    "    'train_loader_workers': 3,\n",
    "    'test_loader_workers': 3,\n",
    "    'device': 'cuda:1',\n",
    "    'output_dir': '/home/shared/yikuan/HF/model/PureICD',\n",
    "    'output_name': 'ModelA_20_diag_MLM.bin',\n",
    "    'best_name': 'ModelA_20_diag_MLM_best.bin',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainConfig = TrainConfig(train_params)\n",
    "\n",
    "data_set = HF_data(trainConfig, trainData, testData, BertVocab['token2idx'], ageVocab, code='code', age='age')\n",
    "\n",
    "trainData = data_set.get_weighted_sample_train(4)\n",
    "testData = data_set.get_test_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_param = {\n",
    "    'vocab_size': len(BertVocab['token2idx'].keys()),\n",
    "    'hidden_size': 150,\n",
    "    'num_hidden_layers': 4,\n",
    "    'num_attention_heads': 6,\n",
    "    'hidden_act': 'gelu',\n",
    "    'intermediate_size': 108,\n",
    "    'max_position_embeddings': global_params['max_seq_len'],\n",
    "    'seg_vocab_size': 2,\n",
    "    'age_vocab_size': len(ageVocab.keys()),\n",
    "    'prior_prec': 1e0,\n",
    "    'prec_init': 1e0,\n",
    "    'initializer_range': 0.02,\n",
    "    'num_labels': 1,\n",
    "    'hidden_dropout_prob': 0.29,\n",
    "    'attention_probs_dropout_prob': 0.38\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelConfig = BertConfig(model_param)\n",
    "model = BertHF(modelConfig, num_labels=modelConfig.num_labels, n_dim=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path, model):\n",
    "    # load pretrained model and update weights\n",
    "    pretrained_dict = torch.load(path)\n",
    "    model_dict = model.state_dict()\n",
    "    # 1. filter out unnecessary keys\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and k not in ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']}\n",
    "    # 2. overwrite entries in the existing state dict\n",
    "    model_dict.update(pretrained_dict)\n",
    "    # 3. load the new state dict\n",
    "    model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "model = load_model('/home/shared/yikuan/HF/MLM/PureICD_diag.bin', model)\n",
    "model = model.to(trainConfig.device)\n",
    "optim = adam(list(model.named_parameters()),config=optim_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(logits, label):\n",
    "    sig = nn.Sigmoid()\n",
    "    output=sig(logits)\n",
    "    label, output=label.cpu(), output.detach().cpu()\n",
    "    tempprc= average_precision_score(label.numpy(),output.numpy())\n",
    "    prc = precision_score(label.numpy(),output.numpy())\n",
    "    return tempprc, prc\n",
    "\n",
    "def precision_test(logits, label):\n",
    "    sig = nn.Sigmoid()\n",
    "    output=sig(logits)\n",
    "    tempprc= average_precision_score(label.numpy(),output.numpy())\n",
    "    prc = precision_score(label.numpy(),output.numpy())\n",
    "    return tempprc, prc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(e, trainload, model, optim):\n",
    "    model.train()\n",
    "\n",
    "    tr_loss = 0\n",
    "    temp_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    cnt = 0\n",
    "    for step, batch in enumerate(trainload):\n",
    "        cnt += 1\n",
    "\n",
    "        age_ids, input_ids, posi_ids, segment_ids, attMask, targets, _,_ = batch\n",
    "\n",
    "        age_ids = age_ids.to(train_params['device'])\n",
    "        input_ids = input_ids.to(train_params['device'])\n",
    "        posi_ids = posi_ids.to(train_params['device'])\n",
    "        segment_ids = segment_ids.to(train_params['device'])\n",
    "        attMask = attMask.to(train_params['device'])\n",
    "        targets = targets.view(-1).to(train_params['device'])\n",
    "\n",
    "        loss, _ = model(input_ids, age_ids, segment_ids, posi_ids, attention_mask=attMask, labels=targets)\n",
    "        loss.backward()\n",
    "\n",
    "        temp_loss += loss.item()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            #             prec, a, b = precision(logits, targets)\n",
    "            #             print(\"epoch: {}\\t| Cnt: {}\\t| Loss: {}\\t| precision: {}\".format(e, cnt, temp_loss / 500, prec))\n",
    "            print(\"epoch: {}\\t|step: {}\\t|Loss: {}\".format(e, step, temp_loss / 500))\n",
    "            temp_loss = 0\n",
    "\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "    # Save a trained model\n",
    "    output_model_file = os.path.join(trainConfig.output_dir, trainConfig.output_name)\n",
    "    create_folder(trainConfig.output_dir)\n",
    "    save_model(output_model_file, model)\n",
    "\n",
    "\n",
    "def evaluation(testload, model):\n",
    "    model.eval()\n",
    "    y = []\n",
    "    y_label = []\n",
    "    loss_temp = 0\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(testload):\n",
    "            age_ids, input_ids, posi_ids, segment_ids, attMask, targets, _,_ = batch\n",
    "\n",
    "            age_ids = age_ids.to(train_params['device'])\n",
    "            input_ids = input_ids.to(train_params['device'])\n",
    "            posi_ids = posi_ids.to(train_params['device'])\n",
    "            segment_ids = segment_ids.to(train_params['device'])\n",
    "            attMask = attMask.to(train_params['device'])\n",
    "            targets = targets.view(-1).to(train_params['device'])\n",
    "\n",
    "            logits =model(input_ids, age_ids, segment_ids, posi_ids, attention_mask=attMask, labels=None)\n",
    "            targets = targets.cpu()\n",
    "\n",
    "            y_label.append(targets)\n",
    "            y.append(logits.cpu())\n",
    "\n",
    "        y_label = torch.cat(y_label, dim=0)\n",
    "        y = torch.cat(y, dim=0)\n",
    "\n",
    "        tempprc, prc = precision_test(y.view(-1), y_label.view(-1))\n",
    "        return tempprc, prc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pre = 0\n",
    "for e in range(20):\n",
    "    train(e, trainData, model,optim)\n",
    "#     auc_train, time_cost_train = evaluation(trainData, model)\n",
    "    auc_test, prc = evaluation(testData, model)\n",
    "    print('test roc: {}, aps'.format(auc_test, prc))\n",
    "    if auc_test >best_pre:\n",
    "        # Save a trained model\n",
    "        output_model_file = os.path.join(trainConfig.output_dir, trainConfig.best_name)\n",
    "        create_folder(trainConfig.output_dir)\n",
    "        save_model(output_model_file, model)\n",
    "        best_pre = auc_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3)",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
