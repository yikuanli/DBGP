{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.insert(0, '/home/yikuan/project/Code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.pytorch import save_model\n",
    "from common.common import create_folder,load_obj\n",
    "import os\n",
    "import torch\n",
    "from ACM.model.utils.utils import age_vocab\n",
    "import pandas as pd\n",
    "import pytorch_pretrained_bert as Bert\n",
    "from ACM.dataLoader.HF import HF_data\n",
    "from ACM.model.bertBayesianOutput import BertHF\n",
    "import gpytorch\n",
    "from ACM.model.optimiser import adam\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertConfig(Bert.modeling.BertConfig):\n",
    "    def __init__(self, config):\n",
    "        super(BertConfig, self).__init__(\n",
    "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_hidden_layers=config.get('num_hidden_layers'),\n",
    "            num_attention_heads=config.get('num_attention_heads'),\n",
    "            intermediate_size=config.get('intermediate_size'),\n",
    "            hidden_act=config.get('hidden_act'),\n",
    "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
    "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
    "            max_position_embeddings=config.get('max_position_embeddings'),\n",
    "            initializer_range=config.get('initializer_range'),\n",
    "        )\n",
    "        self.seg_vocab_size = config.get('seg_vocab_size')\n",
    "        self.age_vocab_size = config.get('age_vocab_size')\n",
    "        self.num_labels = config.get('num_labels')\n",
    "\n",
    "\n",
    "class TrainConfig(object):\n",
    "    def __init__(self, config):\n",
    "        self.batch_size = config.get('batch_size')\n",
    "        self.use_cuda = config.get('use_cuda')\n",
    "        self.max_len_seq = config.get('max_len_seq')\n",
    "        self.train_loader_workers = config.get('train_loader_workers')\n",
    "        self.test_loader_workers = config.get('test_loader_workers')\n",
    "        self.device = config.get('device')\n",
    "        self.output_dir = config.get('output_dir')\n",
    "        self.output_name = config.get('output_name')\n",
    "        self.best_name = config.get('best_name')\n",
    "        self.num_samples = config.get('num_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_config = {\n",
    "    'vocab':'/home/yikuan/project/Code/ACM/data/Full_vocab',\n",
    "    'train': '/home/shared/yikuan/ACM/data/Depression/Depression_clean_train.parquet',\n",
    "    'test': '/home/shared/yikuan/ACM/data/Depression/Depression_clean_test.parquet'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_params = {\n",
    "    'max_seq_len': 256,\n",
    "    'max_age': 110,\n",
    "    'month': 1,\n",
    "    'age_symbol': None,\n",
    "}\n",
    "\n",
    "optim_param = {\n",
    "    'lr': 3e-5,\n",
    "    'warmup_proportion': 0.1,\n",
    "    'weight_decay': 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertVocab = load_obj(file_config['vocab'])\n",
    "ageVocab, _ = age_vocab(max_age=global_params['max_age'], mon=global_params['month'], symbol=global_params['age_symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = pd.read_parquet(file_config['train']).reset_index(drop=True)\n",
    "testData = pd.read_parquet(file_config['test']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    'batch_size': 64,\n",
    "    'use_cuda': True,\n",
    "    'max_len_seq': global_params['max_seq_len'],\n",
    "    'train_loader_workers': 3,\n",
    "    'test_loader_workers': 3,\n",
    "    'num_samples': 10,\n",
    "    'device': 'cuda:0',\n",
    "    'output_dir': '/home/shared/yikuan/ACM/model/Depression',\n",
    "    'output_name': 'behrtBayesianOutput.bin',\n",
    "    'best_name': 'behrtBayesianOutput_best.bin',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainConfig = TrainConfig(train_params)\n",
    "\n",
    "data_set = HF_data(trainConfig, trainData, testData, BertVocab['token2idx'], ageVocab, code='code', age='age')\n",
    "\n",
    "trainData = data_set.get_weighted_sample_train(4)\n",
    "testData = data_set.get_test_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_param = {\n",
    "    'vocab_size': len(BertVocab['token2idx'].keys()),\n",
    "    'hidden_size': 150,\n",
    "    'num_hidden_layers': 4,\n",
    "    'num_attention_heads': 6,\n",
    "    'hidden_act': 'gelu',\n",
    "    'intermediate_size': 108,\n",
    "    'max_position_embeddings': global_params['max_seq_len'],\n",
    "    'seg_vocab_size': 2,\n",
    "    'age_vocab_size': len(ageVocab.keys()),\n",
    "    'prior_prec': 1e0,\n",
    "    'prec_init': 1e0,\n",
    "    'initializer_range': 0.02,\n",
    "    'num_labels': 1,\n",
    "    'hidden_dropout_prob': 0.29,\n",
    "    'attention_probs_dropout_prob': 0.38\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelConfig = BertConfig(model_param)\n",
    "model = BertHF(modelConfig, num_labels=modelConfig.num_labels, n_dim=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n"
     ]
    }
   ],
   "source": [
    "def load_model(path, model):\n",
    "    # load pretrained model and update weights\n",
    "    pretrained_dict = torch.load(path)\n",
    "    model_dict = model.state_dict()\n",
    "    # 1. filter out unnecessary keys\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and k not in ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']}\n",
    "    # 2. overwrite entries in the existing state dict\n",
    "    model_dict.update(pretrained_dict)\n",
    "    # 3. load the new state dict\n",
    "    model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "model = load_model('/home/shared/yikuan/HF/MLM/PureICD_diag_med.bin', model)\n",
    "model = model.to(trainConfig.device)\n",
    "optim = adam(list(model.named_parameters()),config=optim_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(logits, label):\n",
    "    sig = nn.Sigmoid()\n",
    "    output=sig(logits)\n",
    "    label, output=label.cpu(), output.detach().cpu()\n",
    "    tempprc= average_precision_score(label.numpy(),output.numpy())\n",
    "    auc = roc_auc_score(label.numpy(),output.numpy())\n",
    "    return tempprc, auc\n",
    "\n",
    "def precision_test(prob, label):\n",
    "#     sig = nn.Sigmoid()\n",
    "#     output=sig(logits)\n",
    "    tempprc= average_precision_score(label.numpy(),prob.numpy())\n",
    "    auc = roc_auc_score(label.numpy(),prob.numpy())\n",
    "    return tempprc, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(e, trainload, model, optim):\n",
    "    model.train()\n",
    "\n",
    "    tr_loss = 0\n",
    "    temp_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    cnt = 0\n",
    "    for step, batch in enumerate(trainload):\n",
    "        cnt += 1\n",
    "\n",
    "        age_ids, input_ids, posi_ids, segment_ids, attMask, targets, _, _ = batch\n",
    "\n",
    "        age_ids = age_ids.to(train_params['device'])\n",
    "        input_ids = input_ids.to(train_params['device'])\n",
    "        posi_ids = posi_ids.to(train_params['device'])\n",
    "        segment_ids = segment_ids.to(train_params['device'])\n",
    "        attMask = attMask.to(train_params['device'])\n",
    "        targets = targets.view(-1).to(train_params['device'])\n",
    "\n",
    "        loss, _ = model(input_ids, age_ids, segment_ids, posi_ids, attention_mask=attMask, labels=targets)\n",
    "        loss.backward()\n",
    "\n",
    "        temp_loss += loss.item()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            #             prec, a, b = precision(logits, targets)\n",
    "            #             print(\"epoch: {}\\t| Cnt: {}\\t| Loss: {}\\t| precision: {}\".format(e, cnt, temp_loss / 500, prec))\n",
    "            print(\"epoch: {}\\t|step: {}\\t|Loss: {}\".format(e, step, temp_loss / 500))\n",
    "            temp_loss = 0\n",
    "\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "    # Save a trained model\n",
    "    output_model_file = os.path.join(trainConfig.output_dir, trainConfig.output_name)\n",
    "    create_folder(trainConfig.output_dir)\n",
    "    save_model(output_model_file, model)\n",
    "\n",
    "\n",
    "def evaluation(testload, model):\n",
    "    model.eval()\n",
    "    y = []\n",
    "    y_label = []\n",
    "    loss_temp = 0\n",
    "    sig = nn.Sigmoid()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(testload):\n",
    "            age_ids, input_ids, posi_ids, segment_ids, attMask, targets, _, _ = batch\n",
    "\n",
    "            age_ids = age_ids.to(train_params['device'])\n",
    "            input_ids = input_ids.to(train_params['device'])\n",
    "            posi_ids = posi_ids.to(train_params['device'])\n",
    "            segment_ids = segment_ids.to(train_params['device'])\n",
    "            attMask = attMask.to(train_params['device'])\n",
    "            targets = targets.view(-1).to(train_params['device'])\n",
    "            \n",
    "            logits_prob = []\n",
    "            for i in range(trainConfig.num_samples):\n",
    "                logits =model(input_ids, age_ids, segment_ids, posi_ids, attention_mask=attMask, labels=None)\n",
    "                logits_prob.append(logits)\n",
    "            \n",
    "            logits_prob = torch.mean(sig(torch.stack(logits_prob, dim=1)), dim=1)\n",
    "            \n",
    "            # get mean of logits\n",
    "            \n",
    "            \n",
    "            targets = targets.cpu()\n",
    "\n",
    "            y_label.append(targets)\n",
    "            y.append(logits_prob.cpu())\n",
    "\n",
    "        y_label = torch.cat(y_label, dim=0)\n",
    "        y = torch.cat(y, dim=0)\n",
    "\n",
    "        tempprc, auc = precision_test(y.view(-1), y_label.view(-1))\n",
    "        return tempprc, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t|step: 0\t|Loss: 0.8687518920898437\n",
      "epoch: 0\t|step: 500\t|Loss: 424.39012774658204\n",
      "epoch: 0\t|step: 1000\t|Loss: 413.84518560791014\n",
      "epoch: 0\t|step: 1500\t|Loss: 406.16969256591796\n",
      "epoch: 0\t|step: 2000\t|Loss: 399.3637398071289\n",
      "epoch: 0\t|step: 2500\t|Loss: 392.9792962036133\n",
      "epoch: 0\t|step: 3000\t|Loss: 386.8419306640625\n",
      "epoch: 0\t|step: 3500\t|Loss: 380.8504656982422\n",
      "epoch: 0\t|step: 4000\t|Loss: 374.9823165283203\n",
      "epoch: 0\t|step: 4500\t|Loss: 369.1873760986328\n",
      "epoch: 0\t|step: 5000\t|Loss: 363.45857720947265\n",
      "epoch: 0\t|step: 5500\t|Loss: 357.77860443115236\n",
      "epoch: 0\t|step: 6000\t|Loss: 352.14414642333986\n",
      "epoch: 0\t|step: 6500\t|Loss: 346.57065197753906\n",
      "epoch: 0\t|step: 7000\t|Loss: 341.04316235351564\n",
      "epoch: 0\t|step: 7500\t|Loss: 335.5500056152344\n",
      "epoch: 0\t|step: 8000\t|Loss: 330.096452331543\n",
      "epoch: 0\t|step: 8500\t|Loss: 324.69916302490236\n",
      "epoch: 0\t|step: 9000\t|Loss: 319.3389397583008\n",
      "epoch: 0\t|step: 9500\t|Loss: 314.0164705810547\n",
      "epoch: 0\t|step: 10000\t|Loss: 308.7440638427734\n",
      "epoch: 0\t|step: 10500\t|Loss: 303.50182788085937\n",
      "epoch: 0\t|step: 11000\t|Loss: 298.3011008300781\n",
      "epoch: 0\t|step: 11500\t|Loss: 293.14945806884765\n",
      "epoch: 0\t|step: 12000\t|Loss: 288.04000677490234\n",
      "epoch: 0\t|step: 12500\t|Loss: 282.9650040893555\n",
      "epoch: 0\t|step: 13000\t|Loss: 277.93938299560546\n",
      "epoch: 0\t|step: 13500\t|Loss: 272.94708502197267\n",
      "epoch: 0\t|step: 14000\t|Loss: 268.01150579833984\n",
      "epoch: 0\t|step: 14500\t|Loss: 263.09827087402346\n",
      "epoch: 0\t|step: 15000\t|Loss: 258.2378324584961\n",
      "epoch: 0\t|step: 15500\t|Loss: 253.4132586364746\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test prc: 0.4238083319406986, auc:0.7621414757630153\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t|step: 0\t|Loss: 0.4936871337890625\n",
      "epoch: 1\t|step: 500\t|Loss: 244.51362396240233\n",
      "epoch: 1\t|step: 1000\t|Loss: 239.80856784057616\n",
      "epoch: 1\t|step: 1500\t|Loss: 235.15033093261718\n",
      "epoch: 1\t|step: 2000\t|Loss: 230.53027114868163\n",
      "epoch: 1\t|step: 2500\t|Loss: 225.95818829345703\n",
      "epoch: 1\t|step: 3500\t|Loss: 216.9311901550293\n",
      "epoch: 1\t|step: 4000\t|Loss: 212.47346563720703\n",
      "epoch: 1\t|step: 4500\t|Loss: 208.0608850708008\n",
      "epoch: 1\t|step: 5000\t|Loss: 203.70525128173827\n",
      "epoch: 1\t|step: 5500\t|Loss: 199.3803966369629\n",
      "epoch: 1\t|step: 6000\t|Loss: 195.09422946166993\n",
      "epoch: 1\t|step: 6500\t|Loss: 190.8535915222168\n",
      "epoch: 1\t|step: 7000\t|Loss: 186.6622526855469\n",
      "epoch: 1\t|step: 7500\t|Loss: 182.51045751953126\n",
      "epoch: 1\t|step: 8000\t|Loss: 178.40071432495117\n",
      "epoch: 1\t|step: 8500\t|Loss: 174.33802407836913\n",
      "epoch: 1\t|step: 9000\t|Loss: 170.31645532226563\n",
      "epoch: 1\t|step: 9500\t|Loss: 166.33837939453124\n",
      "epoch: 1\t|step: 10000\t|Loss: 162.40105987548827\n",
      "epoch: 1\t|step: 10500\t|Loss: 158.5176703186035\n",
      "epoch: 1\t|step: 11000\t|Loss: 154.65682412719727\n",
      "epoch: 1\t|step: 11500\t|Loss: 150.85750045776368\n",
      "epoch: 1\t|step: 12000\t|Loss: 147.09881896972655\n",
      "epoch: 1\t|step: 12500\t|Loss: 143.37404312133788\n",
      "epoch: 1\t|step: 13000\t|Loss: 139.70731567382813\n",
      "epoch: 1\t|step: 13500\t|Loss: 136.08140881347657\n",
      "epoch: 1\t|step: 14000\t|Loss: 132.4949637145996\n",
      "epoch: 1\t|step: 14500\t|Loss: 128.95388288879394\n",
      "epoch: 1\t|step: 15000\t|Loss: 125.4658250579834\n",
      "epoch: 1\t|step: 15500\t|Loss: 122.00617976379395\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test prc: 0.4211235135006055, auc:0.7600999586703096\n",
      "epoch: 2\t|step: 0\t|Loss: 0.23450502014160157\n",
      "epoch: 2\t|step: 500\t|Loss: 115.66869737243653\n",
      "epoch: 2\t|step: 1000\t|Loss: 112.3552792816162\n",
      "epoch: 2\t|step: 1500\t|Loss: 109.07013063049317\n",
      "epoch: 2\t|step: 2000\t|Loss: 105.83741232299805\n",
      "epoch: 2\t|step: 2500\t|Loss: 102.64883219909667\n",
      "epoch: 2\t|step: 3000\t|Loss: 99.5039891204834\n",
      "epoch: 2\t|step: 3500\t|Loss: 96.40766009521484\n",
      "epoch: 2\t|step: 4000\t|Loss: 93.34687976074218\n",
      "epoch: 2\t|step: 4500\t|Loss: 90.34072384643555\n",
      "epoch: 2\t|step: 5000\t|Loss: 87.38543139648438\n",
      "epoch: 2\t|step: 5500\t|Loss: 84.46465237426757\n",
      "epoch: 2\t|step: 6000\t|Loss: 81.59802406311036\n",
      "epoch: 2\t|step: 6500\t|Loss: 78.77082940673829\n",
      "epoch: 2\t|step: 7000\t|Loss: 76.00605154418945\n",
      "epoch: 2\t|step: 7500\t|Loss: 73.26686471557618\n",
      "epoch: 2\t|step: 8000\t|Loss: 70.5848800201416\n",
      "epoch: 2\t|step: 8500\t|Loss: 67.9565196838379\n",
      "epoch: 2\t|step: 9000\t|Loss: 65.35750025939942\n",
      "epoch: 2\t|step: 9500\t|Loss: 62.82218720245361\n",
      "epoch: 2\t|step: 10000\t|Loss: 60.314818336486816\n",
      "epoch: 2\t|step: 10500\t|Loss: 57.88712672424317\n",
      "epoch: 2\t|step: 11000\t|Loss: 55.48442263793945\n",
      "epoch: 2\t|step: 11500\t|Loss: 53.140598579406735\n",
      "epoch: 2\t|step: 12000\t|Loss: 50.83919361877442\n",
      "epoch: 2\t|step: 12500\t|Loss: 48.58998108673096\n",
      "epoch: 2\t|step: 13000\t|Loss: 46.385587547302244\n",
      "epoch: 2\t|step: 13500\t|Loss: 44.22738883209229\n",
      "epoch: 2\t|step: 14000\t|Loss: 42.12595613098144\n",
      "epoch: 2\t|step: 14500\t|Loss: 40.06293524169922\n",
      "epoch: 2\t|step: 15000\t|Loss: 38.06484368896484\n",
      "epoch: 2\t|step: 15500\t|Loss: 36.102942932128904\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test prc: 0.4141322507735796, auc:0.7438460584510646\n",
      "epoch: 3\t|step: 0\t|Loss: 0.06709959411621094\n",
      "epoch: 3\t|step: 500\t|Loss: 32.5921704788208\n",
      "epoch: 3\t|step: 1000\t|Loss: 30.779928241729735\n",
      "epoch: 3\t|step: 1500\t|Loss: 29.024020652770997\n",
      "epoch: 3\t|step: 2000\t|Loss: 27.31313906097412\n",
      "epoch: 3\t|step: 2500\t|Loss: 25.660394119262694\n",
      "epoch: 3\t|step: 3000\t|Loss: 24.06295965576172\n",
      "epoch: 3\t|step: 3500\t|Loss: 22.511246562957762\n",
      "epoch: 3\t|step: 4000\t|Loss: 21.015496181488036\n",
      "epoch: 3\t|step: 4500\t|Loss: 19.574459728240967\n",
      "epoch: 3\t|step: 5000\t|Loss: 18.17828515625\n",
      "epoch: 3\t|step: 5500\t|Loss: 16.84159775161743\n",
      "epoch: 3\t|step: 6000\t|Loss: 15.570210939407348\n",
      "epoch: 3\t|step: 6500\t|Loss: 14.351907495498658\n",
      "epoch: 3\t|step: 7000\t|Loss: 13.169256954193115\n",
      "epoch: 3\t|step: 7500\t|Loss: 12.044864624023438\n",
      "epoch: 3\t|step: 8000\t|Loss: 10.990391382217407\n",
      "epoch: 3\t|step: 8500\t|Loss: 9.991205764770507\n",
      "epoch: 3\t|step: 9000\t|Loss: 9.036751811981201\n",
      "epoch: 3\t|step: 9500\t|Loss: 8.14361898803711\n",
      "epoch: 3\t|step: 10000\t|Loss: 7.286262652397156\n",
      "epoch: 3\t|step: 10500\t|Loss: 6.5132625207901\n",
      "epoch: 3\t|step: 11000\t|Loss: 5.77059446144104\n",
      "epoch: 3\t|step: 11500\t|Loss: 5.101861123085022\n",
      "epoch: 3\t|step: 12000\t|Loss: 4.497801547050476\n",
      "epoch: 3\t|step: 12500\t|Loss: 3.9207443237304687\n",
      "epoch: 3\t|step: 13000\t|Loss: 3.420638799190521\n",
      "epoch: 3\t|step: 13500\t|Loss: 2.957069506645203\n",
      "epoch: 3\t|step: 14000\t|Loss: 2.542536382675171\n",
      "epoch: 3\t|step: 14500\t|Loss: 2.1806120178699495\n",
      "epoch: 3\t|step: 15000\t|Loss: 1.8723773148059846\n",
      "epoch: 3\t|step: 15500\t|Loss: 1.618780250787735\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test prc: 0.4016453529581042, auc:0.7337898703745832\n",
      "epoch: 4\t|step: 0\t|Loss: 0.0022430214881896974\n",
      "epoch: 4\t|step: 500\t|Loss: 1.2131892136335374\n",
      "epoch: 4\t|step: 1000\t|Loss: 1.0615220292806626\n",
      "epoch: 4\t|step: 1500\t|Loss: 0.9468112980127334\n",
      "epoch: 4\t|step: 2000\t|Loss: 0.8449988018274307\n",
      "epoch: 4\t|step: 2500\t|Loss: 0.7673895851969719\n",
      "epoch: 4\t|step: 3000\t|Loss: 0.7025414702892303\n",
      "epoch: 4\t|step: 3500\t|Loss: 0.6598326635956764\n",
      "epoch: 4\t|step: 4000\t|Loss: 0.6590942072272301\n",
      "epoch: 4\t|step: 4500\t|Loss: 0.6374998773336411\n",
      "epoch: 4\t|step: 5000\t|Loss: 0.6138662620782852\n",
      "epoch: 4\t|step: 5500\t|Loss: 0.620026595890522\n",
      "epoch: 4\t|step: 6000\t|Loss: 0.610272461950779\n",
      "epoch: 4\t|step: 6500\t|Loss: 0.610715250313282\n",
      "epoch: 4\t|step: 7000\t|Loss: 0.6028955162763595\n",
      "epoch: 4\t|step: 7500\t|Loss: 0.6077586407065392\n",
      "epoch: 4\t|step: 8000\t|Loss: 0.6011400239467621\n",
      "epoch: 4\t|step: 8500\t|Loss: 0.6116668412089348\n",
      "epoch: 4\t|step: 9000\t|Loss: 0.5942888882756233\n",
      "epoch: 4\t|step: 9500\t|Loss: 0.6051190940439701\n",
      "epoch: 4\t|step: 10000\t|Loss: 0.6082070435881615\n",
      "epoch: 4\t|step: 10500\t|Loss: 0.5995163687467575\n",
      "epoch: 4\t|step: 11000\t|Loss: 0.613571019411087\n",
      "epoch: 4\t|step: 11500\t|Loss: 0.600577154815197\n",
      "epoch: 4\t|step: 12000\t|Loss: 0.5981000614762306\n",
      "epoch: 4\t|step: 12500\t|Loss: 0.6141815207004547\n",
      "epoch: 4\t|step: 13000\t|Loss: 0.603339049577713\n",
      "epoch: 4\t|step: 13500\t|Loss: 0.6136374643445015\n",
      "epoch: 4\t|step: 14000\t|Loss: 0.5992394407391548\n",
      "epoch: 4\t|step: 14500\t|Loss: 0.5963717767000198\n",
      "epoch: 4\t|step: 15000\t|Loss: 0.6027807053923607\n",
      "epoch: 4\t|step: 15500\t|Loss: 0.5968278695344925\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test prc: 0.395916260330946, auc:0.731284684280716\n",
      "epoch: 5\t|step: 0\t|Loss: 0.0014302239418029785\n",
      "epoch: 5\t|step: 500\t|Loss: 0.6101267312169075\n",
      "epoch: 5\t|step: 1000\t|Loss: 0.622060057759285\n",
      "epoch: 5\t|step: 1500\t|Loss: 0.5928521487116813\n",
      "epoch: 5\t|step: 2000\t|Loss: 0.6021612744331359\n",
      "epoch: 5\t|step: 2500\t|Loss: 0.621455812215805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5\t|step: 3000\t|Loss: 0.6086055017709732\n",
      "epoch: 5\t|step: 3500\t|Loss: 0.6045184375047684\n",
      "epoch: 5\t|step: 4000\t|Loss: 0.608800000667572\n",
      "epoch: 5\t|step: 4500\t|Loss: 0.6044152315855026\n",
      "epoch: 5\t|step: 5000\t|Loss: 0.6096917740106582\n",
      "epoch: 5\t|step: 5500\t|Loss: 0.6063489438891411\n",
      "epoch: 5\t|step: 6000\t|Loss: 0.6020791621804238\n",
      "epoch: 5\t|step: 6500\t|Loss: 0.6014245954155922\n",
      "epoch: 5\t|step: 7000\t|Loss: 0.5954768059849739\n",
      "epoch: 5\t|step: 7500\t|Loss: 0.595269157230854\n",
      "epoch: 5\t|step: 8000\t|Loss: 0.6029245373606682\n",
      "epoch: 5\t|step: 8500\t|Loss: 0.592436628818512\n",
      "epoch: 5\t|step: 9000\t|Loss: 0.5981842354536057\n",
      "epoch: 5\t|step: 9500\t|Loss: 0.5932671169042587\n",
      "epoch: 5\t|step: 10000\t|Loss: 0.6059331078529357\n",
      "epoch: 5\t|step: 10500\t|Loss: 0.5886808621883393\n",
      "epoch: 5\t|step: 11000\t|Loss: 0.6074197350144386\n",
      "epoch: 5\t|step: 11500\t|Loss: 0.6186041397452354\n",
      "epoch: 5\t|step: 12000\t|Loss: 0.5982168833613396\n",
      "epoch: 5\t|step: 12500\t|Loss: 0.6064134824872017\n",
      "epoch: 5\t|step: 13000\t|Loss: 0.6130271757245064\n",
      "epoch: 5\t|step: 13500\t|Loss: 0.5879077862501144\n",
      "epoch: 5\t|step: 14000\t|Loss: 0.5914423514008522\n",
      "epoch: 5\t|step: 14500\t|Loss: 0.6092524017095566\n",
      "epoch: 5\t|step: 15000\t|Loss: 0.5859486488103867\n",
      "epoch: 5\t|step: 15500\t|Loss: 0.5988570310473442\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test prc: 0.3995075285293913, auc:0.7422982858316358\n",
      "epoch: 6\t|step: 0\t|Loss: 0.0009706329703330994\n",
      "epoch: 6\t|step: 500\t|Loss: 0.6005681551098824\n",
      "epoch: 6\t|step: 1000\t|Loss: 0.5921518430113792\n",
      "epoch: 6\t|step: 1500\t|Loss: 0.5990342152118683\n",
      "epoch: 6\t|step: 2000\t|Loss: 0.6023097676634789\n",
      "epoch: 6\t|step: 2500\t|Loss: 0.5858196955919266\n",
      "epoch: 6\t|step: 3000\t|Loss: 0.6015633608698845\n",
      "epoch: 6\t|step: 3500\t|Loss: 0.5957455379962922\n",
      "epoch: 6\t|step: 4000\t|Loss: 0.5984232025146484\n",
      "epoch: 6\t|step: 4500\t|Loss: 0.5878362632989883\n",
      "epoch: 6\t|step: 5000\t|Loss: 0.5840703939199448\n",
      "epoch: 6\t|step: 5500\t|Loss: 0.5934798904061317\n",
      "epoch: 6\t|step: 6000\t|Loss: 0.5879695121943951\n",
      "epoch: 6\t|step: 6500\t|Loss: 0.603845100402832\n",
      "epoch: 6\t|step: 7000\t|Loss: 0.5989241616725922\n",
      "epoch: 6\t|step: 7500\t|Loss: 0.5965485479831696\n",
      "epoch: 6\t|step: 8000\t|Loss: 0.595379071354866\n",
      "epoch: 6\t|step: 8500\t|Loss: 0.5845112861990929\n",
      "epoch: 6\t|step: 9000\t|Loss: 0.592410395681858\n",
      "epoch: 6\t|step: 9500\t|Loss: 0.592214625954628\n",
      "epoch: 6\t|step: 10000\t|Loss: 0.5995884631872177\n",
      "epoch: 6\t|step: 10500\t|Loss: 0.5980023503899574\n",
      "epoch: 6\t|step: 11000\t|Loss: 0.5837411705851555\n",
      "epoch: 6\t|step: 11500\t|Loss: 0.6113507077693939\n",
      "epoch: 6\t|step: 12000\t|Loss: 0.5931676263809205\n",
      "epoch: 6\t|step: 12500\t|Loss: 0.5976905472278595\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9ba153a12ca8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbest_pre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#     auc_train, time_cost_train = evaluation(trainData, model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mauc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-93eb1a98005b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(e, trainload, model, optim)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposi_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattMask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtemp_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_pre = 0\n",
    "for e in range(20):\n",
    "    train(e, trainData, model,optim)\n",
    "#     auc_train, time_cost_train = evaluation(trainData, model)\n",
    "    auc_test, auc = evaluation(testData, model)\n",
    "    print('test prc: {}, auc:{}'.format(auc_test, auc))\n",
    "    if auc_test >best_pre:\n",
    "        # Save a trained model\n",
    "        output_model_file = os.path.join(trainConfig.output_dir, trainConfig.best_name)\n",
    "        create_folder(trainConfig.output_dir)\n",
    "        save_model(output_model_file, model)\n",
    "        best_pre = auc_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3)",
   "language": "python",
   "name": "py3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
