{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.insert(0, '/home/yikuan/project/Code/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.pytorch import save_model\n",
    "from common.common import create_folder,load_obj\n",
    "from common.pytorch import load_model as torch_load_model\n",
    "import os\n",
    "import torch\n",
    "from ACM.model.utils.utils import age_vocab\n",
    "import pandas as pd\n",
    "import pytorch_pretrained_bert as Bert\n",
    "from ACM.dataLoader.HF import HF_data\n",
    "from ACM.model.bertBayesianEmbeddingOutput import BertHF\n",
    "from ACM.model.optimiser import adam\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertConfig(Bert.modeling.BertConfig):\n",
    "    def __init__(self, config):\n",
    "        super(BertConfig, self).__init__(\n",
    "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_hidden_layers=config.get('num_hidden_layers'),\n",
    "            num_attention_heads=config.get('num_attention_heads'),\n",
    "            intermediate_size=config.get('intermediate_size'),\n",
    "            hidden_act=config.get('hidden_act'),\n",
    "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
    "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
    "            max_position_embeddings=config.get('max_position_embeddings'),\n",
    "            initializer_range=config.get('initializer_range'),\n",
    "        )\n",
    "        self.seg_vocab_size = config.get('seg_vocab_size')\n",
    "        self.age_vocab_size = config.get('age_vocab_size')\n",
    "        self.num_labels = config.get('num_labels')\n",
    "\n",
    "\n",
    "class TrainConfig(object):\n",
    "    def __init__(self, config):\n",
    "        self.batch_size = config.get('batch_size')\n",
    "        self.use_cuda = config.get('use_cuda')\n",
    "        self.max_len_seq = config.get('max_len_seq')\n",
    "        self.train_loader_workers = config.get('train_loader_workers')\n",
    "        self.test_loader_workers = config.get('test_loader_workers')\n",
    "        self.device = config.get('device')\n",
    "        self.output_dir = config.get('output_dir')\n",
    "        self.output_name = config.get('output_name')\n",
    "        self.best_name = config.get('best_name')\n",
    "        self.num_samples = config.get('num_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_config = {\n",
    "    'vocab':'/home/yikuan/project/Code/ACM/data/Full_vocab',\n",
    "    'train': '/home/shared/yikuan/ACM/data/Depression/Depression_clean_train.parquet',\n",
    "    'test': '/home/shared/yikuan/ACM/data/Depression/Depression_clean_test.parquet'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_params = {\n",
    "    'max_seq_len': 256,\n",
    "    'max_age': 110,\n",
    "    'month': 1,\n",
    "    'age_symbol': None,\n",
    "}\n",
    "\n",
    "optim_param = {\n",
    "    'lr': 3e-5,\n",
    "    'warmup_proportion': 0.1,\n",
    "    'weight_decay': 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BertVocab = load_obj(file_config['vocab'])\n",
    "ageVocab, _ = age_vocab(max_age=global_params['max_age'], mon=global_params['month'], symbol=global_params['age_symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = pd.read_parquet(file_config['train']).reset_index(drop=True)\n",
    "testData = pd.read_parquet(file_config['test']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    'batch_size': 64,\n",
    "    'use_cuda': True,\n",
    "    'max_len_seq': global_params['max_seq_len'],\n",
    "    'train_loader_workers': 3,\n",
    "    'test_loader_workers': 3,\n",
    "    'num_samples': 10,\n",
    "    'device': 'cuda:1',\n",
    "    'output_dir': '/home/shared/yikuan/ACM/model/Depression',\n",
    "    'output_name': 'behrtBayesianEmbeddingsOutput.bin',\n",
    "    'best_name': 'behrtBayesianEmbeddingsOutput_best.bin',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainConfig = TrainConfig(train_params)\n",
    "\n",
    "data_set = HF_data(trainConfig, trainData, testData, BertVocab['token2idx'], ageVocab, code='code', age='age')\n",
    "\n",
    "trainData = data_set.get_weighted_sample_train(4)\n",
    "testData = data_set.get_test_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_param = {\n",
    "    'vocab_size': len(BertVocab['token2idx'].keys()),\n",
    "    'hidden_size': 150,\n",
    "    'num_hidden_layers': 4,\n",
    "    'num_attention_heads': 6,\n",
    "    'hidden_act': 'gelu',\n",
    "    'intermediate_size': 108,\n",
    "    'max_position_embeddings': global_params['max_seq_len'],\n",
    "    'seg_vocab_size': 2,\n",
    "    'age_vocab_size': len(ageVocab.keys()),\n",
    "    'prior_prec': 1e0,\n",
    "    'prec_init': 1e0,\n",
    "    'initializer_range': 0.02,\n",
    "    'num_labels': 1,\n",
    "    'hidden_dropout_prob': 0.29,\n",
    "    'attention_probs_dropout_prob': 0.38\n",
    "}\n",
    "\n",
    "feature_dict = {\n",
    "    'word': True,\n",
    "    'age': False,\n",
    "    'seg': False,\n",
    "    'norm': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelConfig = BertConfig(model_param)\n",
    "model = BertHF(modelConfig, num_labels=modelConfig.num_labels, n_dim=150, feature_dict=feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n"
     ]
    }
   ],
   "source": [
    "def load_model(path, model):\n",
    "    mapdict = {\n",
    "        'bert.embeddings.word_embeddings.weight_posterior.loc': 'bert.embeddings.word_embeddings.weight',\n",
    "#         'bert.embeddings.age_embeddings.weight': 'bert.embeddings.age_embeddings.weight_posterior.loc'\n",
    "    }\n",
    "    \n",
    "#     # load pretrained model and update weights\n",
    "    pretrained_dict = torch.load(path)\n",
    "#     model_dict = model.state_dict()\n",
    "#     # 1. filter out unnecessary keys\n",
    "# #     pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and k not in ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']}\n",
    "#     pretrained_dict = {k: v for k, v in pretrained_dict.items()}\n",
    "#     # 2. overwrite entries in the existing state dict\n",
    "#     model_dict.update(pretrained_dict)\n",
    "    \n",
    "#     pretrained_dict = {mapdict.get(k): v for k, v in pretrained_dict.items() if k in list(mapdict.keys())}\n",
    "    \n",
    "#     model_dict.update(pretrained_dict)\n",
    "    \n",
    "#     # 3. load the new state dict\n",
    "#     model.load_state_dict(model_dict)\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    for k,v in pretrained_dict.items():\n",
    "        if (k in model_dict) and (k not in ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']):\n",
    "            model_dict[k] = v\n",
    "    for k,v in mapdict.items():\n",
    "        model_dict[k] = pretrained_dict[v]\n",
    "    model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "# model = torch_load_model('/home/shared/yikuan/ACM/model/Diabetes/behrtBayesianEmbeddings_best.bin', model)\n",
    "model = load_model('/home/shared/yikuan/HF/MLM/PureICD_diag_med.bin', model)\n",
    "model = model.to(trainConfig.device)\n",
    "optim = adam(list(model.named_parameters()),config=optim_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(logits, label):\n",
    "    sig = nn.Sigmoid()\n",
    "    output=sig(logits)\n",
    "    label, output=label.cpu(), output.detach().cpu()\n",
    "    tempprc= average_precision_score(label.numpy(),output.numpy())\n",
    "    auc = roc_auc_score(label.numpy(),output.numpy())\n",
    "    return tempprc, auc\n",
    "\n",
    "def precision_test(prob, label):\n",
    "#     sig = nn.Sigmoid()\n",
    "#     output=sig(logits)\n",
    "    tempprc= average_precision_score(label.numpy(),prob.numpy())\n",
    "    auc = roc_auc_score(label.numpy(),prob.numpy())\n",
    "    return tempprc, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beta(batch_idx, m, beta_type):\n",
    "    if beta_type == \"Blundell\":\n",
    "        beta = 2 ** (m - (batch_idx + 1)) / (2 ** m - 1) \n",
    "    elif beta_type == \"Soenderby\":\n",
    "        beta = min(epoch / (num_epochs // 4), 1)\n",
    "    elif beta_type == \"Standard\":\n",
    "        beta = 1 / m \n",
    "    else:\n",
    "        beta = 0\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(e, trainload, model, optim, m, beta_type):\n",
    "    model.train()\n",
    "\n",
    "    tr_loss = 0\n",
    "    temp_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    cnt = 0\n",
    "    for step, batch in enumerate(trainload):\n",
    "        cnt += 1\n",
    "        beta = get_beta(step, m, beta_type)\n",
    "\n",
    "        age_ids, input_ids, posi_ids, segment_ids, attMask, targets, _, _ = batch\n",
    "\n",
    "        age_ids = age_ids.to(train_params['device'])\n",
    "        input_ids = input_ids.to(train_params['device'])\n",
    "        posi_ids = posi_ids.to(train_params['device'])\n",
    "        segment_ids = segment_ids.to(train_params['device'])\n",
    "        attMask = attMask.to(train_params['device'])\n",
    "        targets = targets.view(-1).to(train_params['device'])\n",
    "\n",
    "        loss, _ = model(input_ids, age_ids, segment_ids, posi_ids, attention_mask=attMask, labels=targets, beta=beta)\n",
    "        loss.backward()\n",
    "\n",
    "        temp_loss += loss.item()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        if step % 500 == 0:\n",
    "            #             prec, a, b = precision(logits, targets)\n",
    "            #             print(\"epoch: {}\\t| Cnt: {}\\t| Loss: {}\\t| precision: {}\".format(e, cnt, temp_loss / 500, prec))\n",
    "            print(\"epoch: {}\\t|step: {}\\t|Loss: {}\".format(e, step, temp_loss / 500))\n",
    "            temp_loss = 0\n",
    "\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "    # Save a trained model\n",
    "    output_model_file = os.path.join(trainConfig.output_dir, trainConfig.output_name)\n",
    "    create_folder(trainConfig.output_dir)\n",
    "    save_model(output_model_file, model)\n",
    "\n",
    "\n",
    "def evaluation(testload, model):\n",
    "    model.eval()\n",
    "    y = []\n",
    "    y_label = []\n",
    "    loss_temp = 0\n",
    "    sig = nn.Sigmoid()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(testload):\n",
    "            age_ids, input_ids, posi_ids, segment_ids, attMask, targets, _, _ = batch\n",
    "\n",
    "            age_ids = age_ids.to(train_params['device'])\n",
    "            input_ids = input_ids.to(train_params['device'])\n",
    "            posi_ids = posi_ids.to(train_params['device'])\n",
    "            segment_ids = segment_ids.to(train_params['device'])\n",
    "            attMask = attMask.to(train_params['device'])\n",
    "            targets = targets.view(-1).to(train_params['device'])\n",
    "            \n",
    "            logits_prob = []\n",
    "            for i in range(trainConfig.num_samples):\n",
    "                logits =model(input_ids, age_ids, segment_ids, posi_ids, attention_mask=attMask, labels=None)\n",
    "                logits_prob.append(logits)\n",
    "            \n",
    "            logits_prob = torch.mean(sig(torch.stack(logits_prob, dim=1)), dim=1)\n",
    "            \n",
    "            # get mean of logits\n",
    "            \n",
    "            \n",
    "            targets = targets.cpu()\n",
    "\n",
    "            y_label.append(targets)\n",
    "            y.append(logits_prob.cpu())\n",
    "\n",
    "        y_label = torch.cat(y_label, dim=0)\n",
    "        y = torch.cat(y, dim=0)\n",
    "\n",
    "        tempprc, auc = precision_test(y.view(-1), y_label.view(-1))\n",
    "        return tempprc, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t|step: 0\t|Loss: 828.8224375\n",
      "epoch: 0\t|step: 500\t|Loss: 829.1026728112101\n",
      "epoch: 0\t|step: 1000\t|Loss: 0.4390993211865425\n",
      "epoch: 0\t|step: 1500\t|Loss: 0.4371802775263786\n",
      "epoch: 0\t|step: 2000\t|Loss: 0.43502347719669343\n",
      "epoch: 0\t|step: 2500\t|Loss: 0.4353513500392437\n",
      "epoch: 0\t|step: 3000\t|Loss: 0.4392665742635727\n",
      "epoch: 0\t|step: 3500\t|Loss: 0.4382819624841213\n",
      "epoch: 0\t|step: 4000\t|Loss: 0.43890997382998465\n",
      "epoch: 0\t|step: 4500\t|Loss: 0.4402759317755699\n",
      "epoch: 0\t|step: 5000\t|Loss: 0.4299034067094326\n",
      "epoch: 0\t|step: 5500\t|Loss: 0.42331701096892355\n",
      "epoch: 0\t|step: 6000\t|Loss: 0.43830069231987\n",
      "epoch: 0\t|step: 6500\t|Loss: 0.4324846982955933\n",
      "epoch: 0\t|step: 7000\t|Loss: 0.42691379112005234\n",
      "epoch: 0\t|step: 7500\t|Loss: 0.4305554014444351\n",
      "epoch: 0\t|step: 8000\t|Loss: 0.42616144585609433\n",
      "epoch: 0\t|step: 8500\t|Loss: 0.42814017111063\n",
      "epoch: 0\t|step: 9000\t|Loss: 0.4301676575839519\n",
      "epoch: 0\t|step: 9500\t|Loss: 0.4257418938875198\n",
      "epoch: 0\t|step: 10000\t|Loss: 0.433092533826828\n",
      "epoch: 0\t|step: 10500\t|Loss: 0.4257934483587742\n",
      "epoch: 0\t|step: 11000\t|Loss: 0.4344760993719101\n",
      "epoch: 0\t|step: 11500\t|Loss: 0.4291050752699375\n",
      "epoch: 0\t|step: 12000\t|Loss: 0.42612607032060623\n",
      "epoch: 0\t|step: 12500\t|Loss: 0.42572628700733184\n",
      "epoch: 0\t|step: 13000\t|Loss: 0.42888798275589946\n",
      "epoch: 0\t|step: 13500\t|Loss: 0.4303102444112301\n",
      "epoch: 0\t|step: 14000\t|Loss: 0.42802309480309486\n",
      "epoch: 0\t|step: 14500\t|Loss: 0.4276914171278477\n",
      "epoch: 0\t|step: 15000\t|Loss: 0.4302808160483837\n",
      "epoch: 0\t|step: 15500\t|Loss: 0.4275954897105694\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.40534057240186716, auc\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 1\t|step: 0\t|Loss: 815.3075625\n",
      "epoch: 1\t|step: 500\t|Loss: 815.5572121546865\n",
      "epoch: 1\t|step: 1000\t|Loss: 0.42795317113399506\n",
      "epoch: 1\t|step: 1500\t|Loss: 0.42571442818641664\n",
      "epoch: 1\t|step: 2000\t|Loss: 0.42129169243574144\n",
      "epoch: 1\t|step: 2500\t|Loss: 0.4293456047475338\n",
      "epoch: 1\t|step: 3000\t|Loss: 0.42111755287647246\n",
      "epoch: 1\t|step: 3500\t|Loss: 0.42781751960515974\n",
      "epoch: 1\t|step: 4000\t|Loss: 0.4221127049922943\n",
      "epoch: 1\t|step: 4500\t|Loss: 0.4239116549193859\n",
      "epoch: 1\t|step: 5000\t|Loss: 0.4248406109809876\n",
      "epoch: 1\t|step: 5500\t|Loss: 0.429185577750206\n",
      "epoch: 1\t|step: 6000\t|Loss: 0.4210750663280487\n",
      "epoch: 1\t|step: 6500\t|Loss: 0.4243160699903965\n",
      "epoch: 1\t|step: 7000\t|Loss: 0.4198422923088074\n",
      "epoch: 1\t|step: 7500\t|Loss: 0.4218980481028557\n",
      "epoch: 1\t|step: 8000\t|Loss: 0.4257948760688305\n",
      "epoch: 1\t|step: 8500\t|Loss: 0.4265155048072338\n",
      "epoch: 1\t|step: 9000\t|Loss: 0.42396344399452207\n",
      "epoch: 1\t|step: 9500\t|Loss: 0.42493868884444236\n",
      "epoch: 1\t|step: 10000\t|Loss: 0.4213191046118736\n",
      "epoch: 1\t|step: 10500\t|Loss: 0.4217593243122101\n",
      "epoch: 1\t|step: 11000\t|Loss: 0.42387216371297837\n",
      "epoch: 1\t|step: 11500\t|Loss: 0.4207072751522064\n",
      "epoch: 1\t|step: 12000\t|Loss: 0.4224544792771339\n",
      "epoch: 1\t|step: 12500\t|Loss: 0.4253828989863396\n",
      "epoch: 1\t|step: 13000\t|Loss: 0.42264969903230665\n",
      "epoch: 1\t|step: 13500\t|Loss: 0.42310046792030337\n",
      "epoch: 1\t|step: 14000\t|Loss: 0.4280506706237793\n",
      "epoch: 1\t|step: 14500\t|Loss: 0.42488018357753754\n",
      "epoch: 1\t|step: 15000\t|Loss: 0.4225303156375885\n",
      "epoch: 1\t|step: 15500\t|Loss: 0.4196670917868614\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.4102594546512227, auc\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 2\t|step: 0\t|Loss: 802.032\n",
      "epoch: 2\t|step: 500\t|Loss: 802.2842029804885\n",
      "epoch: 2\t|step: 1000\t|Loss: 0.4239914714694023\n",
      "epoch: 2\t|step: 1500\t|Loss: 0.4292247285246849\n",
      "epoch: 2\t|step: 2000\t|Loss: 0.4187765522003174\n",
      "epoch: 2\t|step: 2500\t|Loss: 0.4245618996024132\n",
      "epoch: 2\t|step: 3000\t|Loss: 0.42392694064974784\n",
      "epoch: 2\t|step: 3500\t|Loss: 0.4248147122859955\n",
      "epoch: 2\t|step: 4000\t|Loss: 0.42748737379908563\n",
      "epoch: 2\t|step: 4500\t|Loss: 0.41974396938085556\n",
      "epoch: 2\t|step: 5000\t|Loss: 0.4191649807393551\n",
      "epoch: 2\t|step: 5500\t|Loss: 0.421095024228096\n",
      "epoch: 2\t|step: 6000\t|Loss: 0.4278559530377388\n",
      "epoch: 2\t|step: 6500\t|Loss: 0.4240591302514076\n",
      "epoch: 2\t|step: 7000\t|Loss: 0.42455120319128037\n",
      "epoch: 2\t|step: 7500\t|Loss: 0.41726319575309756\n",
      "epoch: 2\t|step: 8000\t|Loss: 0.41825064861774447\n",
      "epoch: 2\t|step: 8500\t|Loss: 0.41910883563756945\n",
      "epoch: 2\t|step: 9000\t|Loss: 0.42595453771948816\n",
      "epoch: 2\t|step: 9500\t|Loss: 0.4220023861825466\n",
      "epoch: 2\t|step: 10000\t|Loss: 0.4164966586530209\n",
      "epoch: 2\t|step: 10500\t|Loss: 0.41871814116835593\n",
      "epoch: 2\t|step: 11000\t|Loss: 0.41843674731254576\n",
      "epoch: 2\t|step: 11500\t|Loss: 0.41951437178254125\n",
      "epoch: 2\t|step: 12000\t|Loss: 0.4191118159294128\n",
      "epoch: 2\t|step: 12500\t|Loss: 0.4251056624650955\n",
      "epoch: 2\t|step: 13000\t|Loss: 0.4198823888003826\n",
      "epoch: 2\t|step: 13500\t|Loss: 0.41757229471206664\n",
      "epoch: 2\t|step: 14000\t|Loss: 0.42319447952508926\n",
      "epoch: 2\t|step: 14500\t|Loss: 0.4238796325623989\n",
      "epoch: 2\t|step: 15000\t|Loss: 0.41931301206350324\n",
      "epoch: 2\t|step: 15500\t|Loss: 0.4172761837244034\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.41193355516221447, auc\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 3\t|step: 0\t|Loss: 788.9364375\n",
      "epoch: 3\t|step: 500\t|Loss: 789.1857968418002\n",
      "epoch: 3\t|step: 1000\t|Loss: 0.42729908880591394\n",
      "epoch: 3\t|step: 1500\t|Loss: 0.42062512689828874\n",
      "epoch: 3\t|step: 2000\t|Loss: 0.42402942830324175\n",
      "epoch: 3\t|step: 2500\t|Loss: 0.4186682488322258\n",
      "epoch: 3\t|step: 3000\t|Loss: 0.4217211818099022\n",
      "epoch: 3\t|step: 3500\t|Loss: 0.4209393147230148\n",
      "epoch: 3\t|step: 4000\t|Loss: 0.4211752713620663\n",
      "epoch: 3\t|step: 4500\t|Loss: 0.4221090160608292\n",
      "epoch: 3\t|step: 5000\t|Loss: 0.4241820493936539\n",
      "epoch: 3\t|step: 5500\t|Loss: 0.41703901225328444\n",
      "epoch: 3\t|step: 6000\t|Loss: 0.4183292642235756\n",
      "epoch: 3\t|step: 6500\t|Loss: 0.41717488449811935\n",
      "epoch: 3\t|step: 7000\t|Loss: 0.41828997945785523\n",
      "epoch: 3\t|step: 7500\t|Loss: 0.4230541029572487\n",
      "epoch: 3\t|step: 8000\t|Loss: 0.4148179913759232\n",
      "epoch: 3\t|step: 8500\t|Loss: 0.4232809989452362\n",
      "epoch: 3\t|step: 9000\t|Loss: 0.4208317527472973\n",
      "epoch: 3\t|step: 9500\t|Loss: 0.4217177234888077\n",
      "epoch: 3\t|step: 10000\t|Loss: 0.42087559959292414\n",
      "epoch: 3\t|step: 10500\t|Loss: 0.4215169889032841\n",
      "epoch: 3\t|step: 11000\t|Loss: 0.4112304513454437\n",
      "epoch: 3\t|step: 11500\t|Loss: 0.4204950603544712\n",
      "epoch: 3\t|step: 12000\t|Loss: 0.4244452246427536\n",
      "epoch: 3\t|step: 12500\t|Loss: 0.42083114781975745\n",
      "epoch: 3\t|step: 13000\t|Loss: 0.421965546309948\n",
      "epoch: 3\t|step: 13500\t|Loss: 0.42029697093367574\n",
      "epoch: 3\t|step: 14000\t|Loss: 0.4160594679713249\n",
      "epoch: 3\t|step: 14500\t|Loss: 0.4220544098615646\n",
      "epoch: 3\t|step: 15000\t|Loss: 0.4125921172499657\n",
      "epoch: 3\t|step: 15500\t|Loss: 0.4146656042933464\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.41530096161751634, auc\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 4\t|step: 0\t|Loss: 776.0275625\n",
      "epoch: 4\t|step: 500\t|Loss: 776.2721221559643\n",
      "epoch: 4\t|step: 1000\t|Loss: 0.4105765189230442\n",
      "epoch: 4\t|step: 1500\t|Loss: 0.41744424325227736\n",
      "epoch: 4\t|step: 2000\t|Loss: 0.4161617275476456\n",
      "epoch: 4\t|step: 2500\t|Loss: 0.4214888236522675\n",
      "epoch: 4\t|step: 3000\t|Loss: 0.4159782691001892\n",
      "epoch: 4\t|step: 3500\t|Loss: 0.41998909395933154\n",
      "epoch: 4\t|step: 4000\t|Loss: 0.4164721910059452\n",
      "epoch: 4\t|step: 4500\t|Loss: 0.42112219697237013\n",
      "epoch: 4\t|step: 5000\t|Loss: 0.42181307196617124\n",
      "epoch: 4\t|step: 5500\t|Loss: 0.41740165624022485\n",
      "epoch: 4\t|step: 6000\t|Loss: 0.4212604678273201\n",
      "epoch: 4\t|step: 6500\t|Loss: 0.4214487118721008\n",
      "epoch: 4\t|step: 7000\t|Loss: 0.417883358925581\n",
      "epoch: 4\t|step: 7500\t|Loss: 0.42121594661474226\n",
      "epoch: 4\t|step: 8000\t|Loss: 0.4196297941207886\n",
      "epoch: 4\t|step: 8500\t|Loss: 0.41865340402722356\n",
      "epoch: 4\t|step: 9000\t|Loss: 0.4154892000555992\n",
      "epoch: 4\t|step: 9500\t|Loss: 0.419366005897522\n",
      "epoch: 4\t|step: 10000\t|Loss: 0.4196077048182488\n",
      "epoch: 4\t|step: 10500\t|Loss: 0.4211901910007\n",
      "epoch: 4\t|step: 11000\t|Loss: 0.4176191725432873\n",
      "epoch: 4\t|step: 11500\t|Loss: 0.4173819229900837\n",
      "epoch: 4\t|step: 12000\t|Loss: 0.4164372251033783\n",
      "epoch: 4\t|step: 12500\t|Loss: 0.4180334530472755\n",
      "epoch: 4\t|step: 13000\t|Loss: 0.4172804332971573\n",
      "epoch: 4\t|step: 13500\t|Loss: 0.4174796285331249\n",
      "epoch: 4\t|step: 14000\t|Loss: 0.41831764322519305\n",
      "epoch: 4\t|step: 14500\t|Loss: 0.41696444898843765\n",
      "epoch: 4\t|step: 15000\t|Loss: 0.41806235548853876\n",
      "epoch: 4\t|step: 15500\t|Loss: 0.41881200501322746\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.41993912145057766, auc\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "epoch: 5\t|step: 0\t|Loss: 763.286375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5\t|step: 500\t|Loss: 763.5314534712732\n",
      "epoch: 5\t|step: 1000\t|Loss: 0.41862406802177426\n",
      "epoch: 5\t|step: 1500\t|Loss: 0.41541320756077765\n",
      "epoch: 5\t|step: 2000\t|Loss: 0.41664264419674873\n",
      "epoch: 5\t|step: 2500\t|Loss: 0.4183664395213127\n",
      "epoch: 5\t|step: 3000\t|Loss: 0.41411837765574455\n",
      "epoch: 5\t|step: 3500\t|Loss: 0.4216207355856895\n",
      "epoch: 5\t|step: 4000\t|Loss: 0.41628529185056684\n",
      "epoch: 5\t|step: 4500\t|Loss: 0.4166868434250355\n",
      "epoch: 5\t|step: 5000\t|Loss: 0.4104483119547367\n",
      "epoch: 5\t|step: 5500\t|Loss: 0.41959629964828493\n",
      "epoch: 5\t|step: 6000\t|Loss: 0.417732636898756\n",
      "epoch: 5\t|step: 6500\t|Loss: 0.41669280475378034\n",
      "epoch: 5\t|step: 7000\t|Loss: 0.4119749778807163\n",
      "epoch: 5\t|step: 7500\t|Loss: 0.4141731736958027\n",
      "epoch: 5\t|step: 8000\t|Loss: 0.4132458476424217\n",
      "epoch: 5\t|step: 8500\t|Loss: 0.4202275656461716\n",
      "epoch: 5\t|step: 9000\t|Loss: 0.4155992553830147\n",
      "epoch: 5\t|step: 9500\t|Loss: 0.4165930699110031\n",
      "epoch: 5\t|step: 10000\t|Loss: 0.42042563939094546\n",
      "epoch: 5\t|step: 10500\t|Loss: 0.4215616232454777\n",
      "epoch: 5\t|step: 11000\t|Loss: 0.41456899854540824\n",
      "epoch: 5\t|step: 11500\t|Loss: 0.41439680570363996\n",
      "epoch: 5\t|step: 12000\t|Loss: 0.41568578758835795\n",
      "epoch: 5\t|step: 12500\t|Loss: 0.41484608337283135\n",
      "epoch: 5\t|step: 13000\t|Loss: 0.4186768108010292\n",
      "epoch: 5\t|step: 13500\t|Loss: 0.4103057966828346\n",
      "epoch: 5\t|step: 14000\t|Loss: 0.41443091601133347\n",
      "epoch: 5\t|step: 14500\t|Loss: 0.4172304680347443\n",
      "epoch: 5\t|step: 15000\t|Loss: 0.4141980040371418\n",
      "epoch: 5\t|step: 15500\t|Loss: 0.4148332037627697\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.4187224628164366, auc\n",
      "epoch: 6\t|step: 0\t|Loss: 750.7365\n",
      "epoch: 6\t|step: 500\t|Loss: 750.9846116852463\n",
      "epoch: 6\t|step: 1000\t|Loss: 0.41157152304053307\n",
      "epoch: 6\t|step: 1500\t|Loss: 0.4139024266600609\n",
      "epoch: 6\t|step: 2000\t|Loss: 0.41675133889913557\n",
      "epoch: 6\t|step: 2500\t|Loss: 0.41451688155531885\n",
      "epoch: 6\t|step: 3000\t|Loss: 0.41216818064451216\n",
      "epoch: 6\t|step: 3500\t|Loss: 0.41313715252280236\n",
      "epoch: 6\t|step: 4000\t|Loss: 0.42361809134483336\n",
      "epoch: 6\t|step: 4500\t|Loss: 0.4183493811786175\n",
      "epoch: 6\t|step: 5000\t|Loss: 0.4109682491719723\n",
      "epoch: 6\t|step: 5500\t|Loss: 0.4138354496359825\n",
      "epoch: 6\t|step: 6000\t|Loss: 0.41800009363889695\n",
      "epoch: 6\t|step: 6500\t|Loss: 0.4139634582996368\n",
      "epoch: 6\t|step: 7000\t|Loss: 0.4167664618492126\n",
      "epoch: 6\t|step: 7500\t|Loss: 0.4129148845076561\n",
      "epoch: 6\t|step: 8000\t|Loss: 0.42034314823150637\n",
      "epoch: 6\t|step: 8500\t|Loss: 0.41392048534750936\n",
      "epoch: 6\t|step: 9000\t|Loss: 0.41528978377580644\n",
      "epoch: 6\t|step: 9500\t|Loss: 0.41323067057132723\n",
      "epoch: 6\t|step: 10000\t|Loss: 0.4057995915412903\n",
      "epoch: 6\t|step: 10500\t|Loss: 0.419267648935318\n",
      "epoch: 6\t|step: 11000\t|Loss: 0.41058357018232344\n",
      "epoch: 6\t|step: 11500\t|Loss: 0.41352156600356105\n",
      "epoch: 6\t|step: 12000\t|Loss: 0.4113528551161289\n",
      "epoch: 6\t|step: 12500\t|Loss: 0.4153597322702408\n",
      "epoch: 6\t|step: 13000\t|Loss: 0.41433639338612555\n",
      "epoch: 6\t|step: 13500\t|Loss: 0.4121442462205887\n",
      "epoch: 6\t|step: 14000\t|Loss: 0.4165050291419029\n",
      "epoch: 6\t|step: 14500\t|Loss: 0.41272792303562167\n",
      "epoch: 6\t|step: 15000\t|Loss: 0.4107464919090271\n",
      "epoch: 6\t|step: 15500\t|Loss: 0.4133360235393047\n",
      "** ** * Saving fine - tuned model ** ** * \n",
      "test precision: 0.41898298565193715, auc\n",
      "epoch: 7\t|step: 0\t|Loss: 738.35325\n",
      "epoch: 7\t|step: 500\t|Loss: 738.6010704015494\n",
      "epoch: 7\t|step: 1000\t|Loss: 0.4139410726726055\n",
      "epoch: 7\t|step: 1500\t|Loss: 0.4117884593904018\n",
      "epoch: 7\t|step: 2000\t|Loss: 0.4193021709918976\n",
      "epoch: 7\t|step: 2500\t|Loss: 0.41255502209067346\n",
      "epoch: 7\t|step: 3000\t|Loss: 0.4132375973165035\n",
      "epoch: 7\t|step: 3500\t|Loss: 0.41000381803512576\n",
      "epoch: 7\t|step: 4000\t|Loss: 0.4062989222109318\n",
      "epoch: 7\t|step: 4500\t|Loss: 0.41802478072047233\n",
      "epoch: 7\t|step: 5000\t|Loss: 0.4106577994823456\n",
      "epoch: 7\t|step: 5500\t|Loss: 0.4109119782447815\n",
      "epoch: 7\t|step: 6000\t|Loss: 0.41734631097316743\n",
      "epoch: 7\t|step: 6500\t|Loss: 0.41312672233581543\n",
      "epoch: 7\t|step: 7000\t|Loss: 0.41128341925144196\n",
      "epoch: 7\t|step: 7500\t|Loss: 0.4146929777264595\n",
      "epoch: 7\t|step: 8000\t|Loss: 0.4140565836131573\n",
      "epoch: 7\t|step: 8500\t|Loss: 0.4146798466145992\n",
      "epoch: 7\t|step: 9000\t|Loss: 0.41592876958847047\n",
      "epoch: 7\t|step: 9500\t|Loss: 0.4118988604545593\n",
      "epoch: 7\t|step: 10000\t|Loss: 0.418048139333725\n",
      "epoch: 7\t|step: 10500\t|Loss: 0.412648648917675\n",
      "epoch: 7\t|step: 11000\t|Loss: 0.4089212155044079\n",
      "epoch: 7\t|step: 11500\t|Loss: 0.4073932045996189\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a467317124a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbest_pre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Blundell\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#     auc_train, time_cost_train = evaluation(trainData, model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mauc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-c1e5f1d13312>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(e, trainload, model, optim, m, beta_type)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mtemp_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py3/lib/python3.7/site-packages/pytorch_pretrained_bert/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    269\u001b[0m                 \u001b[0;31m# Add grad clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_grad_norm'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                     \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_grad_norm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py3/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_pre = 0.0\n",
    "for e in range(20):\n",
    "    train(e, trainData, model,optim, len(trainData), \"Blundell\")\n",
    "#     auc_train, time_cost_train = evaluation(trainData, model)\n",
    "    auc_test, auc = evaluation(testData, model)\n",
    "    print('test precision: {}, auc:{}'.format(auc_test, auc))\n",
    "    if auc_test >best_pre:\n",
    "        # Save a trained model\n",
    "        output_model_file = os.path.join(trainConfig.output_dir, trainConfig.best_name)\n",
    "        create_folder(trainConfig.output_dir)\n",
    "        save_model(output_model_file, model)\n",
    "        best_pre = auc_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3)",
   "language": "python",
   "name": "py3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
